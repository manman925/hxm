# äººå·¥æ™ºèƒ½APIåº”ç”¨

æœ¬ç« èŠ‚ä»¥ Hugging Faceã€ChatAnywhereã€ç™¾ç‚¼å¹³å°ä¸ºä¾‹ï¼Œä»‹ç»å¤§æ¨¡å‹APIè°ƒç”¨çš„é‡è¦çŸ¥è¯†ã€‚

## Hugging Face

[HuggingFace 10åˆ†é’Ÿå¿«é€Ÿå…¥é—¨ï¼ˆä¸€ï¼‰ï¼Œåˆ©ç”¨Transformersï¼ŒPipelineæ¢ç´¢AI](https://www.bilibili.com/video/BV1vN41127ob/?share_source=copy_web&vd_source=d345aeeaedba2347709dc8ca7b1b5cb1)

Hugging Face æ˜¯ä¸€ä¸ªä¸“æ³¨äºäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å’Œæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰çš„å¹³å°ï¼Œç‰¹åˆ«ä»¥è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å·¥å…·å’Œæ¨¡å‹è€Œé—»åã€‚å®ƒæä¾›äº†ä¸€ä¸ªä¸°å¯Œçš„å¼€æºç”Ÿæ€ç³»ç»Ÿï¼Œä¾›ç ”ç©¶äººå‘˜ã€å¼€å‘è€…å’Œæ•°æ®ç§‘å­¦å®¶ä½¿ç”¨ã€‚å…¶ä¸­ï¼Œæœ€è‘—åçš„å·¥å…·ä¹‹ä¸€æ˜¯ **Transformers** åº“ï¼Œè¿™ä¸ªåº“åŒ…å«äº†å¤šç§é¢„è®­ç»ƒçš„æ¨¡å‹ï¼ˆå¦‚BERTã€GPTã€T5ç­‰ï¼‰ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥ç”¨äºå„ç§ ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€ç¿»è¯‘ã€é—®ç­”ã€æƒ…æ„Ÿåˆ†æç­‰ã€‚

Hugging Face è¿˜æä¾›äº† **Hub**ï¼Œä¸€ä¸ªæ¨¡å‹æ‰˜ç®¡å¹³å°ï¼Œç”¨æˆ·å¯ä»¥åœ¨ä¸Šé¢åˆ†äº«å’Œä¸‹è½½å„ç§é¢„è®­ç»ƒçš„AIæ¨¡å‹ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æ”¯æŒè‡ªå®šä¹‰è®­ç»ƒæ¨¡å‹å’Œé€šè¿‡APIè¿›è¡Œæ¨ç†ã€‚Hugging Face çš„å¹³å°å’Œç¤¾åŒºéå¸¸æ´»è·ƒï¼Œæ˜¯å½“å‰AIå’ŒNLPé¢†åŸŸçš„é‡è¦èµ„æºä¹‹ä¸€ã€‚


### å®‰è£…

1. å®‰è£…æœºå™¨å­¦ä¹ åŸºç¡€åº“ï¼ˆpytorch æˆ–è€… TensorFlowï¼‰ã€‚
    é¦–å…ˆåˆ›å»ºä¸€ä¸ª Conda ç¯å¢ƒï¼Œç„¶åå®‰è£… Pytorchã€‚`conda install pytorch::pytorch torchvision torchaudio -c pytorch`

2. å®‰è£… transformersï¼š
   `pip install transformers datasets evaluate accelerate`ã€‚

### é…ç½® GPU åŠ é€Ÿ

Pytorch æ”¯æŒ CUDAï¼ˆNVIDIAï¼‰å’Œ MPSï¼ˆMacï¼‰å¹³å°çš„ GPU åŠ é€Ÿï¼Œæ‰€ä»¥è¿™é‡Œæ£€æµ‹ä¸€ä¸‹ç¡¬ä»¶ç¯å¢ƒã€‚

```{python}
import torch

# select the device for computation
if torch.cuda.is_available():
    device = torch.device("cuda")
elif torch.backends.mps.is_available():
    device = torch.device("mps")
else:
    device = torch.device("cpu")

print(f"using device: {device}")
```


### Pipeline

`pipeline` æ˜¯ `transformers` åº“ä¸­çš„ä¸€ä¸ªé«˜å±‚æ¥å£ï¼Œæ—¨åœ¨ç®€åŒ–æ¨¡å‹çš„ä½¿ç”¨ã€‚å®ƒå°è£…äº†æ¨¡å‹çš„åŠ è½½ã€è¾“å…¥å¤„ç†ã€é¢„æµ‹å’Œè¾“å‡ºå¤„ç†çš„ç»†èŠ‚ï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥ä»¥æ›´ç®€å•çš„æ–¹å¼æ‰§è¡Œå¸¸è§ä»»åŠ¡ã€‚

åœ¨ Hugging Face Transformers åº“ä¸­ï¼Œpipelines å¯ä»¥è¢«åˆ†ä¸ºä¸åŒçš„ç±»åˆ«ï¼Œä»¥é€‚åº”éŸ³é¢‘ã€è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å’Œå¤šæ¨¡æ€ä»»åŠ¡ã€‚ä»¥ä¸‹æ˜¯è¿™äº›ç±»åˆ«ä¸­å¸¸è§çš„ pipelinesï¼š

1. **éŸ³é¢‘ï¼ˆAudioï¼‰**:
   - è¯­éŸ³è¯†åˆ«ï¼ˆSpeech Recognitionï¼‰: å°†éŸ³é¢‘è½¬æ¢ä¸ºæ–‡æœ¬ã€‚
   - è¯­éŸ³åˆæˆï¼ˆText-to-Speechï¼‰: å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³ã€‚
   - è¯­éŸ³åˆ†ç±»ï¼ˆSpeech Classificationï¼‰: å¯¹éŸ³é¢‘è¿›è¡Œåˆ†ç±»ï¼Œå¦‚æƒ…ç»ªè¯†åˆ«ã€‚

2. **è®¡ç®—æœºè§†è§‰ï¼ˆComputer Visionï¼‰**:
   - å›¾åƒåˆ†ç±»ï¼ˆImage Classificationï¼‰: è¯†åˆ«å›¾åƒä¸­çš„ä¸»è¦å¯¹è±¡æˆ–åœºæ™¯ã€‚
   - å¯¹è±¡æ£€æµ‹ï¼ˆObject Detectionï¼‰: è¯†åˆ«å¹¶å®šä½å›¾åƒä¸­çš„å¯¹è±¡ã€‚
   - å›¾åƒåˆ†å‰²ï¼ˆImage Segmentationï¼‰: å°†å›¾åƒåˆ†å‰²æˆå¤šä¸ªéƒ¨åˆ†æˆ–å¯¹è±¡ã€‚
   - å›¾åƒç”Ÿæˆï¼ˆImage Generationï¼‰: æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆå›¾åƒã€‚

3. **è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰**:
   - æ–‡æœ¬åˆ†ç±»ï¼ˆText Classificationï¼‰: å¯¹æ–‡æœ¬è¿›è¡Œåˆ†ç±»ï¼Œå¦‚æƒ…æ„Ÿåˆ†æã€‚
   - å‘½åå®ä½“è¯†åˆ«ï¼ˆNamed Entity Recognition, NERï¼‰: è¯†åˆ«æ–‡æœ¬ä¸­çš„å®ä½“ã€‚
   - é—®ç­”ï¼ˆQuestion Answeringï¼‰: ä»æ–‡æœ¬ä¸­æ‰¾åˆ°é—®é¢˜çš„ç­”æ¡ˆã€‚
   - æ–‡æœ¬ç”Ÿæˆï¼ˆText Generationï¼‰: ç”Ÿæˆæ–°çš„æ–‡æœ¬å†…å®¹ã€‚
   - æ‘˜è¦ï¼ˆSummarizationï¼‰: ç”Ÿæˆæ–‡æœ¬çš„æ‘˜è¦ã€‚
   - ç¿»è¯‘ï¼ˆTranslationï¼‰: å°†æ–‡æœ¬ä»ä¸€ç§è¯­è¨€ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€ã€‚

4. **å¤šæ¨¡æ€ï¼ˆMultimodalï¼‰**:
   - è§†è§‰é—®ç­”ï¼ˆVisual Question Answering, VQAï¼‰: ç»“åˆå›¾åƒå’Œæ–‡æœ¬é—®é¢˜ï¼Œæä¾›ç­”æ¡ˆã€‚
   - å›¾åƒå­—å¹•ç”Ÿæˆï¼ˆImage Captioningï¼‰: ä¸ºå›¾åƒç”Ÿæˆæè¿°æ€§æ–‡æœ¬ã€‚
   - è§†é¢‘é—®ç­”ï¼ˆVideo Question Answeringï¼‰: æ ¹æ®è§†é¢‘å†…å®¹å›ç­”é—®é¢˜ã€‚
   - å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æï¼ˆMultimodal Sentiment Analysisï¼‰: ç»“åˆæ–‡æœ¬ã€éŸ³é¢‘å’Œè§†è§‰ä¿¡æ¯è¿›è¡Œæƒ…æ„Ÿåˆ†æã€‚

è¿™äº› pipelines åˆ©ç”¨äº†é¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œå¯ä»¥å¤„ç†å„ç§ä»»åŠ¡ï¼Œä»å•ä¸€æ¨¡æ€çš„éŸ³é¢‘æˆ–å›¾åƒå¤„ç†åˆ°ç»“åˆå¤šç§æ¨¡æ€ä¿¡æ¯çš„å¤æ‚ä»»åŠ¡ã€‚ç”¨æˆ·å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ¨¡å‹å’Œpipelineæ¥å®ç°ç‰¹å®šçš„ä»»åŠ¡ã€‚


```{python}
import inspect
from transformers import pipelines

# è·å– transformers.pipeline æ¨¡å—ä¸­çš„æ‰€æœ‰æˆå‘˜
pipeline_members = inspect.getmembers(pipelines)

# è¿‡æ»¤å‡ºç±»ï¼Œå¹¶ä¸”åç§°ä»¥ 'Pipeline' ç»“å°¾
pipeline_classes = [name for name, obj in pipeline_members if inspect.isclass(obj) and name.endswith('Pipeline')]

# æ‰“å°ç¬¦åˆæ¡ä»¶çš„ç±»åç§°
print("Classes ending with 'Pipeline':")
for class_name in pipeline_classes:
    print(class_name)

```


ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸è§çš„ `pipeline` ç±»å‹å’Œå®ƒä»¬çš„åº”ç”¨åœºæ™¯ï¼š

#### æ–‡æœ¬åˆ†ç±» (`text-classification`)

ç”¨äºå¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†ç±»ï¼Œä¾‹å¦‚æƒ…æ„Ÿåˆ†æã€‚

```{python}
from transformers import pipeline
from pprint import pprint  # pretty print

classifier = pipeline('text-classification')
result = classifier("I love using transformers!")
pprint(result)
```

#### å‘½åå®ä½“è¯†åˆ« (`ner`)

ç”¨äºè¯†åˆ«æ–‡æœ¬ä¸­çš„å‘½åå®ä½“ï¼ˆå¦‚äººåã€åœ°ç‚¹ã€ç»„ç»‡ç­‰ï¼‰ã€‚

```{python}
from transformers import pipeline

ner = pipeline('ner', device = device)
result = ner("Hugging Face is based in New York City.")
pprint(result)
```

#### é—®ç­” (`question-answering`)

ç”¨äºä»ç»™å®šä¸Šä¸‹æ–‡ä¸­å›ç­”é—®é¢˜ã€‚

```{python}
from transformers import pipeline

question_answerer = pipeline('question-answering', device = device)
result = question_answerer(question="What is the capital of France?", context="The capital of France is Paris.")
pprint(result)
```

#### æ–‡æœ¬ç”Ÿæˆ (`text-generation`)

ç”¨äºç”Ÿæˆæ–‡æœ¬ï¼Œä¾‹å¦‚ç”Ÿæˆç»­å†™æˆ–å¯¹è¯ã€‚

```{python}
from transformers import pipeline

generator = pipeline('text-generation', device = device)
result = generator("Once upon a time", max_length=50)
pprint(result)
```

#### ç¿»è¯‘ (`translation`)

ç”¨äºå°†æ–‡æœ¬ä»ä¸€ç§è¯­è¨€ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€ã€‚

```{python}
from transformers import pipeline

translator = pipeline('translation_en_to_fr', device = device)
result = translator("Hello, how are you?")
pprint(result)
```

#### æ–‡æœ¬æ‘˜è¦ (`summarization`)

ç”¨äºå¯¹é•¿æ–‡æœ¬è¿›è¡Œæ‘˜è¦ï¼Œæå–ä¸»è¦å†…å®¹ã€‚

```{python}
from transformers import pipeline

summarizer = pipeline('summarization', device = device)
result = summarizer("Hugging Face is creating a tool that democratizes AI. The library will support various tasks and models.")
pprint(result)
```

### è¿è¡Œæœºåˆ¶

ä½¿ç”¨ Hugging Face çš„ `pipeline` è¿è¡Œä»»åŠ¡æ—¶ï¼Œä»»åŠ¡é»˜è®¤æ˜¯åœ¨æœ¬åœ°æ‰§è¡Œçš„ã€‚

å½“ä½ ä½¿ç”¨ `pipeline` å‡½æ•°æ—¶ï¼Œå®ƒä¼šåŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹ï¼ˆå¯ä»¥æ˜¯ Hugging Face Hub ä¸Šçš„æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥æ˜¯ä½ æœ¬åœ°çš„æ¨¡å‹ï¼‰ï¼Œç„¶ååœ¨ä½ çš„æœ¬åœ°æœºå™¨ä¸Šæ‰§è¡Œæ¨ç†ä»»åŠ¡ã€‚è¿™æ„å‘³ç€æ‰€æœ‰è®¡ç®—éƒ½æ˜¯åœ¨ä½ çš„æœ¬åœ°è®¡ç®—æœºä¸Šè¿›è¡Œçš„ï¼Œè€Œä¸æ˜¯åœ¨ Hugging Face çš„æœåŠ¡å™¨ä¸Šè¿›è¡Œçš„ã€‚

ä¸è¿‡ï¼Œ`pipeline` ä¹Ÿå¯ä»¥è®¿é—®åœ¨çº¿çš„æ¨¡å‹å­˜å‚¨åº“ã€‚å¦‚æœä½ æŒ‡å®šäº†ä¸€ä¸ªåœ¨çº¿æ¨¡å‹ï¼ˆä¾‹å¦‚ Hugging Face Hub ä¸Šçš„æŸä¸ªæ¨¡å‹ï¼‰ï¼Œé‚£ä¹ˆ `pipeline` ä¼šå…ˆä»åœ¨çº¿å­˜å‚¨åº“ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ï¼Œç„¶ååœ¨æœ¬åœ°è¿è¡Œæ¨ç†ä»»åŠ¡ã€‚å› æ­¤ï¼Œå³ä½¿ä½ è®¿é—®çš„æ˜¯åœ¨çº¿æ¨¡å‹ï¼Œæ‰§è¡Œè¿‡ç¨‹ä»ç„¶æ˜¯åœ¨æœ¬åœ°å®Œæˆçš„ã€‚

æœ¬åœ°è¿è¡Œæ¨ç†ï¼Œå¯ä»¥å¾ˆæ–¹ä¾¿çš„æ‰§è¡Œæ‰¹å¤„ç†ä»»åŠ¡ã€‚


```{python}
from transformers import pipeline

classifier = pipeline("sentiment-analysis", device=device)

results = classifier(["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."])
for result in results:
    print(f"label: {result['label']}, with score: {round(result['score'], 4)}")
```


#### é…ç½®æ¨¡å‹å’Œå‚æ•°

`pipeline` ä¼šè‡ªåŠ¨ä¸‹è½½å’Œä½¿ç”¨é»˜è®¤çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚ä½†æ˜¯æœ‰äº›ä»»åŠ¡å¯èƒ½æ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œè¿™æ—¶å€™å°±éœ€è¦é…ç½®æ¨¡å‹å‚æ•°ã€‚æ­¤å¤–ï¼Œå¦‚æœéœ€è¦ä½¿ç”¨ç‰¹å®šçš„æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥åœ¨ `pipeline` æ„é€ å‡½æ•°ä¸­æŒ‡å®šæ¨¡å‹åç§°ã€‚

`pipeline` ä¼šæ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨å¤„ç†è¾“å…¥å’Œè¾“å‡ºã€‚ä¾‹å¦‚ï¼Œæ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„è¾“å‡ºé€šå¸¸æ˜¯æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ï¼Œè€Œç¿»è¯‘ä»»åŠ¡çš„è¾“å‡ºæ˜¯ç¿»è¯‘åçš„æ–‡æœ¬ã€‚å¦‚æœæä¾›çš„ä»»åŠ¡åç§°ä¸æ­£ç¡®ï¼Œ`pipeline` å¯èƒ½ä¼šæŠ›å‡ºé”™è¯¯ã€‚å¦‚æœæ¨¡å‹ä¸é€‚åˆç‰¹å®šä»»åŠ¡ï¼Œä¹Ÿå¯èƒ½ä¼šå¾—åˆ°ä¸å‡†ç¡®çš„ç»“æœæˆ–é‡åˆ°è¿è¡Œæ—¶é”™è¯¯ã€‚


ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹ `ZeroShotClassificationPipeline` ä»»åŠ¡æŒ‡å®šä½¿ç”¨äº† Facebook çš„ `bart-large-mnli` æ¨¡å‹ã€‚

```{python}
oracle = pipeline("zero-shot-classification", 
                  model="facebook/bart-large-mnli",
                  device=device)
oracle(
    "I have a problem with my iphone that needs to be resolved asap!!",
    candidate_labels=["urgent", "not urgent", "phone", "tablet", "computer"],
)
```

```{python}
oracle(
    "I have a problem with my iphone that needs to be resolved asap!!",
    candidate_labels=["english", "german"],
)
```

æ•´ä¸ªæµç¨‹ä»¥åŠæœ¬åœ°æ¨¡å‹çš„å®‰è£…å’Œè¿è¡Œæƒ…å†µï¼š

1. **æ¨¡å‹ä¸‹è½½**ï¼š
   - å½“ä½ æŒ‡å®š `model="facebook/bart-large-mnli"` æ—¶ï¼ŒHugging Face çš„ `transformers` åº“ä¼šä» Hugging Face Hub ä¸‹è½½è¿™ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼ˆ`facebook/bart-large-mnli`ï¼‰ã€‚
   - æ¨¡å‹ä¸‹è½½åï¼Œä¼šè¢«å­˜å‚¨åœ¨ä½ çš„æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿä¸­ï¼Œé»˜è®¤ä½ç½®é€šå¸¸æ˜¯ `~/.cache/huggingface/transformers/` ç›®å½•ä¸‹ã€‚å¦‚æœä½ æœ‰è‡ªå®šä¹‰çš„ç¼“å­˜ç›®å½•è®¾ç½®ï¼Œæ¨¡å‹å°†ä¸‹è½½åˆ°æŒ‡å®šä½ç½®ã€‚

2. **æ¨¡å‹åŠ è½½**ï¼š
   - ä¸‹è½½å®Œæˆåï¼Œ`pipeline` ä¼šå°†è¯¥æ¨¡å‹åŠ è½½åˆ°å†…å­˜ä¸­ã€‚è¿™åŒ…æ‹¬æ¨¡å‹çš„æƒé‡ã€é…ç½®æ–‡ä»¶ä»¥åŠä¸ä¹‹ç›¸å…³çš„è¯æ±‡è¡¨ï¼ˆtokenizerï¼‰ã€‚

3. **ä»»åŠ¡æ‰§è¡Œ**ï¼š
   - å½“ä½ ä½¿ç”¨ `oracle` è¿™ä¸ª `pipeline` å¯¹è±¡æ¥è¿›è¡Œé›¶æ ·æœ¬åˆ†ç±»ä»»åŠ¡æ—¶ï¼Œæ‰€æœ‰çš„è®¡ç®—å’Œæ¨ç†ï¼ˆinferenceï¼‰éƒ½æ˜¯åœ¨ä½ çš„æœ¬åœ°æœºå™¨ä¸Šæ‰§è¡Œçš„ã€‚è¿™åŒ…æ‹¬æ–‡æœ¬çš„é¢„å¤„ç†ã€æ¨¡å‹çš„å‰å‘ä¼ æ’­è®¡ç®—ã€ä»¥åŠåå¤„ç†å’Œè¾“å‡ºç»“æœã€‚

#### æœ¬åœ°æ¨¡å‹çš„å­˜å‚¨å’Œç®¡ç†

- **å­˜å‚¨ä½ç½®**ï¼šæ¨¡å‹çš„æƒé‡æ–‡ä»¶ã€é…ç½®æ–‡ä»¶å’Œè¯æ±‡è¡¨ä¼šå­˜å‚¨åœ¨ `~/.cache/huggingface/hub/` ä¸‹çš„ä¸€ä¸ªä»¥æ¨¡å‹åç§°å‘½åçš„ç›®å½•ä¸­ã€‚ä¾‹å¦‚ï¼š`~/.cache/huggingface/hub/models--facebook--bart-large-mnli/`ã€‚
  
- **ç¼“å­˜æœºåˆ¶**ï¼šå¦‚æœä½ å†æ¬¡ä½¿ç”¨åŒä¸€ä¸ªæ¨¡å‹ï¼ˆå¦‚ `facebook/bart-large-mnli`ï¼‰ï¼Œ`pipeline` ä¼šç›´æ¥ä»æœ¬åœ°ç¼“å­˜ä¸­åŠ è½½æ¨¡å‹ï¼Œè€Œä¸ä¼šå†æ¬¡ä» Hugging Face Hub ä¸‹è½½ï¼Œé™¤éä½ æ‰‹åŠ¨æ¸…é™¤ç¼“å­˜æˆ–æŒ‡å®šä¸‹è½½æ–°çš„æ¨¡å‹ç‰ˆæœ¬ã€‚

#### é…ç½®ä¸€ä¸ªç¿»è¯‘å™¨

ä¸Šé¢æˆ‘ä»¬è°ƒç”¨ä¸€ä¸ªç¿»è¯‘å™¨ï¼Œå°†è‹±æ–‡ç¿»è¯‘ä¸ºæ³•æ–‡ã€‚

```{python}
en_fr_translator = pipeline("translation_en_to_fr", device=device)
en_fr_translator("How old are you?")
```

ä¸è¿‡ï¼Œè°ƒç”¨ `pipeline("tranlation_en_to_zh")` å´ä¼šå‡ºé”™ã€‚è¿™æ˜¯å› ä¸ºï¼šè™½ç„¶ "translation_en_to_zh" æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ä»»åŠ¡æ ‡è¯†ç¬¦ï¼Œä½†å®ƒå¹¶ä¸æ˜¯ç›´æ¥æŒ‡å®šæ¨¡å‹çš„åç§°ã€‚è¿™æ—¶ï¼Œéœ€è¦æ˜¾å¼æŒ‡å®šæ¨¡å‹åç§°æ¥é¿å…è¿™ç§æƒ…å†µï¼š

::: {.callout-note}
è¿™é‡Œè¿˜éœ€è¦å®‰è£…ä¸€ä¸ªç¼ºå¤±çš„æ¨¡å—ï¼š`SentencePiece`ã€‚

`SentencePiece` æ˜¯ä¸€ä¸ªç”¨äºæ–‡æœ¬åˆ†è¯å’Œè¯æ±‡ç”Ÿæˆçš„å·¥å…·ï¼Œå®ƒåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­éå¸¸æœ‰ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒå’Œä½¿ç”¨åŸºäºå­è¯å•å…ƒçš„æ¨¡å‹æ—¶ã€‚`SentencePiece` ç”± Google å¼€å‘ï¼Œä½œä¸ºä¸€ç§æ— è¯­è¨€ä¾èµ–çš„æ–¹æ³•ï¼Œå®ƒå¯ä»¥å¤„ç†å‡ ä¹ä»»ä½•è¯­è¨€çš„æ–‡æœ¬æ•°æ®ã€‚

**`SentencePiece` çš„ä¸»è¦åŠŸèƒ½**

1. **å­è¯å•å…ƒï¼ˆSubword Unitsï¼‰ç”Ÿæˆ**:
   - `SentencePiece` ä¸ä¾èµ–äºè¯­è¨€çš„ç‰¹å®šè¯æ±‡è¡¨ï¼Œè€Œæ˜¯é€šè¿‡æ•°æ®é©±åŠ¨çš„æ–¹æ³•ç”Ÿæˆå­è¯å•å…ƒã€‚å®ƒé€šè¿‡åˆ†æè®­ç»ƒæ•°æ®ä¸­çš„å¸¸è§å­—ç¬¦åºåˆ—ï¼Œç”Ÿæˆé€‚åˆè¯¥æ•°æ®é›†çš„å­è¯å•å…ƒï¼Œè¿™äº›å­è¯å•å…ƒå¯ä»¥æ˜¯å®Œæ•´çš„è¯ã€è¯çš„ä¸€éƒ¨åˆ†ï¼ˆå¦‚è¯ç¼€ã€è¯æ ¹ï¼‰ã€ç”šè‡³æ˜¯å•ä¸ªå­—ç¬¦ã€‚
   - è¿™åœ¨å¤„ç†ä½èµ„æºè¯­è¨€æˆ–å¤šè¯­è¨€ä»»åŠ¡æ—¶ç‰¹åˆ«æœ‰ç”¨ï¼Œå› ä¸ºå®ƒå¯ä»¥ç”Ÿæˆè·¨è¯­è¨€çš„ç»Ÿä¸€è¯æ±‡è¡¨ï¼Œå‡å°‘OOVï¼ˆOut of Vocabularyï¼Œè¯æ±‡è¡¨å¤–çš„è¯ï¼‰é—®é¢˜ã€‚

2. **BPEï¼ˆByte-Pair Encodingï¼‰å’Œ Unigram æ¨¡å‹**:
   - `SentencePiece` æ”¯æŒå¤šç§å­è¯åˆ†å‰²æ–¹æ³•ï¼ŒåŒ…æ‹¬ BPEï¼ˆByte-Pair Encodingï¼‰å’Œ Unigram æ¨¡å‹ã€‚BPE æ˜¯ä¸€ç§å¸¸ç”¨çš„å­è¯åˆ†å‰²ç®—æ³•ï¼Œé€šè¿‡é¢‘ç¹åœ°åˆå¹¶å­—ç¬¦å¯¹æ¥ç”Ÿæˆå­è¯å•å…ƒã€‚Unigram æ¨¡å‹åˆ™æ˜¯ä¸€ç§åŸºäºæ¦‚ç‡çš„æ¨¡å‹ï¼Œå®ƒæ ¹æ®å­è¯å•å…ƒçš„æ¦‚ç‡æ¥åˆ†å‰²æ–‡æœ¬ã€‚

3. **è¯­è¨€æ— å…³æ€§**:
   - ä¸ä¼ ç»Ÿçš„åˆ†è¯å™¨ä¸åŒï¼Œ`SentencePiece` ä¸éœ€è¦ä¾èµ–äºç©ºæ ¼æˆ–å…¶ä»–ç‰¹å®šçš„æ ‡è®°æ¥åˆ†å‰²è¯è¯­ã€‚è¿™ä½¿å¾—å®ƒåœ¨å¤„ç†æ²¡æœ‰æ˜ç¡®å•è¯è¾¹ç•Œçš„è¯­è¨€ï¼ˆå¦‚ä¸­æ–‡ã€æ—¥æ–‡ã€æ³°è¯­ç­‰ï¼‰æ—¶éå¸¸æœ‰æ•ˆã€‚

4. **å¤„ç†æœªå½’ä¸€åŒ–çš„æ–‡æœ¬**:
   - `SentencePiece` å¯ä»¥ç›´æ¥å¤„ç†æœªç»å½’ä¸€åŒ–çš„åŸå§‹æ–‡æœ¬ï¼ˆå¦‚å¸¦æœ‰æ ‡ç‚¹ç¬¦å·çš„æ–‡æœ¬ï¼‰ï¼Œè¿™åœ¨å®é™…åº”ç”¨ä¸­éå¸¸æœ‰ç”¨ï¼Œé¿å…äº†å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†çš„éœ€æ±‚ã€‚

**ä½¿ç”¨ `SentencePiece` çš„åœºæ™¯**

- **æœºå™¨ç¿»è¯‘**: åœ¨è®­ç»ƒæœºå™¨ç¿»è¯‘æ¨¡å‹æ—¶ï¼Œä½¿ç”¨ `SentencePiece` å¯ä»¥å°†è¾“å…¥å’Œè¾“å‡ºæ–‡æœ¬åˆ†å‰²æˆå­è¯å•å…ƒï¼Œå‡å°‘è¯æ±‡è¡¨å¤§å°ï¼Œå¹¶æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
- **é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹**: è¯¸å¦‚ BERTã€GPTã€T5 ç­‰æ¨¡å‹åœ¨é¢„è®­ç»ƒæ—¶ï¼Œé€šå¸¸ä½¿ç”¨ `SentencePiece` ç”Ÿæˆå­è¯å•å…ƒè¯æ±‡è¡¨ï¼Œè¿™äº›è¯æ±‡è¡¨æœ‰åŠ©äºå¤„ç†å¤šè¯­è¨€æ•°æ®å’Œç¨€æœ‰è¯æ±‡ã€‚
- **æ–‡æœ¬ç”Ÿæˆ**: åœ¨ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œå­è¯å•å…ƒå¯ä»¥æ›´å¥½åœ°è¡¨ç¤ºç¨€æœ‰æˆ–é•¿å°¾è¯æ±‡ï¼Œå‡å°‘ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°çš„OOVé—®é¢˜ã€‚


`SentencePiece` æ˜¯ä¸€ä¸ªå¼ºå¤§çš„åˆ†è¯å·¥å…·ï¼Œå®ƒé€šè¿‡ç”Ÿæˆæ•°æ®é©±åŠ¨çš„å­è¯å•å…ƒè¯æ±‡è¡¨ï¼Œåœ¨ NLP ä»»åŠ¡ä¸­å¹¿æ³›ä½¿ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤šè¯­è¨€æ–‡æœ¬å’Œè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹æ—¶ã€‚
:::

```{python}
translator = pipeline("translation_en_to_zh",
                      model="Helsinki-NLP/opus-mt-en-zh", device=device)
translator("This is a introduction to Huggingface.")
```

ç»“æœå¾ˆä¸€èˆ¬ã€‚è¿™æ˜¯å› ä¸ºç¿»è¯‘ä»»åŠ¡ä¸ä»…éœ€è¦æ¨¡å‹æ”¯æŒï¼Œè¿˜éœ€è¦æœ‰ä¸€ä¸ªä¸­æ–‡çš„åˆ†è¯å™¨ã€‚è€Œé»˜è®¤çš„åˆ†è¯å™¨ä¸é€‚åˆè¿›è¡Œä¸­æ–‡åˆ†è¯çš„ä»»åŠ¡ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬ä¼˜åŒ–ä¸€ä¸‹åˆ†è¯å™¨çš„è®¾ç½®ã€‚

```{python}
from transformers import AutoModelWithLMHead,AutoTokenizer,pipeline
mode_name = 'liam168/trans-opus-mt-en-zh'
model = AutoModelWithLMHead.from_pretrained(mode_name)
tokenizer = AutoTokenizer.from_pretrained(mode_name)
translation = pipeline("translation_en_to_zh", 
                      model=model, 
                      tokenizer=tokenizer, device=device)
translation('This is a introduction to Huggingface.')
```

è®©æˆ‘ä»¬é€è¡Œè§£é‡Šä»£ç çš„å«ä¹‰ï¼š

```python
from transformers import AutoModelWithLMHead, AutoTokenizer, pipeline
```

- **`AutoModelWithLMHead`**: è¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŠ è½½è¯­è¨€æ¨¡å‹ï¼ˆLanguage Modelï¼‰çš„ç±»ï¼Œé€šå¸¸ç”¨äºåŠ è½½å¸¦æœ‰è¯­è¨€å»ºæ¨¡å¤´çš„æ¨¡å‹ã€‚`LMHead` ä»£è¡¨è¯­è¨€æ¨¡å‹çš„è¾“å‡ºå±‚ã€‚
- **`AutoTokenizer`**: è¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŠ è½½é€‚å½“çš„åˆ†è¯å™¨ï¼ˆtokenizerï¼‰çš„ç±»ã€‚åˆ†è¯å™¨ç”¨äºå°†è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„ä»¤ç‰Œï¼ˆtokensï¼‰åºåˆ—ã€‚
- **`pipeline`**: è¿™æ˜¯ Hugging Face çš„é«˜å±‚ APIï¼Œå®ƒæä¾›äº†å„ç§ NLP ä»»åŠ¡çš„é¢„å®šä¹‰ç®¡é“ï¼ˆpipelineï¼‰ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆç­‰ã€‚

```python
mode_name = 'liam168/trans-opus-mt-en-zh'
```

- **`mode_name`**: è¿™æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²å˜é‡ï¼Œå­˜å‚¨äº†æ¨¡å‹çš„åç§°æˆ–è·¯å¾„ã€‚è¿™é‡Œçš„ `liam168/trans-opus-mt-en-zh` æ˜¯åœ¨ Hugging Face æ¨¡å‹åº“ä¸­çš„ä¸€ä¸ªæ¨¡å‹åç§°ï¼Œè¡¨ç¤ºä¸€ä¸ªé¢„è®­ç»ƒçš„ä»è‹±è¯­åˆ°ä¸­æ–‡ç¿»è¯‘çš„æ¨¡å‹ã€‚

```python
model = AutoModelWithLMHead.from_pretrained(mode_name)
```

- **`AutoModelWithLMHead.from_pretrained(mode_name)`**: è¿™è¡Œä»£ç åŠ è½½äº† `liam168/trans-opus-mt-en-zh` æ¨¡å‹çš„é¢„è®­ç»ƒæƒé‡å’Œé…ç½®ã€‚`from_pretrained` æ–¹æ³•ä» Hugging Face çš„æ¨¡å‹åº“ä¸‹è½½ï¼ˆå¦‚æœå°šæœªä¸‹è½½ï¼‰å¹¶åŠ è½½è¯¥æ¨¡å‹åˆ°å†…å­˜ä¸­ã€‚

```python
tokenizer = AutoTokenizer.from_pretrained(mode_name)
```

- **`AutoTokenizer.from_pretrained(mode_name)`**: è¿™è¡Œä»£ç åŠ è½½äº†ä¸æ¨¡å‹é…å¥—çš„åˆ†è¯å™¨ã€‚åˆ†è¯å™¨å°†è¾“å…¥çš„è‹±æ–‡å¥å­è½¬æ¢ä¸ºæ¨¡å‹æ‰€éœ€çš„ä»¤ç‰Œï¼ˆtokensï¼‰ï¼Œå¹¶ä¸”ä¼šæ‰§è¡Œå¿…è¦çš„æ–‡æœ¬é¢„å¤„ç†ã€‚

```python
translation = pipeline("translation_en_to_zh", model=model, tokenizer=tokenizer)
```

- **`pipeline("translation_en_to_zh", model=model, tokenizer=tokenizer)`**: è¿™é‡Œä½¿ç”¨äº† `pipeline` å‡½æ•°æ¥åˆ›å»ºä¸€ä¸ªç¿»è¯‘ç®¡é“ï¼ŒæŒ‡å®šäº†ä»»åŠ¡ç±»å‹ä¸º `"translation_en_to_zh"`ï¼ˆä»è‹±è¯­åˆ°ä¸­æ–‡çš„ç¿»è¯‘ï¼‰ï¼Œå¹¶ä¼ å…¥äº†ä¹‹å‰åŠ è½½çš„æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚è¿™ä¸ªç®¡é“å°è£…äº†ç¿»è¯‘ä»»åŠ¡çš„æ‰€æœ‰æ­¥éª¤ï¼Œä½¿å¾—ç¿»è¯‘æ–‡æœ¬å˜å¾—ç®€å•ä¸”æ˜“äºä½¿ç”¨ã€‚

```python
translation('This is a introduction to Huggingface.')
```

- **`translation('This is a introduction to Huggingface.')`**: è¿™è¡Œä»£ç è°ƒç”¨äº†ç¿»è¯‘ç®¡é“ï¼Œå°†è¾“å…¥çš„è‹±æ–‡å¥å­ `"This is a introduction to Huggingface."` ç¿»è¯‘ä¸ºä¸­æ–‡ã€‚ç®¡é“ä¼šè‡ªåŠ¨æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š
  1. ä½¿ç”¨ `tokenizer` å°†è¾“å…¥çš„è‹±æ–‡å¥å­åˆ†è¯ä¸ºä»¤ç‰Œã€‚
  2. å°†ä»¤ç‰Œè¾“å…¥åˆ° `model` ä¸­è¿›è¡Œç¿»è¯‘ã€‚
  3. ç”Ÿæˆçš„ä¸­æ–‡ä»¤ç‰Œåºåˆ—è¢«è§£ç æˆè‡ªç„¶è¯­è¨€æ–‡æœ¬ã€‚

æœ€ç»ˆè¾“å‡ºä¼šæ˜¯ `"This is a introduction to Huggingface."` çš„ä¸­æ–‡ç¿»è¯‘ç‰ˆæœ¬ï¼Œä¾‹å¦‚ `"è¿™æ˜¯å¯¹ Huggingface çš„ä»‹ç»ã€‚"`ï¼ˆå…·ä½“ç¿»è¯‘ç»“æœå¯èƒ½æœ‰æ‰€ä¸åŒï¼Œå–å†³äºæ¨¡å‹çš„æ€§èƒ½ï¼‰ã€‚

è¿™æ®µä»£ç é€šè¿‡åŠ è½½ Hugging Face æä¾›çš„é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œå®ç°äº†ä»è‹±è¯­åˆ°ä¸­æ–‡çš„è‡ªåŠ¨ç¿»è¯‘ä»»åŠ¡ï¼Œå¹¶ä¸”é€šè¿‡ä½¿ç”¨é«˜å±‚çš„ `pipeline` APIï¼Œç®€åŒ–äº†ç¿»è¯‘ä»»åŠ¡çš„æ‰§è¡Œã€‚


### ä¸¤ç§è°ƒç”¨æ¨¡å‹çš„æ–¹å¼

æŸ¥æ‰¾ GPU è®¾å¤‡ã€‚

```{python}
import torch
from pprint import pprint

# select the device for computation
if torch.cuda.is_available():
    device = torch.device("cuda")
elif torch.backends.mps.is_available():
    device = torch.device("mps")
else:
    device = torch.device("cpu")

print(f"using device: {device}")
```

å›¾ç‰‡æ•°æ®ã€‚

```{python}
import io
import requests
from PIL import Image

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
```

#### ä½¿ç”¨ Pipeline

```{python}
#| cache: true
# Use a pipeline as a high-level helper
from transformers import pipeline

object_detector = pipeline("object-detection", model="facebook/detr-resnet-50", device=device)

detection_results = object_detector(image)
pprint(detection_results)
```


#### ç›´æ¥ä½¿ç”¨æ¨¡å‹

```{python}
# Load model directly
from transformers import AutoImageProcessor, AutoModelForObjectDetection

image_processor = AutoImageProcessor.from_pretrained("facebook/detr-resnet-50", device=device)
model = AutoModelForObjectDetection.from_pretrained("facebook/detr-resnet-50")
```


### å¯¹è±¡æ£€æµ‹

DEtection TRansformerï¼ˆDETRï¼‰æ¨¡å‹ï¼Œé€šè¿‡ç«¯åˆ°ç«¯è®­ç»ƒåœ¨ COCO 2017 å¯¹è±¡æ£€æµ‹æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼ˆåŒ…å« 118K å¼ æ ‡æ³¨å›¾åƒï¼‰ã€‚

DETR æ¨¡å‹æ˜¯ä¸€ç§å…·æœ‰å·ç§¯éª¨å¹²çš„ç¼–ç å™¨-è§£ç å™¨å˜æ¢å™¨ã€‚åœ¨è§£ç å™¨è¾“å‡ºä¹‹ä¸Šæ·»åŠ äº†ä¸¤ä¸ªå¤´éƒ¨ï¼Œä»¥æ‰§è¡Œå¯¹è±¡æ£€æµ‹ï¼šä¸€ä¸ªçº¿æ€§å±‚ç”¨äºç±»åˆ«æ ‡ç­¾ï¼Œä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ç”¨äºè¾¹ç•Œæ¡†ã€‚è¯¥æ¨¡å‹ä½¿ç”¨æ‰€è°“çš„å¯¹è±¡æŸ¥è¯¢æ¥æ£€æµ‹å›¾åƒä¸­çš„å¯¹è±¡ã€‚æ¯ä¸ªå¯¹è±¡æŸ¥è¯¢éƒ½åœ¨å¯»æ‰¾å›¾åƒä¸­çš„ç‰¹å®šå¯¹è±¡ã€‚å¯¹äº COCOï¼Œè®¾ç½®çš„å¯¹è±¡æŸ¥è¯¢æ•°é‡ä¸º 100ã€‚

æ¨¡å‹ä½¿ç”¨â€œäºŒéƒ¨åŒ¹é…æŸå¤±â€è¿›è¡Œè®­ç»ƒï¼šå°†é¢„æµ‹çš„ N=100 ä¸ªå¯¹è±¡æŸ¥è¯¢ä¸­çš„æ¯ä¸ªç±»åˆ«çš„é¢„æµ‹æ¡†ä¸ ground truth æ³¨é‡Šè¿›è¡Œæ¯”è¾ƒï¼Œå¡«å……åˆ°ç›¸åŒçš„é•¿åº¦ Nï¼ˆå¦‚æœä¸€å¼ å›¾ç‰‡åªåŒ…å« 4 ä¸ªå¯¹è±¡ï¼Œé‚£ä¹ˆ 96 ä¸ªæ³¨é‡Šå°†åªæ˜¯â€œæ— å¯¹è±¡â€ä½œä¸ºç±»åˆ«ï¼Œâ€œæ— æ¡†â€ä½œä¸ºæ¡†ï¼‰ã€‚åŒˆç‰™åˆ©åŒ¹é…ç®—æ³•ç”¨äºåœ¨æ¯ä¸ª N æŸ¥è¯¢å’Œæ¯ä¸ª N æ³¨é‡Šä¹‹é—´åˆ›å»ºæœ€ä¼˜çš„ä¸€å¯¹ä¸€æ˜ å°„ã€‚æ¥ä¸‹æ¥ï¼Œä½¿ç”¨æ ‡å‡†äº¤å‰ç†µï¼ˆå¯¹äºç±»åˆ«ï¼‰ä»¥åŠ L1 å’Œé€šç”¨ IoU æŸå¤±çš„çº¿æ€§ç»„åˆï¼ˆå¯¹äºæ¡†ï¼‰æ¥ä¼˜åŒ–æ¨¡å‹å‚æ•°ã€‚

ä¸‹é¢æ˜¯ä½¿ç”¨ç¬¬ä¸€ç§è°ƒç”¨æ–¹å¼è°ƒç”¨ DETR æ¨¡å‹æ—¶ç»“æœçš„å¤„ç†ç¤ºä¾‹ã€‚

```{python}
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np
import matplotlib.patches as patches

def random_color():
    """Generate a random color."""
    return np.random.rand(3,)

# Create a figure and axis for plotting
fig, ax = plt.subplots(1, 1, figsize=(12, 8))

# Display the original image
ax.imshow(image)

# Overlay bounding boxes and labels with random colors
for result in detection_results:
    score = result['score']
    label = result['label']
    box = result['box']
    
    # Generate a random color
    color = random_color()
    
    # Draw bounding box
    rect = patches.Rectangle(
        (box['xmin'], box['ymin']),
        box['xmax'] - box['xmin'],
        box['ymax'] - box['ymin'],
        linewidth=2,
        edgecolor=color,
        facecolor='none'
    )
    ax.add_patch(rect)
    
    # Draw label and score with the same color as the rectangle
    label_text = f"{label}: {score:.2f}"
    ax.text(
        box['xmin'],
        box['ymin'] - 10,
        label_text,
        color=color,
        fontsize=12,
        bbox=dict(facecolor='white', alpha=0.5, 
                  edgecolor=color, boxstyle='round,pad=0.5')
    )


# Hide axis
plt.axis('off')

# Show the plot with bounding boxes and labels
plt.show()

```


ä¸‹é¢æ˜¯å¯¹ç¬¬äºŒç§è°ƒç”¨æ–¹å¼ç»“æœå¤„ç†çš„æ–¹æ³•ã€‚

```{python}
#| cache: true
# prepare image for the model
inputs = image_processor(images=image, return_tensors="pt")

# forward pass
outputs = model(**inputs)

# convert outputs (bounding boxes and class logits) to COCO API
# let's only keep detections with score > 0.9
target_sizes = torch.tensor([image.size[::-1]])
results = image_processor.post_process_object_detection(
    outputs, 
    target_sizes=target_sizes, 
    threshold=0.9)[0]

for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
    box = [round(i, 2) for i in box.tolist()]
    print(
            f"Detected {model.config.id2label[label.item()]} with confidence "
            f"{round(score.item(), 3)} at location {box}"
    )
```


### YOLOv8

YOLOv8 éœ€è¦ä½¿ç”¨ pip æˆ–è€… conda å®‰è£…ã€‚å®‰è£…åæä¾› cli å’Œ Python ç­‰ä¸¤ç§è¿è¡Œæ–¹å¼ã€‚è¯¦æƒ…å‚è§ï¼šhttps://docs.ultralytics.com/quickstart/ã€‚


### ViT å›¾åƒåˆ†ç±»

The Vision Transformerï¼ˆViTï¼‰æ˜¯ä¸€ç§ä»¥ç›‘ç£æ–¹å¼åœ¨å¤§é‡å›¾åƒé›†åˆï¼ˆå³ ImageNet - 21kï¼‰ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ Transformer ç¼–ç å™¨æ¨¡å‹ï¼ˆç±»ä¼¼äº BERTï¼‰ï¼Œå›¾åƒåˆ†è¾¨ç‡ä¸º 224x224 åƒç´ ã€‚æ¥ä¸‹æ¥ï¼Œè¯¥æ¨¡å‹åœ¨ ImageNetï¼ˆä¹Ÿç§°ä¸º ILSVRC2012ï¼‰ä¸Šè¿›è¡Œå¾®è°ƒï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 100 ä¸‡å¼ å›¾åƒå’Œ 1000 ä¸ªç±»åˆ«çš„æ•°æ®é›†ï¼Œå›¾åƒåˆ†è¾¨ç‡åŒæ ·ä¸º 224x224ã€‚

å›¾åƒä½œä¸ºä¸€ç³»åˆ—å›ºå®šå¤§å°çš„è¡¥ä¸ï¼ˆåˆ†è¾¨ç‡ä¸º 16x16ï¼‰å‘ˆç°ç»™æ¨¡å‹ï¼Œè¿™äº›è¡¥ä¸æ˜¯çº¿æ€§åµŒå…¥çš„ã€‚è¿˜åœ¨åºåˆ—å¼€å¤´æ·»åŠ ä¸€ä¸ª[CLS]æ ‡è®°ï¼Œç”¨äºåˆ†ç±»ä»»åŠ¡ã€‚åœ¨å°†åºåˆ—è¾“å…¥åˆ° Transformer ç¼–ç å™¨çš„å±‚ä¹‹å‰ï¼Œè¿˜æ·»åŠ äº†ç»å¯¹ä½ç½®åµŒå…¥ã€‚

é€šè¿‡å¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œå®ƒå­¦ä¹ åˆ°å›¾åƒçš„å†…éƒ¨è¡¨ç¤ºï¼Œç„¶åå¯ç”¨äºæå–å¯¹ä¸‹æ¸¸ä»»åŠ¡æœ‰ç”¨çš„ç‰¹å¾ï¼šä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰ä¸€ä¸ªå¸¦æ ‡ç­¾å›¾åƒçš„æ•°æ®é›†ï¼Œåˆ™å¯ä»¥é€šè¿‡åœ¨é¢„è®­ç»ƒçš„ç¼–ç å™¨é¡¶éƒ¨æ”¾ç½®ä¸€ä¸ªçº¿æ€§å±‚æ¥è®­ç»ƒæ ‡å‡†åˆ†ç±»å™¨ã€‚é€šå¸¸ä¼šåœ¨ [CLS] æ ‡è®°çš„é¡¶éƒ¨æ”¾ç½®ä¸€ä¸ªçº¿æ€§å±‚ï¼Œå› ä¸ºæ­¤æ ‡è®°çš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€å¯ä»¥è§†ä¸ºæ•´ä¸ªå›¾åƒçš„è¡¨ç¤ºã€‚

```{python}
image_classifier = pipeline("image-classification", 
                            model="google/vit-base-patch16-224", 
                            device=device)

class_results = image_classifier(image)
pprint(class_results)
```


### ViT ç‰¹å¾æå–

è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ¨¡å‹åœ¨ ImageNet-21k ä¸Šé¢„è®­ç»ƒï¼ŒåŒ…å« 1.4 äº¿å¼ å›¾ç‰‡å’Œ 21843 ä¸ªç±»åˆ«ã€‚å…¶åˆ†è¾¨ç‡ä¸º 224 x 224ã€‚


```{python}
#| cache: true
from transformers import ViTImageProcessor, ViTModel
from PIL import Image
import requests

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')
model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')
inputs = processor(images=image, return_tensors="pt")

outputs = model(**inputs)
last_hidden_states = outputs.last_hidden_state
```

`BaseModelOutputWithPooling` æ˜¯ Hugging Face çš„ `transformers` åº“ä¸­çš„ä¸€ä¸ªç±»ï¼Œç”¨äºæ¨¡å‹è¾“å‡ºçš„è¡¨ç¤ºã€‚è¿™ä¸ªç±»é€šå¸¸åœ¨æ¨¡å‹è¿”å›çš„è¾“å‡ºä¸­åŒ…å«äº†æ± åŒ–å±‚çš„ç»“æœï¼Œè¿™å¯¹äºä¸€äº›ä»»åŠ¡ï¼Œæ¯”å¦‚æ–‡æœ¬åˆ†ç±»æˆ–åµŒå…¥ç”Ÿæˆï¼Œç‰¹åˆ«æœ‰ç”¨ã€‚

#### ä¸»è¦åŠŸèƒ½

`BaseModelOutputWithPooling` ç±»æ˜¯ä» `BaseModelOutput` æ´¾ç”Ÿè€Œæ¥çš„ï¼Œå®ƒåŒ…å«ä»¥ä¸‹å‡ ä¸ªé‡è¦çš„ç»„ä»¶ï¼š

- **`last_hidden_state`**ï¼šæ¨¡å‹åœ¨æ‰€æœ‰éšè—å±‚çš„è¾“å‡ºï¼Œè¿™äº›è¾“å‡ºé€šå¸¸ç”¨äºè·å–åºåˆ—çš„ç‰¹å¾è¡¨ç¤ºã€‚
- **`pooler_output`**ï¼šç»è¿‡æ± åŒ–å±‚ï¼ˆé€šå¸¸æ˜¯æ± åŒ–åçš„ç¬¬ä¸€ä¸ª tokenï¼‰çš„è¾“å‡ºï¼Œç”¨äºè·å¾—åºåˆ—çš„æ•´ä½“è¡¨ç¤ºã€‚å¯¹äº BERT ç­‰æ¨¡å‹ï¼Œè¿™é€šå¸¸æ˜¯ `[CLS]` token çš„è¾“å‡ºç»è¿‡æ± åŒ–æ“ä½œçš„ç»“æœã€‚
- **`hidden_states`**ï¼ˆå¯é€‰ï¼‰ï¼šæ¨¡å‹åœ¨æ¯ä¸ªéšè—å±‚çš„è¾“å‡ºï¼ˆå¦‚æœ `output_hidden_states=True` æ—¶ä¼šè¿”å›ï¼‰ã€‚

#### ç”¨é€”

- **`pooler_output`**ï¼šè¿™ä¸ªè¾“å‡ºæ˜¯ç”¨æ¥è·å–åºåˆ—çš„æ•´ä½“è¡¨ç¤ºçš„ï¼Œä¾‹å¦‚ç”¨äºåˆ†ç±»ä»»åŠ¡ã€‚å¯¹äºå¾ˆå¤šé¢„è®­ç»ƒæ¨¡å‹æ¥è¯´ï¼Œè¿™ä¸ªè¾“å‡ºæ˜¯å¯¹ `[CLS]` token çš„è¡¨ç¤ºç»è¿‡æ± åŒ–åçš„ç»“æœã€‚
- **`last_hidden_state`**ï¼šå¦‚æœä½ éœ€è¦å¯¹æ¯ä¸ª token çš„è¡¨ç¤ºè¿›è¡Œè¿›ä¸€æ­¥çš„å¤„ç†æˆ–åˆ†æï¼ˆä¾‹å¦‚ï¼Œè¿›è¡Œåºåˆ—æ ‡æ³¨ä»»åŠ¡ï¼‰ï¼Œè¿™ä¸ªè¾“å‡ºå°†æ˜¯æœ‰ç”¨çš„ã€‚

#### ç¤ºä¾‹

ä»¥ä¸‹æ˜¯å¦‚ä½•åœ¨ä½¿ç”¨ Hugging Face æ¨¡å‹æ—¶ï¼Œåˆ©ç”¨ `BaseModelOutputWithPooling` è·å–æ¨¡å‹è¾“å‡ºçš„ä¸€ä¸ªä¾‹å­ï¼š

```{python}
# Extract the output
last_hidden_state = outputs.last_hidden_state  # Shape: [batch_size, sequence_length, hidden_size]
pooler_output = outputs.pooler_output  # Shape: [batch_size, hidden_size]

print("Last hidden state:", last_hidden_state.shape)
print("Pooler output:", pooler_output.shape)
```

#### è¯´æ˜ï¼š

1. **`last_hidden_state`**ï¼šé€šå¸¸æ˜¯ä¸‰ç»´å¼ é‡ï¼Œå½¢çŠ¶ä¸º `[batch_size, sequence_length, hidden_size]`ã€‚
2. **`pooler_output`**ï¼šé€šå¸¸æ˜¯äºŒç»´å¼ é‡ï¼Œå½¢çŠ¶ä¸º `[batch_size, hidden_size]`ï¼Œç”¨äºè¡¨ç¤ºæ•´ä¸ªåºåˆ—çš„ç‰¹å¾ã€‚

`BaseModelOutputWithPooling` æ˜¯ä¸€ä¸ªç»“æ„åŒ–çš„è¿”å›å¯¹è±¡ï¼Œå¸®åŠ©ä½ ä»æ¨¡å‹ä¸­æå–æœ‰ç”¨çš„ç‰¹å¾è¡¨ç¤ºï¼Œç‰¹åˆ«æ˜¯å½“éœ€è¦å¤„ç†åºåˆ—æ•°æ®æ—¶ã€‚

## ChatAnyWhere æœåŠ¡

ChatAnyWhere ä¸»è¦æä¾› OpenAI å¤§æ¨¡å‹æ¥å…¥æœåŠ¡ï¼ˆä¹ŸåŒ…æ‹¬å°‘é‡é OpenAI çš„æ¨¡å‹ï¼‰ã€‚

åŸºæœ¬ä¿¡æ¯å¦‚ä¸‹ï¼š

-   æœåŠ¡å™¨åœ°å€ï¼š<https://api.chatanywhere.tech>
-   API KEY è´­ä¹°ä¸ç»­è´¹ï¼š<https://peiqishop.me>
-   API KEY ä½™é‡æŸ¥è¯¢ï¼š<https://api.chatanywhere.org>

### æ¨¡å‹åˆ—è¡¨

ChatAnyWhere æä¾›çš„æ¨¡å‹åå­—å¯èƒ½ä¼šä¸ OpenAI ä¸åŒã€‚ä¾‹å¦‚ `*-ca` æ¨¡å‹ä¼šæœ‰æ›´ä¼˜æƒ çš„ä»·æ ¼ã€‚ä¸ºäº†é…ç½®è°ƒç”¨å‚æ•°æ—¶å†™å¯¹æ¨¡å‹åç§°ï¼Œéœ€è¦æŸ¥è¯¢æä¾›çš„æ¨¡å‹åˆ—è¡¨ã€‚

```{r}
library(httr)
library(glue)

# è¯»å– CHATANYWHERE_API_KEY ç¯å¢ƒå˜é‡
OPENAI_API_KEY = Sys.getenv("CHATANYWHERE_API_KEY")

headers = c(
   'Authorization' = glue('Bearer {OPENAI_API_KEY}'),
   'User-Agent' = 'Apifox/1.0.0 (https://apifox.com)',
   'Content-Type' = 'application/json'
)

# ä½¿ç”¨ GET æ–¹æ³•è·å–
res <- VERB("GET", 
            url = "https://api.chatanywhere.tech/v1/models", 
            add_headers(headers))
```

å°† JSON è¾“å‡ºä¸ºè¡¨æ ¼ã€‚


```{r}
library(jsonlite)
library(tidyverse)
library(gt)

models = jsonlite::fromJSON(content(res, as = "text", encoding = "UTF-8"))$data

models |> 
  mutate(created = as_datetime(created) |> as_date()) |> 
  gt(groupname_col = "owned_by")
```

ä¸‹é¢ä¾æ¬¡ä»‹ç»è¿™äº›æ¨¡å‹çš„ç”¨æ³•ã€‚

### å¯¹è¯æ¨¡å‹

ä»¥ `gpt-*` å¼€å¤´çš„éƒ½æ˜¯æ–‡æœ¬å¯¹è¯æ¨¡å‹ã€‚è°ƒç”¨ OpenAI çš„æ¨¡å‹æ—¶ï¼Œé€šå¸¸éœ€è¦é…ç½®ä»¥ä¸‹ä¸€äº›å…³é”®å‚æ•°æ¥æ§åˆ¶æ¨¡å‹çš„è¡Œä¸ºå’Œç”Ÿæˆç»“æœçš„æ–¹å¼ã€‚

1.  **Promptï¼ˆæç¤ºï¼‰**
    -   æ¨¡å‹çš„è¾“å…¥æ–‡æœ¬ï¼Œé€šå¸¸ç§°ä¸ºâ€œæç¤ºâ€ã€‚
    -   å¯ä»¥æ˜¯ç®€å•çš„æ–‡æœ¬ï¼Œæˆ–è€…å¸¦æœ‰ä¸€äº›é—®é¢˜æˆ–ä»»åŠ¡æè¿°ï¼Œå‘Šè¯‰æ¨¡å‹ç”Ÿæˆå“ªç±»å†…å®¹ã€‚
2.  **Max Tokensï¼ˆæœ€å¤§ä»¤ç‰Œæ•°ï¼‰**
    -   æŒ‡å®šç”Ÿæˆçš„æ–‡æœ¬ä¸­æœ€å¤šåŒ…å«å¤šå°‘ä¸ªä»¤ç‰Œï¼ˆtokensï¼‰ã€‚ä¸€ä¸ªä»¤ç‰Œå¤§çº¦å¯¹åº”ä¸€ä¸ªè‹±æ–‡å•è¯æˆ–æ ‡ç‚¹ç¬¦å·ã€‚
    -   è¯¥å‚æ•°å¯ä»¥æ§åˆ¶ç”Ÿæˆçš„å“åº”é•¿åº¦ï¼Œä½†åŒ…æ‹¬è¾“å…¥å’Œè¾“å‡ºåœ¨å†…çš„ä»¤ç‰Œæ€»æ•°ä¸èƒ½è¶…è¿‡æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ã€‚
3.  **Temperatureï¼ˆæ¸©åº¦ï¼‰**
    -   æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ã€‚èŒƒå›´æ˜¯ `0` åˆ° `2`ï¼š
        -   `temperature=0` æ—¶ï¼Œè¾“å‡ºæ›´åŠ ç¡®å®šå’Œä¿å®ˆï¼Œåå‘ç”Ÿæˆå¸¸è§çš„æˆ–â€œæœ€å¯èƒ½â€çš„ç­”æ¡ˆã€‚
        -   è¾ƒé«˜çš„ `temperature` å€¼ï¼ˆå¦‚ `0.7`ï¼‰ä¼šè®©ç”Ÿæˆçš„å†…å®¹æ›´åŠ éšæœºå’Œå¤šæ ·åŒ–ã€‚
4.  **nï¼ˆç”Ÿæˆæ¬¡æ•°ï¼‰**
    -   æ§åˆ¶ç”Ÿæˆå¤šå°‘ä¸ªä¸åŒçš„å“åº”ã€‚
    -   `n=1` åªç”Ÿæˆä¸€ä¸ªå“åº”ï¼›`n=2` ä¼šç”Ÿæˆå¤šä¸ªå“åº”ï¼Œé€‚åˆæ¯”è¾ƒæˆ–é€‰æ‹©æœ€åˆé€‚çš„å†…å®¹ã€‚

è°ƒç”¨ OpenAI æ¨¡å‹æ—¶ï¼Œé€šå¸¸éœ€è¦è®¾ç½® **æ¨¡å‹åç§°ã€æç¤ºã€æœ€å¤§ä»¤ç‰Œæ•°ã€æ¸©åº¦ã€top-pã€ç”Ÿæˆæ¬¡æ•°** ç­‰å‚æ•°ï¼Œè§†ä»»åŠ¡éœ€æ±‚è¿˜å¯ä»¥è°ƒæ•´ **å‡ºç°æƒ©ç½šã€é¢‘ç‡æƒ©ç½šã€åœæ­¢åºåˆ—** ç­‰å…¶ä»–é…ç½®ï¼Œä»¥æ§åˆ¶ç”Ÿæˆçš„å†…å®¹è´¨é‡å’Œè¡Œä¸ºã€‚

ä¸‹é¢è¿™ä¸ªä¾‹å­ï¼Œå±•ç¤ºäº† ChatGPT æ•°ä¸æ¸…æ¥šâ€œtemperatureâ€è¿™ä¸ªå•è¯é‡Œé¢æœ‰å‡ ä¸ªå­—æ¯â€œeâ€ã€‚

```{r}
body = '{
   "model": "gpt-4o-mini",
   "messages": [
      {
         "role": "system",
         "content": "You are a helpful assistant."
      },
      {
         "role": "user",
         "content": "Temperatureè¿™ä¸ªå•è¯ä¸­å«æœ‰å‡ ä¸ªå­—æ¯eï¼Ÿ"
      }
   ],
   "temperature": 2
}';

res <- VERB("POST", 
            url = "https://api.chatanywhere.tech/v1/chat/completions", 
            body = body, 
            add_headers(headers))

content(res, 'text', encoding = "UTF-8") |> 
  fromJSON() |> 
  str()
```

### ä½¿ç”¨ OpenAI API

```{python}
from openai import OpenAI
import os

# åˆ›å»º client
client = OpenAI(
    api_key=os.getenv("CHATANYWHERE_API_KEY"), # å¦‚æœæ‚¨æ²¡æœ‰é…ç½®ç¯å¢ƒå˜é‡ï¼Œè¯·åœ¨æ­¤å¤„ç”¨æ‚¨çš„API Keyè¿›è¡Œæ›¿æ¢
    base_url="https://api.chatanywhere.tech",  # å¡«å†™ openAI æœåŠ¡çš„ base_url
)

# ç”Ÿæˆå¯¹è¯
completion = client.chat.completions.create(
    model="gpt-4o-ca",
    messages=[
        {'role': 'system', 'content': 'You are a helpful assistant.'},
        {'role': 'user', 'content': 'ä½ æ˜¯è°'}],
    temperature=0.8,
    top_p=0.8
    )

print(completion.choices[0].message.content)
```


### è¯åµŒå…¥æ¨¡å‹

æ‰€æœ‰ä¸‰ä¸ªè¯åµŒå…¥æ¨¡å‹éƒ½æ˜¯åŸºäº Transformer æ¶æ„ï¼Œè¿™ä½¿å¾—å®ƒä»¬åœ¨å¤„ç†è‡ªç„¶è¯­è¨€æ—¶å…·æœ‰è‰¯å¥½çš„æ€§èƒ½ã€‚ä¸‹è¡¨æ¯”è¾ƒäº† `text-embedding-ada-002`ã€`text-embedding-3-small` å’Œ `text-embedding-3-large` è¿™ä¸‰ä¸ªè¯åµŒå…¥æ¨¡å‹çš„ç‰¹ç‚¹ï¼š

| **æ¨¡å‹åç§°**             | **å‚æ•°é‡** | **åµŒå…¥ç»´åº¦** | **æ€§èƒ½**           | **é€‚ç”¨åœºæ™¯**                             | **ä¼˜ç‚¹**                             | **ç¼ºç‚¹**                           |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| `text-embedding-ada-002` | ä¸­ç­‰       | 1536         | é«˜æ•ˆï¼Œæ€§èƒ½ä¼˜ç§€     | é€šç”¨æ–‡æœ¬åµŒå…¥ï¼Œé€‚ç”¨äºå¹¿æ³›çš„ NLP ä»»åŠ¡      | é«˜ç²¾åº¦åµŒå…¥ï¼Œé€‚åˆå„ç§è¯­ä¹‰åŒ¹é…ä»»åŠ¡     | ç›¸æ¯”å°å‹æ¨¡å‹ï¼Œè®¡ç®—èµ„æºéœ€æ±‚è¾ƒé«˜     |
| `text-embedding-3-small` | å°         | 512          | è¾ƒå¿«ï¼Œèµ„æºæ•ˆç‡é«˜   | èµ„æºå—é™çš„åº”ç”¨åœºæ™¯ï¼Œä½è®¡ç®—æˆæœ¬çš„åµŒå…¥ç”Ÿæˆ | è®¡ç®—æ•ˆç‡é«˜ï¼Œé€‚åˆå®æ—¶æˆ–èµ„æºæœ‰é™çš„åœºæ™¯ | åµŒå…¥ç»´åº¦è¾ƒä½ï¼Œå¯èƒ½å½±å“è¯­ä¹‰è¡¨è¾¾èƒ½åŠ› |
| `text-embedding-3-large` | å¤§         | 2048         | æ›´é«˜æ€§èƒ½ï¼Œç²¾åº¦æé«˜ | é«˜ç«¯åº”ç”¨åœºæ™¯ï¼Œå¦‚é«˜ç²¾åº¦è¯­ä¹‰æœç´¢å’Œæ¨èç³»ç»Ÿ | åµŒå…¥ç»´åº¦æ›´é«˜ï¼Œèƒ½å¤Ÿæ•æ‰å¤æ‚è¯­ä¹‰å…³ç³»   | èµ„æºæ¶ˆè€—å¤§ï¼Œé€‚åˆè®¡ç®—èµ„æºå……è¶³çš„åœºæ™¯ |

```{r}
body = '{
   "model": "text-embedding-ada-002",
   "input": "The food was delicious and the waiter..."
}';

res = VERB("POST", 
           url = "https://api.chatanywhere.tech/v1/embeddings", 
           body = body, 
           add_headers(headers))

content = content(res, 'text', encoding = "UTF-8")

embedding = fromJSON(content)
str(embedding)
```

### æ–‡ç”Ÿå›¾æ¨¡å‹

`dall-e-2` å’Œ `dall-e-3` æ˜¯æ–‡ç”Ÿå›¾æ¨¡å‹ã€‚

DALL-E 2 æ”¯æŒä»¥ä¸‹ä¸‰ç§å›¾åƒå°ºå¯¸ï¼š

1.  **256x256**
2.  **512x512**
3.  **1024x1024**

DALL-E 3 æ”¯æŒä»¥ä¸‹å›¾åƒå°ºå¯¸ï¼š

1.  **1024x1024**: æ­£æ–¹å½¢å›¾åƒï¼Œé€‚åˆå¤§å¤šæ•°ä½¿ç”¨åœºæ™¯ï¼Œæ˜¯é»˜è®¤æ¨èçš„å°ºå¯¸ã€‚
2.  **1792x1024**: å®½å±å›¾åƒï¼Œé€‚åˆéœ€è¦æ›´å®½è§†é‡çš„åœºæ™¯æˆ–æ¨ªå‘å¸ƒå±€çš„è®¾è®¡ã€‚
3.  **1024x1792**: çºµå‘å›¾åƒï¼Œé€‚åˆéœ€è¦æ›´é«˜è§†é‡çš„åœºæ™¯æˆ–çºµå‘å¸ƒå±€çš„è®¾è®¡ã€‚

ä½ å¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©åˆé€‚çš„å›¾åƒå°ºå¯¸è¿›è¡Œç”Ÿæˆã€‚

```{r}
url = "https://api.chatanywhere.tech/v1/images/generations"

body = '{
   "prompt": "A colorful sunset over the snow mountains",
   "n": 1,
   "model":  "dall-e-2",
   "size": "256x256"
}';

response = VERB("POST", url, body = body, add_headers(headers))

content = content(response, "text", encoding = "UTF-8")
print(content)
```

è·å–å›¾ç‰‡ã€‚

```{r}
#| results: asis

# è·å–ç”Ÿæˆçš„å›¾åƒ URL
image_url = fromJSON(content)[["data"]][["url"]]

# ä¸‹è½½å›¾ç‰‡
response <- GET(image_url)

# æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
if (status_code(response) == 200) {
  # å°†å›¾ç‰‡ä¿å­˜åˆ°ç£ç›˜
  writeBin(content(response, "raw"), "output/sunset.png")
  cat("å›¾ç‰‡å·²æˆåŠŸä¿å­˜åˆ° `output/sunset.png`ã€‚")
} else {
  cat("ä¸‹è½½å›¾ç‰‡å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š", status_code(response), "\n")
}
```


```{r}
#| echo: false
knitr::include_graphics("output/sunset.png")
```


### è¯†å›¾åŠŸèƒ½ï¼ˆä¸æ”¯æŒï¼‰

ä½¿ç”¨å¤šæ¨¡æ€æ¨¡å‹ï¼Œå¯ä»¥è¯†åˆ«å›¾ç‰‡ä¸­çš„ä¿¡æ¯ã€‚

```{r}
# è®¾ç½®è¯·æ±‚ä½“
body = list(
  model = "gpt-4o-ca",
  file = upload_file("output/sunset.png"),  # æ–‡ä»¶è·¯å¾„
  prompt = "è¿™æ˜¯ä»€ä¹ˆ?",  # æç¤º
  encode = "multipart"
  )

res = VERB("POST", 
            url = "https://api.chatanywhere.tech/v1/chat/completions", 
            body = body, 
            add_headers(headers))

cat(content(res, 'text', encoding = "UTF-8"))
```



### æ–‡å­—è½¬è¯­éŸ³æ¨¡å‹

å°†ä¸€æ®µæ–‡å­—è½¬å˜ä¸ºè¯­éŸ³ï¼Œæ”¯æŒä¸­è‹±æ–‡æ··åˆã€‚

```{r}
body = '{
   "model": "tts-1",
   "input": "ä»Šå¤©å¤©æ°”ä¸é”™ã€‚It is a nice day today.",
   "voice": "alloy"
}';

res <- VERB("POST", 
            url = "https://api.chatanywhere.tech/v1/audio/speech", 
            body = body, 
            add_headers(headers))

# æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
if (status_code(res) == 200) {
  # å°†å“åº”ä¿å­˜ä¸ºéŸ³é¢‘æ–‡ä»¶ï¼ˆå‡è®¾è¿”å›çš„æ˜¯äºŒè¿›åˆ¶éŸ³é¢‘æ•°æ®ï¼‰
  audio_file <- "output/audio-goodday.mp3"  # ä½ å¯ä»¥æ›´æ”¹æ–‡ä»¶åå’Œæ‰©å±•å
  writeBin(content(res, "raw"), audio_file)
  message("Audio saved successfully as: ", audio_file)
} else {
  message("Request failed with status: ", status_code(res))
}
```


### è¯­éŸ³è¯†åˆ«æ¨¡å‹

`whisper-1` æ˜¯ OpenAI å¼€å‘çš„ä¸€ä¸ªå¼ºå¤§çš„è¯­éŸ³è¯†åˆ«æ¨¡å‹ã€‚å®ƒä¸»è¦ç”¨äºå°†è¯­éŸ³è½¬æ¢ä¸ºæ–‡æœ¬ï¼ˆä¹Ÿç§°ä¸ºè¯­éŸ³è½¬æ–‡å­—ï¼ŒSpeech-to-Textï¼Œç®€ç§° STTï¼‰ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿå¤„ç†å¤šç§è¯­è¨€çš„è¯­éŸ³è¾“å…¥ï¼Œå¹¶èƒ½å¤Ÿè¯†åˆ«ä¸åŒçš„å£éŸ³å’Œè¯­éŸ³é£æ ¼ï¼Œéå¸¸é€‚ç”¨äºå„ç§éŸ³é¢‘è½¬å½•ä»»åŠ¡ã€‚

```{r}
headers_multipart = c(
   'Authorization' = glue('Bearer {OPENAI_API_KEY}'),
   'User-Agent' = 'Apifox/1.0.0 (https://apifox.com)',
   'Content-Type' = 'multipart/form-data'
)

body = list(
   'file' = upload_file('output/audio-goodday.mp3'),
   'model' = 'whisper-1',
   'prompt' = 'eiusmod nulla',
   'response_format' = 'json',
   'temperature' = '0',
   'language' = ''
)

res = VERB("POST", 
            url = "https://api.chatanywhere.tech/v1/audio/transcriptions", 
            body = body, 
            add_headers(headers_multipart),
            encode = 'multipart')

cat(content(res, 'text', encoding = "UTF-8"))
```

### Claude æ¨¡å‹

ChatAnywhere æä¾›äº†ä¸€ä¸ª `claude-3-5-sonnet-20240620` æ¨¡å‹ã€‚


```{r}
body = '{
   "model": "claude-3-5-sonnet-20240620",
   "messages": [
      {
         "role": "system",
         "content": "You are a helpful assistant."
      },
      {
         "role": "user",
         "content": "Temperatureè¿™ä¸ªå•è¯ä¸­å«æœ‰å‡ ä¸ªå­—æ¯eï¼Ÿ"
      }
   ],
   "temperature": 2
}';

res <- VERB("POST", 
            url = "https://api.chatanywhere.tech/v1/chat/completions", 
            body = body, 
            add_headers(headers))

content(res, 'text', encoding = "UTF-8") |> 
  fromJSON() |> 
  str()
```


### è¯­éŸ³ç¿»è¯‘æ¨¡å‹

å°†éŸ³é¢‘ç¿»è¯‘æˆè‹±æ–‡ã€‚


```{r}
body = list(
   'file' = upload_file('output/audio-goodday.mp3'),
   'model' = 'whisper-1',
   'prompt' = '',
   'response_format' = 'json',
   'temperature' = '0'
)

res <- VERB("POST", 
            url = "https://api.chatanywhere.tech/v1/audio/translations", 
            body = body, 
            add_headers(headers_multipart), 
            encode = 'multipart')

cat(content(res, 'text', encoding = "UTF-8"))
```


## é˜¿é‡Œäº‘ç™¾ç‚¼

é˜¿é‡Œäº‘çš„å¤§æ¨¡å‹æœåŠ¡å¹³å°ç™¾ç‚¼æ˜¯ä¸€ç«™å¼çš„å¤§æ¨¡å‹å¼€å‘åŠåº”ç”¨æ„å»ºå¹³å°ã€‚ä¸è®ºæ˜¯å¼€å‘è€…è¿˜æ˜¯ä¸šåŠ¡äººå‘˜ï¼Œéƒ½èƒ½æ·±å…¥å‚ä¸å¤§æ¨¡å‹åº”ç”¨çš„è®¾è®¡å’Œæ„å»ºã€‚æ‚¨å¯ä»¥é€šè¿‡ç®€å•çš„ç•Œé¢æ“ä½œï¼Œåœ¨ 5 åˆ†é’Ÿå†…å¼€å‘å‡ºä¸€æ¬¾å¤§æ¨¡å‹åº”ç”¨ï¼Œæˆ–åœ¨å‡ å°æ—¶å†…è®­ç»ƒå‡ºä¸€ä¸ªä¸“å±æ¨¡å‹ï¼Œä»è€Œå°†æ›´å¤šç²¾åŠ›ä¸“æ³¨äºåº”ç”¨åˆ›æ–°ã€‚

```{python}
from pprint import pprint
```


### ç™¾ç‚¼æ¨¡å‹

ç™¾ç‚¼æä¾›äº†ä¸°å¯Œå¤šæ ·çš„æ¨¡å‹é€‰æ‹©ï¼Œå®ƒé›†æˆäº†é€šä¹‰ç³»åˆ—å¤§æ¨¡å‹å’Œç¬¬ä¸‰æ–¹å¤§æ¨¡å‹ï¼Œæ¶µç›–æ–‡æœ¬ã€å›¾åƒã€éŸ³è§†é¢‘ç­‰ä¸åŒæ¨¡æ€ã€‚

å‚è§ï¼š<https://help.aliyun.com/zh/model-studio/getting-started/models?>


### è°ƒç”¨ç™¾ç‚¼æ¨¡å‹

æ‚¨å¯ä»¥ä½¿ç”¨ OpenAI Python SDKã€DashScope SDK æˆ– HTTP æ¥å£è°ƒç”¨é€šä¹‰åƒé—®æ¨¡å‹ã€‚

#### ä½¿ç”¨ DashScope SDK

ä½¿ç”¨ DashScope SDK æ—¶ï¼Œä¼šè‡ªåŠ¨è¯»å–ç¯å¢ƒå˜é‡ä¸­çš„ API_KEY å¹¶åˆ›å»ºå¯¹è±¡ã€‚

```{python}
#| cache: true
#| results: asis

# Refer to the document for workspace information: https://help.aliyun.com/document_detail/2746874.html    
        
from http import HTTPStatus
import dashscope

messages = [{'role': 'user', 'content': 'ä½ æ˜¯è°'}]
response = dashscope.Generation.call("qwen-turbo",
                            messages=messages,
                            result_format='message',  # set the result to be "message"  format.
                            stream=False, # set streaming output
                            )

pprint(response.output.choices[0].message.content, width = 72)
```

#### ä½¿ç”¨ OpenAI SDK

```{python}
#| cache: true

from openai import OpenAI
import os

# åˆ›å»º client
client = OpenAI(
    api_key=os.getenv("DASHSCOPE_API_KEY"), # å¦‚æœæ‚¨æ²¡æœ‰é…ç½®ç¯å¢ƒå˜é‡ï¼Œè¯·åœ¨æ­¤å¤„ç”¨æ‚¨çš„API Keyè¿›è¡Œæ›¿æ¢
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",  # å¡«å†™DashScopeæœåŠ¡çš„base_url
)

# ç”Ÿæˆå¯¹è¯
completion = client.chat.completions.create(
    model="qwen-turbo",
    messages=[
        {'role': 'system', 'content': 'You are a helpful assistant.'},
        {'role': 'user', 'content': 'ä½ æ˜¯è°ï¼Ÿ'}],
    temperature=0.8,
    top_p=0.8
    )

print(completion.choices[0].message.content)
```

### è§†è§‰æ¨ç†

Qwen-VL(`qwen-vl-plus`/`qwen-vl-max`) æ¨¡å‹ç°æœ‰å‡ å¤§ç‰¹ç‚¹ï¼š

- å¤§å¹…å¢å¼ºäº†å›¾ç‰‡ä¸­æ–‡å­—å¤„ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿæˆä¸ºç”Ÿäº§åŠ›å°å¸®æ‰‹ï¼Œæå–ã€æ•´ç†ã€æ€»ç»“æ–‡å­—ä¿¡æ¯ä¸åœ¨è¯ä¸‹ã€‚
- å¢åŠ å¯å¤„ç†åˆ†è¾¨ç‡èŒƒå›´ï¼Œå„åˆ†è¾¨ç‡å’Œé•¿å®½æ¯”çš„å›¾éƒ½èƒ½å¤„ç†ï¼Œå¤§å›¾å’Œé•¿å›¾èƒ½çœ‹æ¸…ã€‚
- å¢å¼ºè§†è§‰æ¨ç†å’Œå†³ç­–èƒ½åŠ›ï¼Œé€‚äºæ­å»ºè§†è§‰Agentï¼Œè®©å¤§æ¨¡å‹Agentçš„æƒ³è±¡åŠ›è¿›ä¸€æ­¥æ‰©å±•ã€‚
- å‡çº§çœ‹å›¾åšé¢˜èƒ½åŠ›ï¼Œæ‹ä¸€æ‹ä¹ é¢˜å›¾å‘ç»™Qwen-VLï¼Œå¤§æ¨¡å‹èƒ½å¸®ç”¨æˆ·ä¸€æ­¥æ­¥è§£é¢˜ã€‚

![](https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg)

```{python}
#| cache: true
from http import HTTPStatus
import dashscope


def simple_multimodal_conversation_call():
    """Simple single round multimodal conversation call.
    """
    messages = [
        {
            "role": "user",
            "content": [
                {"image": "https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg"},
                {"text": "è¿™æ˜¯ä»€ä¹ˆ?"}
            ]
        }
    ]
    response = dashscope.MultiModalConversation.call(model='qwen-vl-plus',
                                                     messages=messages)
    # The response status_code is HTTPStatus.OK indicate success,
    # otherwise indicate request is failed, you can get error code
    # and message from code and message.
    if response.status_code == HTTPStatus.OK:
        return(response)
    else:
        print(response.code)  # The error code.
        print(response.message)  # The error message.


response = simple_multimodal_conversation_call()
content = response.output.choices[0]['message']['content'][0]['text']
pprint(content, width = 72)
```

### æ–‡ç”Ÿå›¾

é€šä¹‰ä¸‡ç›¸-æ–‡æœ¬ç”Ÿæˆå›¾åƒæ˜¯åŸºäºè‡ªç ”çš„Composerç»„åˆç”Ÿæˆæ¡†æ¶çš„AIç»˜ç”»åˆ›ä½œå¤§æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·è¾“å…¥çš„æ–‡å­—å†…å®¹ï¼Œç”Ÿæˆç¬¦åˆè¯­ä¹‰æè¿°çš„å¤šæ ·åŒ–é£æ ¼çš„å›¾åƒã€‚é€šè¿‡çŸ¥è¯†é‡ç»„ä¸å¯å˜ç»´åº¦æ‰©æ•£æ¨¡å‹ï¼ŒåŠ é€Ÿæ”¶æ•›å¹¶æå‡æœ€ç»ˆç”Ÿæˆå›¾ç‰‡çš„æ•ˆæœï¼Œå¸ƒå±€è‡ªç„¶ã€ç»†èŠ‚ä¸°å¯Œã€ç”»é¢ç»†è…»ã€ç»“æœé€¼çœŸã€‚AIæ·±åº¦ç†è§£ä¸­è‹±æ–‡æ–‡æœ¬è¯­ä¹‰ï¼Œè®©æ–‡å­—ç§’å˜ç²¾è‡´AIç”»ä½œã€‚

å½“å‰æ¨¡å‹æ”¯æŒçš„é£æ ¼åŒ…æ‹¬ä½†ä¸é™äºï¼š

- æ°´å½©ã€æ²¹ç”»ã€ä¸­å›½ç”»ã€ç´ æã€æ‰å¹³æ’ç”»ã€äºŒæ¬¡å…ƒã€3Då¡é€šã€‚
- æ”¯æŒä¸­è‹±æ–‡åŒè¯­è¾“å…¥ã€‚
- æ”¯æŒå®¢æˆ·è‡ªå®šä¹‰å’’è¯­ä¹¦/ä¿®é¥°è¯ï¼Œå¯ç”Ÿæˆä¸åŒé£æ ¼ã€ä¸åŒä¸»é¢˜ã€ä¸åŒæ´¾åˆ«çš„å›¾ç‰‡ï¼Œæ»¡è¶³ä¸ªæ€§åˆ›æ„çš„AIå›¾ç‰‡ç”Ÿæˆéœ€æ±‚ã€‚
- æ”¯æŒè¾“å…¥å‚è€ƒå›¾ç‰‡è¿›è¡Œå‚è€ƒå†…å®¹æˆ–è€…å‚è€ƒé£æ ¼è¿ç§»ï¼Œæ”¯æŒæ›´ä¸°å¯Œçš„é£æ ¼ã€ä¸»é¢˜å’Œæ´¾åˆ«ï¼ŒAIä½œç”»è´¨é‡æ›´åŠ é«˜ä¿çœŸã€‚

```{python}
#| cache: true
#| eval: false
from http import HTTPStatus
from urllib.parse import urlparse, unquote
from pathlib import PurePosixPath
import requests
from dashscope import ImageSynthesis

def simple_call(prompt = 'Mouse rides elephant', out_dir = "output"):
    rsp = ImageSynthesis.call(model=ImageSynthesis.Models.wanx_v1,
                              prompt=prompt,
                              n=1,
                              size='1024*1024')
    if rsp.status_code == HTTPStatus.OK:
        print(rsp.output)
        print(rsp.usage)
        # save file to current directory
        files = []
        for result in rsp.output.results:
            file_name = PurePosixPath(unquote(urlparse(result.url).path)).parts[-1]
            file = './%s/%s' % (out_dir, file_name)
            with open(file, 'wb+') as f:
                f.write(requests.get(result.url).content)
            files.append(file)
        return(files)
    else:
        print('Failed, status_code: %s, code: %s, message: %s' %
              (rsp.status_code, rsp.code, rsp.message))


image_path = simple_call()
```

```{python}
from PIL import Image
import matplotlib.pyplot as plt

# è¯»å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
image_path = "output/435e0b57-6884-4e32-9187-2ada667b0ecb-1.png"
image = Image.open(image_path)

plt.imshow(image)
```

### è¯­éŸ³è¯†åˆ«

SenseVoice è¯­éŸ³è¯†åˆ«å¤§æ¨¡å‹ä¸“æ³¨äºé«˜ç²¾åº¦å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«ã€æƒ…æ„Ÿè¾¨è¯†å’ŒéŸ³é¢‘äº‹ä»¶æ£€æµ‹ï¼Œæ”¯æŒè¶…è¿‡ 50 ç§è¯­è¨€çš„è¯†åˆ«ï¼Œæ•´ä½“æ•ˆæœä¼˜äº Whisper æ¨¡å‹ï¼Œä¸­æ–‡ä¸ç²¤è¯­è¯†åˆ«å‡†ç¡®ç‡ç›¸å¯¹æå‡åœ¨ 50% ä»¥ä¸Šã€‚


```{python}
#| cache: true
# For prerequisites running the following sample, visit https://help.aliyun.com/document_detail/611472.html

import json
from urllib import request
from http import HTTPStatus

import dashscope


task_response = dashscope.audio.asr.Transcription.async_call(
    model='sensevoice-v1',
    file_urls=[
        'https://dashscope.oss-cn-beijing.aliyuncs.com/samples/audio/sensevoice/rich_text_example_1.wav'],
    language_hints=['en'],)

transcription_response = dashscope.audio.asr.Transcription.wait(
    task=task_response.output.task_id)

if transcription_response.status_code == HTTPStatus.OK:
    for transcription in transcription_response.output['results']:
        url = transcription['transcription_url']
        result = json.loads(request.urlopen(url).read().decode('utf8'))
        pprint(json.dumps(result, indent=4, ensure_ascii=False))
    print('transcription done!')
else:
    print('Error: ', transcription_response.output.message)
```

### è¯­éŸ³åˆæˆ

Sambert è¯­éŸ³åˆæˆ API åŸºäºè¾¾æ‘©é™¢æ”¹è‰¯çš„è‡ªå›å½’éŸµå¾‹æ¨¡å‹ï¼Œæ”¯æŒæ–‡æœ¬è‡³è¯­éŸ³çš„å®æ—¶æµå¼åˆæˆã€‚


```{python}
#| cache: true
import sys

import dashscope
from dashscope.audio.tts import SpeechSynthesizer

result = SpeechSynthesizer.call(model='sambert-zhichu-v1',
                                text='ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·',
                                sample_rate=48000)

tts_output = "output/weather.wav"
if result.get_audio_data() is not None:
    with open(tts_output, 'wb') as f:
        f.write(result.get_audio_data())
    print('SUCCESS: get audio data: %dbytes in output.wav' %
          (sys.getsizeof(result.get_audio_data())))
else:
    print('ERROR: response is %s' % (result.get_response()))
```

ä½¿ç”¨ IPython.display æ¨¡å—çš„ Audio ç±»æ¥æ˜¾ç¤ºéŸ³é¢‘æ–‡ä»¶ã€‚

```{python}
from IPython.display import Audio
from IPython.core.display import HTML

def html_tag_audio(file, file_type='wav'):
    file_type = file_type.lower()
    if file_type not in ['wav', 'mp3', 'ogg']:
        raise ValueError("Invalid audio type. Supported types: 'wav', 'mp3', 'ogg'.")
    
    audio_tag = f'''
    <audio controls>
      <source src="{file}" type="audio/{file_type}">
      Your browser does not support the audio element.
    </audio>
    '''
    return HTML(audio_tag)

# Example usage
tts_output = "output/weather.wav"
html_tag_audio(tts_output, file_type="wav")
```

### æ–‡æ¡£è§£æ

`Qwen-Long` æ˜¯åœ¨é€šä¹‰åƒé—®é’ˆå¯¹è¶…é•¿ä¸Šä¸‹æ–‡å¤„ç†åœºæ™¯çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒä¸­æ–‡ã€è‹±æ–‡ç­‰ä¸åŒè¯­è¨€è¾“å…¥ï¼Œæ”¯æŒæœ€é•¿ 1000 ä¸‡ tokens(çº¦ 1500 ä¸‡å­—æˆ– 1.5 ä¸‡é¡µæ–‡æ¡£)çš„è¶…é•¿ä¸Šä¸‹æ–‡å¯¹è¯ã€‚é…åˆåŒæ­¥ä¸Šçº¿çš„æ–‡æ¡£æœåŠ¡ï¼Œå¯æ”¯æŒ wordã€pdfã€markdownã€epubã€mobi ç­‰å¤šç§æ–‡æ¡£æ ¼å¼çš„è§£æå’Œå¯¹è¯ã€‚ 

#### ä¸Šä¼ æ–‡æ¡£

```{python}
from pathlib import Path
from openai import OpenAI

client = OpenAI(
    api_key=os.getenv("DASHSCOPE_API_KEY"), # å¦‚æœæ‚¨æ²¡æœ‰é…ç½®ç¯å¢ƒå˜é‡ï¼Œè¯·åœ¨æ­¤å¤„ç”¨æ‚¨çš„API Keyè¿›è¡Œæ›¿æ¢
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",  # å¡«å†™DashScopeæœåŠ¡çš„base_url
)

file_object = client.files.create(file=Path("example/Kraken2.pdf"), 
                                  purpose="file-extract")

print(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œæ–‡ä»¶ID: {file_object.id}")
```

#### æŸ¥è¯¢æ–‡ä»¶

æŸ¥è¯¢ã€åˆ é™¤æ–‡ä»¶ã€‚

```{python}
# æŸ¥è¯¢æ–‡ä»¶å…ƒä¿¡æ¯
client.files.retrieve(file_object.id)

# æŸ¥è¯¢æ–‡ä»¶åˆ—è¡¨
client.files.list()
```


```{python}
#| eval: false
# åˆ é™¤æ–‡ä»¶
client.files.delete(file_object.id)
```


### åŸºäºæ–‡æ¡£çš„å¯¹è¯

**Qwen-Long** æ”¯æŒé•¿æ–‡æœ¬ï¼ˆæ–‡æ¡£ï¼‰å¯¹è¯ï¼Œæ–‡æ¡£å†…å®¹éœ€æ”¾åœ¨ `role` ä¸º `system` çš„ `message` ä¸­ï¼Œæœ‰ä»¥ä¸‹ä¸¤ç§æ–¹å¼å¯å°†æ–‡æ¡£ä¿¡æ¯è¾“å…¥ç»™æ¨¡å‹ï¼š

- åœ¨æå‰ä¸Šä¼ æ–‡æ¡£è·å–æ–‡æ¡£ IDï¼ˆ`fileid`ï¼‰åï¼Œå¯ä»¥ç›´æ¥æä¾› `fileid`ã€‚æ”¯æŒåœ¨å¯¹è¯ä¸­ä½¿ç”¨ä¸€ä¸ªæˆ–å¤šä¸ª `fileid`ã€‚
- ç›´æ¥è¾“å…¥éœ€è¦å¤„ç†çš„æ–‡æœ¬æ ¼å¼çš„æ–‡æ¡£å†…å®¹ï¼ˆfile contentï¼‰ã€‚

```{python}
#| results: asis
from pprint import pprint

# è·å–æ–‡ä»¶å†…å®¹
# æ–°æ–‡æ¡£ä¸Šä¼ åéœ€è¦ç­‰å¾…æ¨¡å‹è§£æï¼Œé¦–è½®å“åº”æ—¶é—´å¯èƒ½è¾ƒé•¿
completion = client.chat.completions.create(
    model="qwen-long",
    messages=[
        {
            'role': 'system',
            'content': 'You are a helpful assistant.'
        },
        {
            'role': 'system',
            'content': f'fileid://{file_object.id}'
        },
        {
            'role': 'user',
            'content': 'è¿™ç¯‡æ–‡ç« è®²äº†ä»€ä¹ˆï¼Ÿ'
        }
    ],
    stream=False
)


pprint(completion.choices[0].message.model_dump(), width = 72)
```

#### å¤šä¸ªæ–‡æ¡£

å½“æœ‰å¤šä¸ªæ–‡æ¡£æ—¶ï¼Œå¯ä»¥å°†å¤šä¸ª `fileid` ä¼ é€’ç»™ `content`ã€‚

```python
# é¦–æ¬¡å¯¹è¯ä¼šç­‰å¾…æ–‡æ¡£è§£æå®Œæˆï¼Œé¦–è½®å“åº”æ—¶é—´å¯èƒ½è¾ƒé•¿
completion = client.chat.completions.create(
    model="qwen-long",
    messages=[
        {
            'role': 'system',
            'content': 'You are a helpful assistant.'
        },
        {
            'role': 'system',
            'content': f"fileid://{file_1.id},fileid://{file_2.id}"
        },
        {
            'role': 'user',
            'content': 'è¿™å‡ ç¯‡æ–‡ç« è®²äº†ä»€ä¹ˆï¼Ÿ'
        }
    ],
    stream=False
)
```

#### è¿½åŠ æ–‡æ¡£

ä½¿ç”¨ä¸‹é¢çš„æ–¹æ³•ï¼Œå¯ä»¥åœ¨å¯¹è¯è¿‡ç¨‹ä¸­è¿½åŠ æ–‡æ¡£ã€‚

```{python}
#| results: asis

# data_1.pdfä¸ºåŸæ–‡æ¡£ï¼Œdata_2.pdfä¸ºè¿½åŠ æ–‡æ¡£
file_1 = client.files.create(file=Path("example/gaoch-cv.md"),
                             purpose="file-extract")

# åˆå§‹åŒ–messagesåˆ—è¡¨
messages = [
    {
        'role': 'system',
        'content': 'You are a helpful assistant.'
    },
    {
        'role': 'system',
        'content': f'fileid://{file_1.id}'
    },
    {
        'role': 'user',
        'content': 'è¿™ç¯‡æ–‡ç« è®²äº†ä»€ä¹ˆï¼Ÿ'
    },
]
# ç¬¬ä¸€è½®å“åº”
completion = client.chat.completions.create(
    model="qwen-long",
    messages=messages,
    stream=False
)

# æ‰“å°å‡ºç¬¬ä¸€è½®å“åº”
print(f"ç¬¬ä¸€è½®å“åº”ï¼š{completion.choices[0].message.model_dump()}")
```

å°†ç¬¬ä¸€è½®å“åº”çš„å†…å®¹æ·»åŠ åˆ°å†å²è®°å½•ä¸­ã€‚


```{python}
# æ„é€ assistant_message
assistant_message = {
    "role": "assistant",
    "content": completion.choices[0].message.content}

# å°†assistant_messageæ·»åŠ åˆ°messagesä¸­
messages.append(assistant_message)
```

ä¸Šä¼ ä¸€ä¸ªæ–°æ–‡æ¡£ã€‚


```{python}
#| results: asis

# è·å–è¿½åŠ æ–‡æ¡£çš„fileid
file_2 = client.files.create(file=Path("example/syncom-group-library.md"), 
                             purpose="file-extract")

# å°†è¿½åŠ æ–‡æ¡£çš„fileidæ·»åŠ åˆ°messagesä¸­
system_message = {
    'role': 'system',
    'content': f'fileid://{file_2.id}'
}
messages.append(system_message)

# æ·»åŠ ç”¨æˆ·é—®é¢˜
messages.append({
    'role': 'user',
    'content': 'è¿™ä¸¤ç¯‡æ–‡ç« çš„å†…å®¹æœ‰ä»€ä¹ˆå¼‚åŒç‚¹ï¼Ÿ'
})

# è¿½åŠ æ–‡æ¡£åçš„å“åº”
completion = client.chat.completions.create(
    model="qwen-long",
    messages=messages,
    stream=False
)

print(f"è¿½åŠ æ–‡æ¡£åçš„å“åº”ï¼š{completion.choices[0].message.model_dump()}")
```

