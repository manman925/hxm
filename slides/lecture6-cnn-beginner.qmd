---
title: "ä»é›¶æ‰‹æ“ç¥ç»ç½‘ç»œ"
author: "é«˜æ˜¥è¾‰"
format:
  revealjs: 
    theme: white
    echo: true
    message: true
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    css: slide.css
    footer: '[è¯¾ç¨‹ä»“åº“](https://github.com/D2RS-2025spring)'
  pptx:
    reference-doc: template.pptx
bibliography: 
    - ../references.bib
    - ../packages.bib
---

## ç¥ç»ç½‘ç»œç®€ä»‹

<iframe src="//player.bilibili.com/player.html?isOutside=true&aid=113332711333508&bvid=BV1atCRYsE7x&cid=26360022345&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%"></iframe>

## ç¥ç»ç½‘ç»œæ¨¡å‹çš„ç»“æ„

```{mermaid}
#| include: false
flowchart LR
  %% è¾“å…¥å±‚
  subgraph è¾“å…¥å±‚
    A1
    A2
    A3
    A4
    A5
  end

  %% éšè—å±‚ 1
  subgraph éšè—å±‚ 1
    H11
    H12
  end

  %% éšè—å±‚ 2
  subgraph éšè—å±‚ 2
    H21
    H22
  end

  %% è¾“å‡ºå±‚
  subgraph è¾“å‡ºå±‚
    O1((y))
  end

  %% è¿æ¥ è¾“å…¥å±‚ â†’ éšè—å±‚ 1
  A1 --> H11
  A1 --> H12
  A2 --> H11
  A2 --> H12
  A3 --> H11
  A3 --> H12
  A4 --> H11
  A4 --> H12
  A5 --> H11
  A5 --> H12

  %% è¿æ¥ éšè—å±‚ 1 â†’ éšè—å±‚ 2
  H11 --> H21
  H11 --> H22
  H12 --> H21
  H12 --> H22

  %% è¿æ¥ éšè—å±‚ 2 â†’ è¾“å‡ºå±‚
  H21 --> O1
  H22 --> O1
```



## å…ˆå†³æ¡ä»¶

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å…·å¤‡ä»¥ä¸‹çŸ¥è¯†å’Œç¯å¢ƒé…ç½®ï¼š

1. **Python ç¼–ç¨‹åŸºç¡€**ï¼šç†Ÿæ‚‰åŸºæœ¬çš„ Python è¯­æ³•å’Œå¸¸ç”¨åº“
2. **ç¯å¢ƒé…ç½®**ï¼š
   - Python 3.10 æˆ–æ›´é«˜ç‰ˆæœ¬
   - Conda åŒ…ç®¡ç†å·¥å…·
   - PyTorch 2.0 æˆ–æ›´é«˜ç‰ˆæœ¬
   - GPU æ”¯æŒï¼ˆå¯é€‰ï¼Œä½†æ¨èï¼‰

## é…ç½®ç¯å¢ƒ

- åˆ›å»º Conda ç¯å¢ƒ
  - `conda create -n lecture6`
- å®‰è£… Pytorch
  - `conda activate lecture6`
  - `conda install pytorch torchvision torchaudio -c pytorch`


## æ‰‹å†™å­—æ¯è¯†åˆ«çš„å†å² {.smaller}

- ã€é‚®æ”¿ç¼–ç ã€‘ï¼šæ‰‹å†™å­—æ¯æˆ–æ•°å­—è¯†åˆ«ä¸€ç›´æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„ç»å…¸é—®é¢˜ã€‚
  - æ—©æœŸç ”ç©¶ä¸­ï¼Œä¸“å®¶éœ€è¦æ‰‹å·¥è®¾è®¡ç‰¹å¾æå–æ–¹æ³•æ¥è¯†åˆ«å›¾åƒä¸­çš„å­—æ¯æˆ–æ•°å­—ã€‚  
  - éšç€ç¥ç»ç½‘ç»œï¼Œå°¤å…¶æ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„å‡ºç°ï¼Œç³»ç»Ÿèƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ å¹¶æå–å›¾åƒç‰¹å¾ï¼Œå¤§å¹…æå‡äº†è¯†åˆ«å‡†ç¡®ç‡ã€‚ 
- æ‰‹å†™å­—æ¯è¯†åˆ«ä½¿ç”¨çš„**æ•°æ®é›†**
  - `MNIST` æ•°æ®é›†ä¾¿æ˜¯ä¸€ä¸ªç»å…¸çš„ä¾‹å­ï¼Œå®ƒåŒ…å«äº† 60000 å¼ è®­ç»ƒå›¾åƒå’Œ 10000 å¼ æµ‹è¯•å›¾åƒï¼Œæ¯å¼ å›¾åƒä¸º 28x28 åƒç´ çš„ç°åº¦å›¾ï¼Œå¹¿æ³›ç”¨äºæ‰‹å†™æ•°å­—è¯†åˆ«çš„æ•™å­¦å’Œç ”ç©¶ä¸­ã€‚


## å¯¼å…¥éœ€è¦ç”¨åˆ°çš„åº“


```{python}
# å¯¼å…¥å¿…è¦çš„åº“
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import torch.nn.functional as F
```


## ä¸‹è½½ MNIST æ•°æ®é›†

æˆ‘ä»¬å°†åˆ©ç”¨ torchvision è‡ªåŠ¨ä¸‹è½½ MNIST æ•°æ®é›†ã€‚

- `transforms.ToTensor()`  å°† PIL å›¾åƒæˆ– numpy æ•°ç»„è½¬æ¢ä¸º tensorï¼Œå¹¶å°†åƒç´ å€¼å½’ä¸€åŒ–åˆ° [0, 1] èŒƒå›´å†…
- `transforms.Normalize()` è¿›ä¸€æ­¥å°†æ•°æ®æ ‡å‡†åŒ–ï¼Œå‡å€¼å’Œæ ‡å‡†å·®æ˜¯é’ˆå¯¹ MNIST æ•°æ®é›†è®¡ç®—å¾—åˆ°çš„

```{python}
# å®šä¹‰å›¾åƒé¢„å¤„ç†æµç¨‹
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

# ä¸‹è½½å¹¶åŠ è½½æ•°æ®é›†
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)
```


## ç»˜åˆ¶æ•°æ®é›†

ç»˜åˆ¶ 12 å¼ è®­ç»ƒé›†å’Œ 4 å¼ æµ‹è¯•é›†å›¾åƒï¼Œå¹¶åœ¨å›¾ä¸Šå³ä¸‹è§’æ ‡å‡ºæ•°æ®é›†å›¾åƒçš„idã€‚

```{python}
#| echo: false
#| label: fig-example-of-numbers
#| fig-cap: æ‰‹å†™æ•°å­—ç¤ºä¾‹ï¼ˆçº¢è‰²å–è‡ªè®­ç»ƒé›†ï¼Œç»¿è‰²å–è‡ªæµ‹è¯•é›†ï¼‰
#| out-width: 100%
plt.figure(figsize=(10, 6))

# ç»˜åˆ¶è®­ç»ƒé›†å›¾åƒ
plt.title("MNIST Dataset Examples")
for i in range(21): # ç»˜åˆ¶ 21 å¼ è®­ç»ƒé›†å›¾åƒ
    plt.subplot(4, 7, i+1) # ç»˜åˆ¶ç¬¬ i+1 å¼ å›¾åƒ
    plt.axis("off") # ä¸æ˜¾ç¤ºåæ ‡è½´
    img = train_dataset[i][0].squeeze() # è·å–ç¬¬ i å¼ å›¾åƒ
    label = train_dataset[i][1] # è·å–ç¬¬ i å¼ å›¾åƒçš„æ ‡ç­¾
    plt.imshow(img, cmap="gray") # ç»˜åˆ¶ç¬¬ i å¼ å›¾åƒ
    plt.text(18, 26, f"{label}", fontsize=10, color="red") # åœ¨å›¾åƒå³ä¸‹è§’æ ‡å‡ºçº¢è‰²æ ‡ç­¾

# ç»˜åˆ¶æµ‹è¯•é›†å›¾åƒ
for i in range(7): # ç»˜åˆ¶ 7 å¼ æµ‹è¯•é›†å›¾åƒ
    plt.subplot(4, 7, i+22) # ç»˜åˆ¶ç¬¬ i+22 å¼ å›¾åƒ
    plt.axis("off") # ä¸æ˜¾ç¤ºåæ ‡è½´
    img = test_dataset[i][0].squeeze() # è·å–ç¬¬ i å¼ å›¾åƒ
    label = test_dataset[i][1] # è·å–ç¬¬ i å¼ å›¾åƒçš„æ ‡ç­¾
    plt.imshow(img, cmap="gray") # ç»˜åˆ¶ç¬¬ i å¼ å›¾åƒ
    plt.text(20, 25, f"{label}", fontsize=10, color="green") # åœ¨å›¾åƒå³ä¸‹è§’æ ‡å‡ºç»¿è‰²æ ‡ç­¾

plt.tight_layout()
plt.show()
```


## æ„å»º LeNet æ¨¡å‹ {.scrollable .smaller}

ä¸‹é¢ä»£ç å®šä¹‰äº† LeNet æ¨¡å‹ï¼Œå…¶ä¸­åŒ…å«ä¸¤ä¸ªå·ç§¯å±‚ã€ä¸¤ä¸ªæ± åŒ–å±‚å’Œä¸‰ä¸ªå…¨è¿æ¥å±‚ã€‚æ¯ä¸€æ­¥å‡é™„æœ‰è¯¦ç»†æ³¨é‡Šã€‚

```{python}
# å®šä¹‰ LeNet ç¥ç»ç½‘ç»œæ¨¡å‹ç±»ï¼Œç»§æ‰¿è‡ª nn.Module
class LeNet(nn.Module):
    def __init__(self):
        # åˆå§‹åŒ–çˆ¶ç±» nn.Module
        super(LeNet, self).__init__()
        # ç¬¬ä¸€ä¸ªå·ç§¯å±‚ï¼š
        # è¾“å…¥é€šé“ï¼š1ï¼ˆç°åº¦å›¾åƒï¼‰ï¼Œè¾“å‡ºé€šé“ï¼š6ï¼Œå·ç§¯æ ¸å¤§å°ï¼š5x5
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)
        
        # å®šä¹‰æ± åŒ–å±‚ï¼š
        # ä½¿ç”¨ 2x2 çš„æœ€å¤§æ± åŒ–ï¼Œèƒ½å¤Ÿå‡å°ç‰¹å¾å›¾çš„å°ºå¯¸
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        
        # ç¬¬äºŒä¸ªå·ç§¯å±‚ï¼š
        # è¾“å…¥é€šé“ï¼š6ï¼Œè¾“å‡ºé€šé“ï¼š16ï¼Œå·ç§¯æ ¸å¤§å°ï¼š5x5
        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)
        
        # ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼š
        # è¾“å…¥ç‰¹å¾æ•°ä¸º 16*4*4ï¼ˆç»è¿‡ä¸¤æ¬¡å·ç§¯å’Œæ± åŒ–åçš„ç‰¹å¾å›¾å°ºå¯¸ï¼‰ï¼Œè¾“å‡ºç‰¹å¾æ•°ä¸º 120
        self.fc1 = nn.Linear(in_features=16*4*4, out_features=120)
        
        # ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚ï¼šå°† 120 ä¸ªç‰¹å¾æ˜ å°„åˆ° 84 ä¸ªç‰¹å¾
        self.fc2 = nn.Linear(in_features=120, out_features=84)
        
        # ç¬¬ä¸‰ä¸ªå…¨è¿æ¥å±‚ï¼šè¾“å‡º 10 ä¸ªç±»åˆ«ï¼Œå¯¹åº” MNIST ä¸­ 10 ä¸ªæ•°å­—
        self.fc3 = nn.Linear(in_features=84, out_features=10)

    def forward(self, x):
        # å°†è¾“å…¥é€šè¿‡ç¬¬ä¸€ä¸ªå·ç§¯å±‚ï¼Œå¹¶ä½¿ç”¨ ReLU æ¿€æ´»å‡½æ•°å¢åŠ éçº¿æ€§
        x = torch.relu(self.conv1(x))
        # åº”ç”¨æ± åŒ–å±‚ï¼Œå‡å°ç‰¹å¾å›¾å°ºå¯¸
        x = self.pool(x)
        # ç¬¬äºŒä¸ªå·ç§¯å±‚ + ReLU æ¿€æ´»
        x = torch.relu(self.conv2(x))
        # å†æ¬¡æ± åŒ–
        x = self.pool(x)
        # å°†å¤šç»´ç‰¹å¾å›¾å±•å¹³ä¸ºä¸€ç»´å‘é‡ï¼Œä¸ºå…¨è¿æ¥å±‚åšå‡†å¤‡
        x = x.view(-1, 16*4*4)
        # ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚ + ReLU æ¿€æ´»
        x = torch.relu(self.fc1(x))
        # ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚ + ReLU æ¿€æ´»
        x = torch.relu(self.fc2(x))
        # ç¬¬ä¸‰ä¸ªå…¨è¿æ¥å±‚å¾—åˆ°æœ€ç»ˆè¾“å‡ºï¼ˆæœªç»è¿‡æ¿€æ´»ï¼Œåç»­ä¼šç»“åˆæŸå¤±å‡½æ•°ä½¿ç”¨ï¼‰
        x = self.fc3(x)
        return x
```

## LeNet æ¨¡å‹ç»“æ„å›¾ {.smalller}

åˆå§‹åŒ–ä¸€ä¸ª LeNet æ¨¡å‹ï¼Œå¹¶æ‰“å°å…¶ç»“æ„ã€‚

```{python}
model = LeNet()
print(model)
```

**æ•°æ®æµå‘è¯´æ˜**ï¼š

1. è¾“å…¥çš„ 28Ã—28 å›¾åƒé¦–å…ˆç»è¿‡ç¬¬ä¸€ä¸ªå·ç§¯å±‚ï¼Œç”Ÿæˆ 6 ä¸ª 24Ã—24 çš„ç‰¹å¾å›¾
2. ç»è¿‡æ± åŒ–å±‚åï¼Œç‰¹å¾å›¾å˜ä¸º 6 ä¸ª 12Ã—12
3. ç¬¬äºŒä¸ªå·ç§¯å±‚å°†ç‰¹å¾å›¾è½¬æ¢ä¸º 16 ä¸ª 8Ã—8 çš„ç‰¹å¾å›¾
4. å†æ¬¡æ± åŒ–åï¼Œå¾—åˆ° 16 ä¸ª 4Ã—4 çš„ç‰¹å¾å›¾
5. å°†ç‰¹å¾å›¾å±•å¹³ä¸ºä¸€ç»´å‘é‡ï¼ˆ16Ã—4Ã—4 = 256ï¼‰
6. é€šè¿‡ä¸‰ä¸ªå…¨è¿æ¥å±‚é€æ­¥å°†ç‰¹å¾é™ç»´ï¼Œæœ€ç»ˆè¾“å‡º 10 ä¸ªç±»åˆ«çš„æ¦‚ç‡åˆ†å¸ƒ


## åˆ›å»ºæ•°æ®åŠ è½½å™¨

åœ¨è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸éœ€è¦ **åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆ`DataLoader`ï¼‰**ï¼Œä»¥æ‰¹é‡å¤„ç†æ•°æ®ï¼Œæ‰“ä¹±æ•°æ®é¡ºåºç­‰ã€‚

```{python}
# åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_loader = torch.utils.data.DataLoader(
    dataset=train_dataset,
    batch_size=64,
    shuffle=True
)

test_loader = torch.utils.data.DataLoader(
    dataset=test_dataset,
    batch_size=1000,
    shuffle=False
)
```


## å®šä¹‰è®­ç»ƒå‡½æ•° {.smaller}

è®­ç»ƒå‡½æ•°ä¸­ï¼Œæ¨¡å‹å¯¹æ¯ä¸ªæ‰¹æ¬¡æ•°æ®è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œè®¡ç®—æŸå¤±åè¿›è¡Œåå‘ä¼ æ’­ï¼Œå¹¶ä½¿ç”¨ä¼˜åŒ–å™¨æ›´æ–°æƒé‡ã€‚æ¯éš”ä¸€å®šæ‰¹æ¬¡è¾“å‡ºå½“å‰æŸå¤±ï¼Œæ–¹ä¾¿è§‚å¯Ÿè®­ç»ƒè¿›åº¦ã€‚

```{python}
# å®šä¹‰è®­ç»ƒå‡½æ•°ï¼Œç”¨äºåœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒæ¨¡å‹
def train(model, device, train_loader, optimizer, criterion, epoch):
    model.train()
    train_loss = 0
    correct = 0
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        
        # ç´¯è®¡æŸå¤±å’Œæ­£ç¡®é¢„æµ‹æ•°
        train_loss += loss.item() * data.size(0)
        pred = output.argmax(dim=1, keepdim=True)
        correct += pred.eq(target.view_as(pred)).sum().item()
        
        if batch_idx % 5000 == 0:
            print(f"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}]\tLoss: {loss.item():.6f}")
    
    # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
    train_loss /= len(train_loader.dataset)
    accuracy = 100. * correct / len(train_loader.dataset)
    return train_loss, accuracy
```

## ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°

åœ¨ç¥ç»ç½‘ç»œè®­ç»ƒä¸­ï¼Œä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°æ˜¯ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š

1. **éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨ï¼ˆSGDï¼‰**ï¼š
   - **ç”¨é€”**ï¼šé€šè¿‡è®¡ç®—æŸå¤±å‡½æ•°å¯¹æ¨¡å‹å‚æ•°çš„æ¢¯åº¦ï¼Œæ²¿ç€æ¢¯åº¦çš„åæ–¹å‘æ›´æ–°å‚æ•°
   - **å­¦ä¹ ç‡**ï¼šæ§åˆ¶æ¯æ¬¡å‚æ•°æ›´æ–°çš„æ­¥é•¿ï¼ˆè¿™é‡Œè®¾ä¸º 0.01ï¼‰
   - **åŠ¨é‡**ï¼šè¿™é‡Œè®¾ä¸º 0.9ï¼Œè¡¨ç¤ºä¿ç•™ 90% çš„å†å²æ¢¯åº¦ä¿¡æ¯

```{python}
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
```

## ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°

2. **äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼ˆCrossEntropyLossï¼‰**ï¼š
   - **ç”¨é€”**ï¼šè®¡ç®—å¤šåˆ†ç±»é—®é¢˜ç»“æœçš„ä¸€è‡´æ€§ï¼ˆå¦‚æœ¬ä¾‹ä¸­çš„ 10 ä¸ªæ•°å­—åˆ†ç±»ï¼‰
   - **ç»“æœ**ï¼šè¾“å‡ºå€¼åœ¨ [0, âˆ) èŒƒå›´å†…ï¼Œ0 è¡¨ç¤ºå®Œç¾é¢„æµ‹

```{python}
criterion = nn.CrossEntropyLoss()
```


## å®šä¹‰æµ‹è¯•å‡½æ•°

æµ‹è¯•å‡½æ•°ä¸­ï¼Œæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œå‰å‘ä¼ æ’­ï¼Œå¹¶ç´¯è®¡è®¡ç®—æ€»ä½“æŸå¤±ä¸æ­£ç¡®é¢„æµ‹æ•°é‡ï¼Œæœ€ç»ˆè¾“å‡ºå¹³å‡æŸå¤±åŠå‡†ç¡®ç‡ï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

```{python}
# å®šä¹‰æµ‹è¯•å‡½æ•°ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°
def test(model, device, test_loader, criterion):
    model.eval()  # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œå…³é—­ dropout ç­‰è®­ç»ƒç‰¹æ€§
    test_loss = 0  # åˆå§‹åŒ–æµ‹è¯•æŸå¤±
    correct = 0    # åˆå§‹åŒ–é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬è®¡æ•°
    all_preds = []  # ç”¨äºå­˜å‚¨æ‰€æœ‰é¢„æµ‹ç»“æœ
    all_targets = []  # ç”¨äºå­˜å‚¨æ‰€æœ‰çœŸå®æ ‡ç­¾
    
    # åœ¨æµ‹è¯•é˜¶æ®µä¸è®¡ç®—æ¢¯åº¦ï¼ŒèŠ‚çœå†…å­˜å’ŒåŠ å¿«è®¡ç®—é€Ÿåº¦
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target).item() * data.size(0)
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            
            # æ”¶é›†é¢„æµ‹ç»“æœå’ŒçœŸå®æ ‡ç­¾
            all_preds.extend(pred.cpu().numpy().flatten())
            all_targets.extend(target.cpu().numpy())
    
    test_loss /= len(test_loader.dataset)  # è®¡ç®—å¹³å‡æŸå¤±
    accuracy = 100. * correct / len(test_loader.dataset)  # è®¡ç®—å‡†ç¡®ç‡
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    from sklearn.metrics import confusion_matrix
    cm = confusion_matrix(all_targets, all_preds)
    
    return test_loss, accuracy, cm
```


## ä¸»å‡½æ•°ï¼šè®­ç»ƒä¸è¯„ä¼°æ¨¡å‹

æ¨¡å‹è®­ç»ƒæ¨èä½¿ç”¨ CUDA æˆ– MPS è¿›è¡Œè®­ç»ƒï¼ˆGPUï¼‰ï¼Œå¦‚æœ CUDA æˆ– MPS ä¸å¯ç”¨ï¼Œåˆ™ä½¿ç”¨ CPU è¿›è¡Œè®­ç»ƒã€‚

```{python}
# ä¸»å‡½æ•°ï¼šè®­ç»ƒä¸è¯„ä¼°æ¨¡å‹
# æ£€æŸ¥æ˜¯å¦æœ‰ GPU å¯ç”¨ï¼Œå¦åˆ™ä½¿ç”¨ CPU
device = torch.device("cuda" if torch.cuda.is_available() else "mps" if torch.mps.is_available() else "cpu")

# å®ä¾‹åŒ– LeNet æ¨¡å‹ï¼Œå¹¶ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ä¸Š
model = LeNet().to(device)
```

## å¼€å§‹è®­ç»ƒ

è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è®°å½•äº†è®­ç»ƒæŸå¤±ã€è®­ç»ƒå‡†ç¡®ç‡ã€æµ‹è¯•æŸå¤±å’Œæµ‹è¯•å‡†ç¡®ç‡ã€‚


```{python}
# ç”¨äºè®°å½•è®­ç»ƒè¿‡ç¨‹çš„æŒ‡æ ‡
train_losses = []
train_accs = []
test_losses = []
test_accs = []

epochs = 4 # è®¾å®šè®­ç»ƒè½®æ•°ä¸º 4
# å¾ªç¯è®­ç»ƒå’Œæµ‹è¯•æ¨¡å‹
for epoch in range(1, epochs + 1):
    # è®­ç»ƒå¹¶è®°å½•æŒ‡æ ‡
    train_loss, train_acc = train(model, device, train_loader, optimizer, criterion, epoch)
    test_loss, test_acc, cm = test(model, device, test_loader, criterion)
    
    # ä¿å­˜æŒ‡æ ‡
    train_losses.append(train_loss)
    train_accs.append(train_acc)
    test_losses.append(test_loss)
    test_accs.append(test_acc)
```


## è®­ç»ƒä¸€è½®éƒ½å‘ç”Ÿäº†å“ªäº›è®¡ç®—ï¼Ÿ {.smaller}

è®­ç»ƒä¸€è½®ï¼ˆ**epoch**ï¼‰å¯ä»¥æƒ³è±¡æˆå°æœ‹å‹å­¦ä¸€é“æ•°å­¦é¢˜çš„å®Œæ•´è¿‡ç¨‹ï¼Œåˆ†æˆä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š

1. **å°è¯•è§£é¢˜ï¼ˆå‰å‘ä¼ æ’­ï¼‰**
   - ä½ çœ‹åˆ°äº†ä¸€é“æ•°å­¦é¢˜ï¼Œæ¯”å¦‚ **"5 + 3 = ?"**ã€‚
   - ä½ å¿ƒé‡Œæƒ³ä¸€ä¸‹ï¼Œè§‰å¾—ç­”æ¡ˆåº”è¯¥æ˜¯ **"8"**ã€‚

2. **æ£€æŸ¥ç­”æ¡ˆï¼ˆè®¡ç®—æŸå¤±ï¼‰**
   - ä½ æŠŠç­”æ¡ˆå†™åœ¨ä½œä¸šæœ¬ä¸Šï¼Œç„¶åè€å¸ˆå‘Šè¯‰ä½ å¯¹ä¸å¯¹ã€‚
   - å¦‚æœä½ å†™é”™äº†ï¼Œæ¯”å¦‚å†™æˆ **"7"**ï¼Œè€å¸ˆå°±ä¼šå‘Šè¯‰ä½ é”™äº† **"1"**ã€‚

3. **æ‰¾å‡ºé”™åœ¨å“ªé‡Œï¼ˆåå‘ä¼ æ’­ï¼‰**
   - ä½ æƒ³ä¸€æƒ³ï¼Œä¸ºä»€ä¹ˆé”™äº†ï¼Ÿ  
   - å¯èƒ½æ˜¯ä½ å¿ƒç®—çš„æ—¶å€™å°‘åŠ äº† **1**ã€‚

## è®­ç»ƒä¸€è½®éƒ½å‘ç”Ÿäº†å“ªäº›è®¡ç®—ï¼Ÿ {.smaller}

4. **æ”¹æ­£é”™è¯¯ï¼ˆå‚æ•°æ›´æ–°ï¼‰**
   - ä½ ä¸‹æ¬¡é‡åˆ°ç±»ä¼¼çš„é¢˜ç›®ï¼Œä¼šæ›´åŠ å°å¿ƒï¼Œæ¯”å¦‚æ•°æ‰‹æŒ‡æ¥ç¡®è®¤ã€‚
   - è¿™æ ·ï¼Œä½ å­¦å¾—è¶Šæ¥è¶Šå¥½ï¼Œé”™è¯¯è¶Šæ¥è¶Šå°‘ã€‚

5. **é‡å¤ç»ƒä¹ **
   - ä½ åšå®Œè¿™é“é¢˜ï¼Œè€å¸ˆå†ç»™ä½ æ–°çš„é¢˜ç›®ã€‚
   - ä½ ç»§ç»­ç»ƒä¹ ï¼Œç›´åˆ°ä½ èƒ½å¿«é€Ÿåˆå‡†ç¡®åœ°åšå‡ºç­”æ¡ˆã€‚

è®­ç»ƒä¸€è½®å°±åƒè¿™æ ·ï¼Œ**è®©ç¥ç»ç½‘ç»œåšé¢˜ï¼ˆé¢„æµ‹ï¼‰ã€æ£€æŸ¥ç­”æ¡ˆï¼ˆè®¡ç®—æŸå¤±ï¼‰ã€æ‰¾é”™è¯¯ï¼ˆåå‘ä¼ æ’­ï¼‰ã€æ”¹æ­£ï¼ˆæ›´æ–°å‚æ•°ï¼‰**ï¼Œç„¶åç»§ç»­å­¦ä¹ ï¼Œç›´åˆ°å˜å¾—å¾ˆèªæ˜ï¼ğŸ“šğŸ˜Š

## ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹å›¾è¡¨

è®­ç»ƒç»“æŸåï¼Œæˆ‘ä»¬å¯ä»¥ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹çš„æŸå¤±æ›²çº¿å’Œå‡†ç¡®ç‡æ›²çº¿ã€‚

```{python}
#| label: fig-traning-process
#| fig-cap: è®­ç»ƒæŸå¤±æ›²çº¿å’Œå‡†ç¡®ç‡å˜åŒ–
#| out-width: 100%
# ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹å›¾è¡¨
epochs_range = range(1, epochs + 1)

plt.figure(figsize=(12, 5))

# ç»˜åˆ¶æŸå¤±æ›²çº¿
plt.subplot(1, 2, 1)
plt.plot(epochs_range, train_losses, 'bo-', label='Training Loss')
plt.plot(epochs_range, test_losses, 'ro-', label='Test Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# ç»˜åˆ¶å‡†ç¡®ç‡æ›²çº¿
plt.subplot(1, 2, 2)
plt.plot(epochs_range, train_accs, 'bo-', label='Training Accuracy')
plt.plot(epochs_range, test_accs, 'ro-', label='Test Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.legend()

plt.tight_layout()
plt.show()
```

