[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "数据驱动的可重复性研究",
    "section": "",
    "text": "前言\n《数据驱动的可重复性研究》课程旨在教授学生如何在科学研究中实现数据驱动的可重复性，确保研究结果的可验证性和可靠性。",
    "crumbs": [
      "前言"
    ]
  },
  {
    "objectID": "index.html#课程目标",
    "href": "index.html#课程目标",
    "title": "数据驱动的可重复性研究",
    "section": "课程目标",
    "text": "课程目标\n\n理解数据驱动的可重复性研究的概念和重要性\n掌握相关工具和技术，能够实施可重复性研究\n培养数据分析和代码编写的能力，提高科研水平",
    "crumbs": [
      "前言"
    ]
  },
  {
    "objectID": "index.html#课程特色",
    "href": "index.html#课程特色",
    "title": "数据驱动的可重复性研究",
    "section": "课程特色",
    "text": "课程特色\n\n强调数据和代码的规范化，确保可重复性\n以项目为导向，关注实际应用场景\n注重实践操作，强调动手能力",
    "crumbs": [
      "前言"
    ]
  },
  {
    "objectID": "index.html#课堂组织形式",
    "href": "index.html#课堂组织形式",
    "title": "数据驱动的可重复性研究",
    "section": "课堂组织形式",
    "text": "课堂组织形式\n因为选课场地限制，无法提供机房。请选课的同学自行准备电脑，并根据课程进度安排，确保电脑上安装了 R、Python 等需要用到的软件。\n\n课堂讲解：介绍相关概念、方法和工具\n实践操作：通过实际案例进行动手实践\n小组讨论：学生之间进行交流与协作\n课后作业：巩固所学知识，提高实践能力",
    "crumbs": [
      "前言"
    ]
  },
  {
    "objectID": "index.html#如何分组",
    "href": "index.html#如何分组",
    "title": "数据驱动的可重复性研究",
    "section": "如何分组？",
    "text": "如何分组？\n\n请研究生们根据自己的实验室或者研究方向，自行分组，每组人数不超过 10 人。\n在第二次课程开始前，请在雨课堂内确认分组。",
    "crumbs": [
      "前言"
    ]
  },
  {
    "objectID": "index.html#如何考察",
    "href": "index.html#如何考察",
    "title": "数据驱动的可重复性研究",
    "section": "如何考察？",
    "text": "如何考察？\n\n使用课程中教授的技能，创建一个数据分析、软件开发、研究复现等类型的项目\n3 - 5 人一组共创一个项目，将项目代码移交到课程仓库（使用 transfer 的方法）\n项目可以以课程建设为立足点，为课程课件提出改进意见。\n需要保证每个人在项目中都有贡献（通过 Git 追踪）。",
    "crumbs": [
      "前言"
    ]
  },
  {
    "objectID": "what-is-reproducible-study.html",
    "href": "what-is-reproducible-study.html",
    "title": "1  什么是可重复性研究",
    "section": "",
    "text": "1.1 可重复性危机与其影响因素",
    "crumbs": [
      "课程简介",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>什么是可重复性研究</span>"
    ]
  },
  {
    "objectID": "what-is-reproducible-study.html#可重复性危机与其影响因素",
    "href": "what-is-reproducible-study.html#可重复性危机与其影响因素",
    "title": "1  什么是可重复性研究",
    "section": "",
    "text": "1.1.1 科学研究中的可重复性问题\n当前，许多研究因数据处理和分析过程中的多变性而存在可重复性问题。实验方法的不透明、数据预处理记录不完整以及代码注释的缺失，使得其他研究人员在尝试复现研究结果时面临巨大障碍。这种现象不仅影响研究的可靠性，也削弱了科学发现的信服力。\n\n\n1.1.2 可重复性危机的成因\n可重复性危机通常由以下原因引起：\n\n实验设计不严谨：缺乏详细的实验步骤描述和统一的协议，使得各实验室在操作过程中存在差异；\n数据管理不规范：数据格式不统一、数据预处理方法不标准，导致信息在共享过程中丢失或变形；\n软件工具多样性：使用不同版本的软件和依赖包，使得相同代码在不同平台上可能产生不同结果；\n透明度不足：实验方法和分析流程未能充分公开，限制了同行对实验过程和结果的核查与复现。\n\n\n\n1.1.3 可重复性危机的后果\n当研究结果不可重复时，容易导致错误结论的传播，进而浪费宝贵的研究资源，并最终损害公共对科学研究的信任。面对这一危机，构建标准化、透明化的研究流程已成为迫切需求。",
    "crumbs": [
      "课程简介",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>什么是可重复性研究</span>"
    ]
  },
  {
    "objectID": "what-is-reproducible-study.html#可重复性研究的定义与重要性",
    "href": "what-is-reproducible-study.html#可重复性研究的定义与重要性",
    "title": "1  什么是可重复性研究",
    "section": "1.2 可重复性研究的定义与重要性",
    "text": "1.2 可重复性研究的定义与重要性\n可重复性研究要求研究者在实验设计、数据收集、数据预处理、数据分析及结果解释等环节中，采用统一标准和详细记录，确保他人能在相同或相似条件下重现整个研究过程，并验证研究结论。\n\n1.2.1 实验设计与数据收集的可重复性\n\n1.2.1.1 实验方法标准化\n为确保结果的一致性，必须将实验步骤细化，制定详细的操作协议。通过明确描述实验环境、仪器配置和操作流程，可以有效降低因方法学不确定性带来的偏差。\n\n\n1.2.1.2 数据格式统一\n采用通用且结构化的数据格式（如 CSV、JSON、XML 等）可以提升数据共享和交流的效率。统一的数据格式不仅便于跨平台使用，还使得数据在预处理和分析过程中保持一致性，减少因格式转换带来的错误。\n下面是一个 csv 文件的示例：\nid,name,age\n1,张三,20\n2,李四,21\n3,王五,22\n下面是一个 json 文件的示例：\n[\n  {\n    \"id\": 1,\n    \"name\": \"张三\",\n    \"age\": 20\n  },\n  {\n    \"id\": 2,\n    \"name\": \"李四\",\n    \"age\": 21\n  },\n  {\n  \"id\": 3,\n    \"name\": \"王五\",\n    \"age\": 22\n  }\n]\n下面是一个 xml 文件的示例：\n&lt;data&gt;\n  &lt;person&gt;\n    &lt;id&gt;1&lt;/id&gt;\n    &lt;name&gt;张三&lt;/name&gt;\n    &lt;age&gt;20&lt;/age&gt;\n  &lt;/person&gt;\n  &lt;person&gt;\n    &lt;id&gt;2&lt;/id&gt;\n    &lt;name&gt;李四&lt;/name&gt;\n    &lt;age&gt;21&lt;/age&gt;\n  &lt;/person&gt;\n  &lt;person&gt;\n    &lt;id&gt;3&lt;/id&gt;\n    &lt;name&gt;王五&lt;/name&gt;\n    &lt;age&gt;22&lt;/age&gt;\n  &lt;/person&gt;\n&lt;/data&gt;\n\n\n1.2.1.3 数据预处理规范\n数据预处理包括去除噪声、处理缺失值和变量标准化。规范化的预处理流程有助于确保数据质量，为后续的分析提供坚实基础，并使得不同研究人员在处理同一数据集时获得相似的初步结果。\n\n\n\n1.2.2 数据分析与结果解释的可重复性\n\n1.2.2.1 数据分析方法的透明化\n在数据分析过程中，必须采用明确的方法和公开的算法，并辅以详尽的代码注释。利用开源软件和共享代码，不仅有助于同行验证分析过程，还能促进学术交流与合作。\n\n\n1.2.2.2 结果解释与不确定性讨论\n研究结果的呈现应科学严谨，同时对潜在的不确定性进行详细讨论。全面阐述结果的合理性和局限性，可以为后续研究提供改进方向，也有助于建立对研究结论的信任。\n\n\n1.2.2.3 数据可视化的重要性\n高质量的数据可视化工具能直观地展示复杂数据，使得图表既具备良好的可读性，又能够清晰解释研究发现。精心设计的图表不仅能帮助读者理解数据，还能成为验证研究结果的重要依据。",
    "crumbs": [
      "课程简介",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>什么是可重复性研究</span>"
    ]
  },
  {
    "objectID": "what-is-reproducible-study.html#构建跨平台可重复性研究环境的策略",
    "href": "what-is-reproducible-study.html#构建跨平台可重复性研究环境的策略",
    "title": "1  什么是可重复性研究",
    "section": "1.3 构建跨平台可重复性研究环境的策略",
    "text": "1.3 构建跨平台可重复性研究环境的策略\n为了构建一个标准化的、跨平台的可重复性研究环境，研究者需要从以下几个方面着手：\n\n硬件与软件环境标准化：选择性能稳定的计算机硬件，统一安装所需的软件和依赖包，确保各平台之间环境一致，减少因平台差异导致的误差。\n文档与代码管理：建立详细的实验记录，包括实验设计、数据处理步骤、代码实现和结果讨论。借助版本控制工具（如 Git），记录每一次修改，确保过程透明可追溯。\n开放数据与代码共享：践行开放科学理念，将实验数据和代码公开于可信赖的平台，使全球同行能够访问、检验和复现研究成果，从而推动科学研究的不断进步。",
    "crumbs": [
      "课程简介",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>什么是可重复性研究</span>"
    ]
  },
  {
    "objectID": "what-is-reproducible-study.html#结语",
    "href": "what-is-reproducible-study.html#结语",
    "title": "1  什么是可重复性研究",
    "section": "1.4 结语",
    "text": "1.4 结语\n在科学探索的道路上，确保研究结果的可重复性是提高研究质量和推动学术进步的基石。通过标准化实验设计、统一数据格式、规范数据预处理和透明数据分析，我们能够有效应对当前的可重复性危机。构建跨平台、标准化的研究环境不仅有助于验证现有结论，更为未来科学创新奠定了坚实基础。只有全社会共同推动可重复性研究的落实，才能在全球范围内建立起一个开放、透明且可信的科学研究生态。",
    "crumbs": [
      "课程简介",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>什么是可重复性研究</span>"
    ]
  },
  {
    "objectID": "project-introduction.html",
    "href": "project-introduction.html",
    "title": "2  课程项目简介",
    "section": "",
    "text": "2.1 项目一：配置可重复的数据分析环境（2学时）",
    "crumbs": [
      "课程简介",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程项目简介</span>"
    ]
  },
  {
    "objectID": "project-introduction.html#项目一配置可重复的数据分析环境2学时",
    "href": "project-introduction.html#项目一配置可重复的数据分析环境2学时",
    "title": "2  课程项目简介",
    "section": "",
    "text": "2.1.1 上手实操\n在自己电脑上配置一个可重复的数据分析环境。\n\n\n2.1.2 课后作业\n\n确保电脑上安装了 R、Python 等需要用到的软件\n请在雨课堂内完成分组（第二次上课时确认）\n请与助教取得联系，建立学习小组",
    "crumbs": [
      "课程简介",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程项目简介</span>"
    ]
  },
  {
    "objectID": "project-introduction.html#项目二r-语言统计分析和可视化基础2学时",
    "href": "project-introduction.html#项目二r-语言统计分析和可视化基础2学时",
    "title": "2  课程项目简介",
    "section": "2.2 项目二：R 语言统计分析和可视化基础（2学时）",
    "text": "2.2 项目二：R 语言统计分析和可视化基础（2学时）\n以 ISME J 的论文数据为例(Gao, Cao, Cai, et al. 2021)，介绍分组数据统计分析和可视化的技巧。\n\n2.2.1 论文简介\n\n研究背景\n科学问题\n结果与分析\n\n\n\n2.2.2 上手实操\n\nggplot2 软件包及图形语法\n统计分析的数学基础\n数据集的介绍\nggpubr 软件包的应用\n\n\n\n2.2.3 课后作业\n\n阅读论文，并且查阅 GitHub 仓库，在自己电脑中复现论文中的分析结果\n将分析结果整理成 PDF 文档，提交到我的文件收集中（使用坚果云收件箱二维码收作业）",
    "crumbs": [
      "课程简介",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程项目简介</span>"
    ]
  },
  {
    "objectID": "project-introduction.html#项目三转录组学数据分析和可视化4学时",
    "href": "project-introduction.html#项目三转录组学数据分析和可视化4学时",
    "title": "2  课程项目简介",
    "section": "2.3 项目三：转录组学数据分析和可视化（4学时）",
    "text": "2.3 项目三：转录组学数据分析和可视化（4学时）\n以 ISME Communications 的论文数据为例(Gao, Cao, Ju, et al. 2021)，介绍转录组学数据分析的技巧。\n\n2.3.1 论文简介\n\n研究背景\n科学问题\n转录组学分析的基本原理\n\n\n\n2.3.2 上手实操\n\n转录组学分析流程\n\n数据预处理\n数据标准化\n差异表达分析\n差异表达基因注释\n差异表达基因可视化\n差异表达基因富集分析\n\nBioconductor 软件（DESeq2、ClusterProfiler、enrichplot）\n\nBioconductor 与 CRAN 的关系\n如何安装 Bioconductor 软件包\n\n基因功能数据库（KEGG、GO、CAZyme、ARG、COG）\n\n\n\n2.3.3 课后作业\n\n阅读论文，并且查阅 GitHub 仓库，在自己电脑中复现论文中的分析结果\n将分析结果整理成 PDF 文档，提交到我的文件收集中（使用坚果云收件箱二维码收作业）",
    "crumbs": [
      "课程简介",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程项目简介</span>"
    ]
  },
  {
    "objectID": "project-introduction.html#项目四微生物组数据分析及可视化4学时",
    "href": "project-introduction.html#项目四微生物组数据分析及可视化4学时",
    "title": "2  课程项目简介",
    "section": "2.4 项目四：微生物组数据分析及可视化（4学时）",
    "text": "2.4 项目四：微生物组数据分析及可视化（4学时）\n本项目基于 dada2 软件包提供的示例数据，展示如何使用 R 语言进行微生物组数据分析及可视化。\n\n2.4.1 主要内容\n\ndada2 软件包介绍\n微生物组数据分析流程\n微生物组数据可视化\n\n\n\n2.4.2 上手实操\n\n安装 dada2 软件包\n加载示例数据\n\n\n\n2.4.3 课后作业\n\n请将代码整理成 PDF 文档，提交到我的文件收集中（使用坚果云收件箱二维码收作业）",
    "crumbs": [
      "课程简介",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程项目简介</span>"
    ]
  },
  {
    "objectID": "project-introduction.html#项目五大模型的api调用4学时",
    "href": "project-introduction.html#项目五大模型的api调用4学时",
    "title": "2  课程项目简介",
    "section": "2.5 项目五：大模型的API调用（4学时）",
    "text": "2.5 项目五：大模型的API调用（4学时）\n本项目将展示如何利用大模型的API调用，实现文本生成、对话、翻译、代码生成等任务。\n\n2.5.1 主要内容\n\nHuggingface\nOpenAI（ChatAnywhere）\n阿里云百炼\n\n\n\n2.5.2 上手实操\n\n使用 R 调用 OpenAI API\n使用 Python 调用 OpenAI API\n使用 R 和 Python 调用 HuggingFace API\n使用 R 和 Python 调用阿里云百炼 API\n\n\n\n2.5.3 课后作业\n\n申请阿里云百炼 API KEY，配置 Chatbox 桌面应用\n使用 Chatbox 桌面应用，选择 DeepSeek 模型，与 AI 对话",
    "crumbs": [
      "课程简介",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程项目简介</span>"
    ]
  },
  {
    "objectID": "project-introduction.html#项目六手搓神经网络模型4学时",
    "href": "project-introduction.html#项目六手搓神经网络模型4学时",
    "title": "2  课程项目简介",
    "section": "2.6 项目六：手搓神经网络模型（4学时）",
    "text": "2.6 项目六：手搓神经网络模型（4学时）\n本项目将展示如何利用 PyTorch 从零开始构建一个神经网络模型，以解决手写字母识别问题。\n\n2.6.1 主要内容\n\n神经网络模型基础\n手写字母识别的历史\n评估模型的准确率\n\n\n\n2.6.2 上手实操\n\n配置一个可用的 Conda 环境\n安装需要的软件 Pytorch 和 torchvision\n下载 MNIST 数据集\n从零搭建一个神经网络模型\n评估模型的准确率\n\n\n\n2.6.3 课后作业\n\n将模型替换为其他模型，如 VGG19，并评估其针对于同一数据集预测时准确率。",
    "crumbs": [
      "课程简介",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程项目简介</span>"
    ]
  },
  {
    "objectID": "project-introduction.html#项目七利用拉曼光谱识别病原菌4学时",
    "href": "project-introduction.html#项目七利用拉曼光谱识别病原菌4学时",
    "title": "2  课程项目简介",
    "section": "2.7 项目七：利用拉曼光谱识别病原菌（4学时）",
    "text": "2.7 项目七：利用拉曼光谱识别病原菌（4学时）\n本项目以一篇 Nature Communications 上面发表的论文为例，展示神经网络模型如何应用于拉曼数据分析，从而实现基于拉曼光谱数据的病原菌识别(Ho et al. 2019)。\n\n2.7.0.1 主要内容\n\n拉曼光谱技术\nResNet 及残差网络\n以 NC 的病原菌检测论文数据为例\n\n\n\n2.7.0.2 上手实操\n\n数据预处理\n数据标准化\n数据可视化\n拉曼光谱数据的分析\n利用 ResNet 进行分类\n\n\n\n2.7.1 课后作业\n\n请将代码整理成 PDF 文档，提交到我的文件收集中（使用坚果云收件箱二维码收作业）",
    "crumbs": [
      "课程简介",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程项目简介</span>"
    ]
  },
  {
    "objectID": "project-introduction.html#项目八计算机视觉分析实验图片4学时",
    "href": "project-introduction.html#项目八计算机视觉分析实验图片4学时",
    "title": "2  课程项目简介",
    "section": "2.8 项目八：计算机视觉分析实验图片（4学时）",
    "text": "2.8 项目八：计算机视觉分析实验图片（4学时）\n本项目以发表在 Plant Phenomics 杂志上的一篇论文为例(Serouart et al. 2022)，讲述如何使用一种名为 SegVeg 的两阶段语义分割方法，将高分辨率 RGB 图像分割成背景、绿色植被和衰老植被三类。以用来评估植被的生长状态。\n\n2.8.1 主要内容\n\n使用 U-net 模型将图像分为植被和背景。\n使用支持向量机（SVM）将植被像素进一步分为绿色和衰老植被。\n\n\n\n2.8.2 上手实操\n\n\n2.8.3 课后作业",
    "crumbs": [
      "课程简介",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程项目简介</span>"
    ]
  },
  {
    "objectID": "project-introduction.html#项目九机器学习算法及实现4学时",
    "href": "project-introduction.html#项目九机器学习算法及实现4学时",
    "title": "2  课程项目简介",
    "section": "2.9 项目九：机器学习算法及实现（4学时）",
    "text": "2.9 项目九：机器学习算法及实现（4学时）\n本项目以 Science 论文为例(Chang et al. 2023)，展示如何利用机器学习算法识别细菌类型。\n\n线性回归及其变种\n决策树、随机森林\n以熔解曲线数据建模为例\n\n\n2.9.1 上手实操\n\n\n2.9.2 课后作业\n\n请将代码整理成 PDF 文档，提交到我的文件收集中（使用坚果云收件箱二维码收作业）",
    "crumbs": [
      "课程简介",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程项目简介</span>"
    ]
  },
  {
    "objectID": "project-introduction.html#项目十与全世界开发者协作4学时",
    "href": "project-introduction.html#项目十与全世界开发者协作4学时",
    "title": "2  课程项目简介",
    "section": "2.10 项目十：与全世界开发者协作（4学时）",
    "text": "2.10 项目十：与全世界开发者协作（4学时）\n本项目将展示如何使用 GitHub 进行版本控制，利用 GitHub Issue 与开发者沟通，利用 GitHub Pull Request 贡献代码，以及利用 GitHub Actions 实现自动化部署。\n\n2.10.1 主要内容\n\nGit 介绍\nGitHub 介绍\nGit 与 GitHub 的关系\n\n\n\n2.10.2 上手实操\n\n安装 Git\n配置 Git\n创建本地仓库\n添加文件到仓库\n提交更改到仓库\n创建远程仓库\n推送本地仓库到远程仓库\n克隆远程仓库到本地\n使用 GitHub Issues\n使用 GitHub Fork\n使用 GitHub Pull Request\n使用 GitHub Actions\n\n\n\n2.10.3 课后作业\n\n使用 Git 管理自己的代码，并提交到 GitHub 上；\n在课程 GitHub 群组提交 GitHub Issue，完成交作业任务。",
    "crumbs": [
      "课程简介",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程项目简介</span>"
    ]
  },
  {
    "objectID": "project-introduction.html#项目十一开发-r-包4学时",
    "href": "project-introduction.html#项目十一开发-r-包4学时",
    "title": "2  课程项目简介",
    "section": "2.11 项目十一：开发 R 包（4学时）",
    "text": "2.11 项目十一：开发 R 包（4学时）\n本项目以 ggVennDiagram 软件包开发为例，展示如何使用工具进行 R 包开发。\n\ndevtools\nCRAN/Bioconductor\n\n本项目将展示如何使用现代工具进行学术写作、R包开发、AI辅助编程，以及项目实践与展示。\n\n2.11.0.1 主要内容\n\nR包开发基础\n\n包结构设计\n函数文档编写\n单元测试\nCRAN发布规范\n\n\n\n\n2.11.0.2 上手实操\n\n安装 RStudio\n安装 Rtools\n安装 devtools\n使用 RStudio 进行 R 包开发\n\n创建 R 包\n添加函数\n编写文档\n单元测试\n发布到 GitHub\n从 GitHub 中安装 R 包\n\n\n\n\n2.11.1 课后作业\n\n写一个 R 包，并发布到 GitHub/Gitee 上\n\n\n\n\n\nChang, Chang-Yu, Djordje Bajić, Jean C. C. Vila, Sylvie Estrela, and Alvaro Sanchez. 2023. “Emergent Coexistence in Multispecies Microbial Communities.” Science 381 (6655): 343–48. https://doi.org/10.1126/science.adg0727.\n\n\nGao, Chun-Hui, Hui Cao, Peng Cai, and Søren J. Sørensen. 2021. “The Initial Inoculation Ratio Regulates Bacterial Coculture Interactions and Metabolic Capacity.” ISME Journal 15 (1): 29–40. https://doi.org/10.1038/s41396-020-00751-7.\n\n\nGao, Chun-Hui, Hui Cao, Feng Ju, Ke-Qing Xiao, Peng Cai, Yichao Wu, and Qiaoyun Huang. 2021. “Emergent Transcriptional Adaption Facilitates Convergent Succession Within a Synthetic Community.” ISME Communications 1 (1): 46. https://doi.org/10.1038/s43705-021-00049-5.\n\n\nHo, Chi-Sing, Neal Jean, Catherine A. Hogan, Lena Blackmon, Stefanie S. Jeffrey, Mark Holodniy, Niaz Banaei, Amr A. E. Saleh, Stefano Ermon, and Jennifer Dionne. 2019. “Rapid Identification of Pathogenic Bacteria Using Raman Spectroscopy and Deep Learning.” Nature Communications 10 (1): 4927. https://doi.org/10.1038/s41467-019-12898-9.\n\n\nSerouart, Mario, Simon Madec, Etienne David, Kaaviya Velumani, Raul Lopez Lozano, Marie Weiss, and Frédéric Baret. 2022. “SegVeg: Segmenting RGB Images into Green and Senescent Vegetation by Combining Deep and Shallow Methods.” Plant Phenomics (Washington, D.C.) 2022: 9803570. https://doi.org/10.34133/2022/9803570.",
    "crumbs": [
      "课程简介",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程项目简介</span>"
    ]
  },
  {
    "objectID": "reproducible-data-analysis-environment.html",
    "href": "reproducible-data-analysis-environment.html",
    "title": "3  可重复性数据分析环境",
    "section": "",
    "text": "3.1 硬件配置",
    "crumbs": [
      "环境搭建",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>可重复性数据分析环境</span>"
    ]
  },
  {
    "objectID": "reproducible-data-analysis-environment.html#硬件配置",
    "href": "reproducible-data-analysis-environment.html#硬件配置",
    "title": "3  可重复性数据分析环境",
    "section": "",
    "text": "计算平台：明确所使用的 CPU 与 GPU 型号及性能参数，确保在不同环境下计算能力的一致性。\n\nCPU 架构：注明采用的 CPU 架构（如 x86、AMD64 或 ARM），以防因架构差异影响运算效率和兼容性。\n\nGPU 加速技术：记录 GPU 加速技术（例如 CUDA 或 MPS）及其版本信息，确保相关计算过程可被精确复现。",
    "crumbs": [
      "环境搭建",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>可重复性数据分析环境</span>"
    ]
  },
  {
    "objectID": "reproducible-data-analysis-environment.html#操作系统",
    "href": "reproducible-data-analysis-environment.html#操作系统",
    "title": "3  可重复性数据分析环境",
    "section": "3.2 操作系统",
    "text": "3.2 操作系统\n选择并记录稳定的操作系统版本（如 Linux、Windows 或 macOS），并说明系统配置及更新策略，确保不同操作系统间实验条件一致。",
    "crumbs": [
      "环境搭建",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>可重复性数据分析环境</span>"
    ]
  },
  {
    "objectID": "reproducible-data-analysis-environment.html#软件框架",
    "href": "reproducible-data-analysis-environment.html#软件框架",
    "title": "3  可重复性数据分析环境",
    "section": "3.3 软件框架",
    "text": "3.3 软件框架\nPython 环境和 R 环境是数据分析的两大主流框架，分别提供了丰富的数据处理、可视化和建模工具。在进行数据分析时，不仅可以直接使用他们提供的模块或者包，还可以通过编写脚本或者自定义函数来实现更加复杂的数据处理和分析任务。",
    "crumbs": [
      "环境搭建",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>可重复性数据分析环境</span>"
    ]
  },
  {
    "objectID": "reproducible-data-analysis-environment.html#环境变量",
    "href": "reproducible-data-analysis-environment.html#环境变量",
    "title": "3  可重复性数据分析环境",
    "section": "3.4 环境变量",
    "text": "3.4 环境变量\n\n3.4.1 环境变量管理\n环境变量用于存储系统和用户的配置信息，并影响程序的运行行为。熟悉环境变量的管理，对于定制工作环境和编写脚本具有重要意义。\n\n\n\n\n\n\nTip\n\n\n\n如何查看环境变量？\n\nmacOS 和 Linux（包括 Ubuntu）\n终端命令：\n\nprintenv 或 env：列出所有环境变量。\necho $VAR_NAME：查看某个环境变量（如 echo $PATH）。\nexport：显示当前 shell 会话的所有导出环境变量。\n\n配置文件：\n\n~/.bashrc、~/.bash_profile（Bash）\n~/.zshrc（Zsh）\n/etc/environment（全局环境变量）\n\nWindows\n命令提示符（CMD）：\n\nset：列出所有环境变量。\necho %VAR_NAME%：查看某个环境变量（如 echo %PATH%）。\n\nPowerShell：\n\nGet-ChildItem Env: 或 gci Env:：列出所有环境变量。\n$env:VAR_NAME：查看某个环境变量（如 $env:PATH）。\n\nGUI（图形界面）：\n\nWindows 10/11：\n\n\n右键“此电脑” → 选择“属性” → “高级系统设置” → “环境变量”。\n\n\nWindows 7：\n\n\n右键“计算机” → 选择“属性” → “高级系统设置” → “环境变量”。\n\nR 中查看环境变量\n\nSys.getenv()：列出所有环境变量。\nSys.getenv(\"VAR_NAME\")：查看某个环境变量（如 Sys.getenv(\"PATH\")）。\nSys.setenv(VAR_NAME = \"value\")：设置环境变量（仅当前会话有效）。\n\nPython 中查看环境变量\n\nimport os\nos.environ：查看所有环境变量（字典类型）。\nos.getenv(\"VAR_NAME\")：获取某个环境变量（如 os.getenv(\"PATH\")）。\nos.environ[\"VAR_NAME\"] = \"value\"：设置环境变量（仅当前会话有效）。\n\n\n\n\n\n3.4.1.1 常见环境变量\n常见的环境变量包括 PATH（定义可执行文件搜索路径）、HOME（当前用户的主目录）以及 USER（当前用户名）。\n例如，可以使用下面的命令查看 PATH 环境变量，了解系统搜索命令时所依赖的路径：\necho $PATH\n执行后，将输出类似如下的信息，表示系统搜索命令时所依赖的路径：\n/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games\n类似地，使用下列命令可以查看用户主目录：\necho $HOME\n运行后，将输出类似如下的信息，表示用户主目录：\n/home/username\n\n\n3.4.1.2 环境变量设置方式\n环境变量可以临时设置，也可以永久保存。临时设置只在当前会话中有效，例如：\nexport MY_VAR=\"HelloWorld\"\necho $MY_VAR\n运行后，将输出类似如下的信息，表示环境变量设置成功：\nHelloWorld",
    "crumbs": [
      "环境搭建",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>可重复性数据分析环境</span>"
    ]
  },
  {
    "objectID": "reproducible-data-analysis-environment.html#文件系统",
    "href": "reproducible-data-analysis-environment.html#文件系统",
    "title": "3  可重复性数据分析环境",
    "section": "3.5 文件系统",
    "text": "3.5 文件系统\n建立统一且清晰的文件目录结构，有助于数据有序管理和团队协作。合理规划目录不仅方便成员快速查找和定位数据，还能有效降低因路径混乱带来的操作风险。\n\n\n\n\n\n\nTip\n\n\n\n文件系统结构\nLinux 和 Windows 的文件系统设计有一些显著的区别，主要体现在结构、灵活性和性能等方面。Linux 文件系统设计在多样性、性能、安全性和可扩展性等方面具有明显优势。其开放源代码的特性使得用户可以根据具体需求进行高度定制，适用于各种不同的应用场景。\n其中，最直观的区别是 Linux 文件系统采用层级目录结构（如根目录 /、/home、/etc等），而 Windows 则使用驱动器字母（如 C:\\Program Files、D:）和文件路径。这种层级化设计使得Linux 系统在文件管理上更加一致和易于维护，同时避免了Windows中的驱动器字母和路径名可能引发的混乱。\nLinux 文件系统采用树形结构，从根目录 / 开始，所有文件和目录均位于该根目录下。常见目录包括：\n\n/bin 和 /sbin：存放系统启动和维护必需的命令和工具。\n/etc：存放系统的配置文件，如网络配置、用户账号等。\n/var：存放经常变化的数据，如日志文件和缓存。\n/usr：存放系统应用程序和共享库文件。\n/home：存放各个用户的个人文件和配置文件。\n\n理解这些目录的结构，有助于定位配置文件、管理日志以及维护系统安全。例如，可以使用以下命令查看 /etc 目录的内容：\nls -l /etc\n运行后，会列出 /etc 中所有文件和子目录的详细信息，帮助用户了解系统配置的存放位置及相关权限设置。\ntotal 28\ndrwxr-xr-x 2 user user 4096 Feb  8 10:00 Documents\n-rw-r--r-- 1 user user  123 Feb  8 10:01 file1.txt\n-rwxr-xr-x 1 user user  456 Feb  8 10:02 script.sh\ndrwxr-xr-x 4 user user 4096 Feb  8 10:03 src\n在 Linux 系统中，~ 表示当前用户的主目录，/ 表示根目录，. 表示当前目录，.. 表示上级目录。用户可以通过这些特殊符号快速定位文件和目录，提高工作效率。这些符号现在在许多操作系统和软件中都得到了广泛应用，是理解和使用文件系统的重要基础。",
    "crumbs": [
      "环境搭建",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>可重复性数据分析环境</span>"
    ]
  },
  {
    "objectID": "reproducible-data-analysis-environment.html#软件依赖",
    "href": "reproducible-data-analysis-environment.html#软件依赖",
    "title": "3  可重复性数据分析环境",
    "section": "3.6 软件依赖",
    "text": "3.6 软件依赖\n借助 conda、renv 或操作系统自带的包管理器（如 apt、brew、winget）管理软件依赖，确保各软件包版本一致。",
    "crumbs": [
      "环境搭建",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>可重复性数据分析环境</span>"
    ]
  },
  {
    "objectID": "reproducible-data-analysis-environment.html#数据规范",
    "href": "reproducible-data-analysis-environment.html#数据规范",
    "title": "3  可重复性数据分析环境",
    "section": "3.7 数据规范",
    "text": "3.7 数据规范\n\n3.7.1 命名规范\n采用一致且具有描述性的命名规则，无论是文件、变量还是函数，都应遵循明确的标准。良好的命名习惯能够提升代码的可读性和维护性，同时减少因命名歧义导致的错误。\n\n\n3.7.2 版本管理\n利用 Git 等版本控制系统记录代码和数据的每一次变更，确保项目各阶段开发轨迹清晰可查。版本管理不仅支持团队协作，也能在问题发生时迅速回溯到历史版本，便于定位和修复错误。\n\n\n3.7.3 注释与文档说明\n对复杂算法和逻辑进行充分注释，详细解释每段代码的功能和实现思路，不仅能提升代码可读性，也便于后续维护。同时，配合外部文档记录设计决策与关键流程，有助于项目整体理解。",
    "crumbs": [
      "环境搭建",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>可重复性数据分析环境</span>"
    ]
  },
  {
    "objectID": "reproducible-data-analysis-environment.html#代码编写",
    "href": "reproducible-data-analysis-environment.html#代码编写",
    "title": "3  可重复性数据分析环境",
    "section": "3.8 代码编写",
    "text": "3.8 代码编写\n\n3.8.1 README 文件\n每个项目均应附带一个 README 文件，详细介绍项目背景、目标、依赖环境、安装方法及基本使用指南。清晰的 README 可帮助新用户快速了解项目核心，并顺利上手。\n\n\n3.8.2 代码注释和文档\n在代码中添加清晰、简洁的注释，解释每个函数的作用、输入输出及关键逻辑。同时，编写函数文档字符串，描述函数用途、参数和返回值，有助于自动化测试和代码审查。\n\n\n3.8.3 完整项目文档\n此外，还需编写全面的项目文档，记录设计决策、功能说明、操作步骤及扩展指南。系统化的文档体系为团队协作和后续维护提供了详实参考。\n\n\n\n\n\n\nTip\n\n\n\n文档撰写工具\n采用 R Markdown、Jupyter Notebook、Quarto 等工具撰写和管理项目文档，可以确保文档与代码始终保持同步更新。",
    "crumbs": [
      "环境搭建",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>可重复性数据分析环境</span>"
    ]
  },
  {
    "objectID": "reproducible-data-analysis-environment.html#版本控制",
    "href": "reproducible-data-analysis-environment.html#版本控制",
    "title": "3  可重复性数据分析环境",
    "section": "3.9 版本控制",
    "text": "3.9 版本控制\n采用 Git、GitHub 或 Gitee 等平台进行版本管理，详细记录每次代码提交及变更内容，确保团队协作和历史回溯的便捷性。\n\n\n\n\n\n\nNote\n\n\n\nGit 与开源协作\nGit 由 Linus Torvalds 为管理 Linux 内核源代码而开发，现已成为最流行的分布式版本控制系统。Git 的高效性能和强大功能不仅革新了代码管理方式，也推动了开源项目的蓬勃发展。Git 的成功故事充分展示了开源协作的力量和技术创新的魅力。",
    "crumbs": [
      "环境搭建",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>可重复性数据分析环境</span>"
    ]
  },
  {
    "objectID": "reproducible-data-analysis-environment.html#小结",
    "href": "reproducible-data-analysis-environment.html#小结",
    "title": "3  可重复性数据分析环境",
    "section": "3.10 小结",
    "text": "3.10 小结\n\n3.10.1 数据集公开\n共享数据集及其详细说明文档，确保其他研究者能够正确获取、理解和使用数据。数据说明应涵盖数据来源、格式、预处理方法及潜在局限性。\n\n\n3.10.2 完整代码共享\n公开所有分析代码，包括数据处理、计算逻辑及函数定义。代码透明是确保结果复现的基础，能够让他人准确理解每个步骤的实现细节。\n\n\n3.10.3 复现过程记录\n详细记录从环境配置、依赖安装到每一步分析过程的操作细节，包括实验过程中出现的变更和结果差异。完整记录是保证在不同平台上准确复现实验的关键。\n通过以上多层面的系统设计与严格记录，我们不仅能构建一个高效、透明的数据分析环境，还能为科研成果的准确复现提供坚实保障。",
    "crumbs": [
      "环境搭建",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>可重复性数据分析环境</span>"
    ]
  },
  {
    "objectID": "setup-a-reproducible-environment.html",
    "href": "setup-a-reproducible-environment.html",
    "title": "4  环境配置",
    "section": "",
    "text": "4.1 配置一个可重复的数据分析环境\n在自己电脑上配置一个可重复的数据分析环境。",
    "crumbs": [
      "环境搭建",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>环境配置</span>"
    ]
  },
  {
    "objectID": "setup-a-reproducible-environment.html#配置一个可重复的数据分析环境",
    "href": "setup-a-reproducible-environment.html#配置一个可重复的数据分析环境",
    "title": "4  环境配置",
    "section": "",
    "text": "安装脚本语言工具 R\n\n安装 R（最新版本）\n安装 Rtools（对应版本）\n安装 tidyverse R 包（CRAN）\n安装 DEseq2 R 包（Bioconductor）\n安装 ggVennDiagram R 包（GitHub）\n\n安装 Conda（Anaconda/Miniconda）和 Python（3.10 版本）\n\n安装 Anaconda/Miniconda，创建一个 Python 3.10 的环境\n使用 Conda 安装 matplotlib 模块\n使用 Pip 安装 htseq 模块\n\n安装 Git，注册 GitHub/Gitee 账号\n\n安装 Git\n注册 GitHub（国际）/Gitee（国内）账号\n加入课程群组（QQ 群：973581293，GitHub Organization：https://github.com/D2RS-2025spring）\n\n安装 Quarto\n\n下载安装 Quarto\n在 VSCode 中配置 Quarto\n创建一个“Hello World”项目\n\n安装和配置 VSCode\n\n安装 VSCode\n安装 R 插件（R Language Support）\n安装 Python 插件（Python 和可用环境）\n安装 Git 插件\n安装 Quarto 插件\n安装 Cline 插件或 CodeGeeX 插件\n\n安装 IDE：RStudio/VSCode/Cursor\n\n下载安装 RStudio/VSCode/Cursor\n配置 VSCode/Cursor\n\n安装 R 插件（R Language Support）\n安装 Python 插件（Python 和可用环境）\n安装 Git 插件\n安装 Quarto 插件\n安装 Cline 插件或 CodeGeeX 插件",
    "crumbs": [
      "环境搭建",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>环境配置</span>"
    ]
  },
  {
    "objectID": "setup-a-reproducible-environment.html#课后作业",
    "href": "setup-a-reproducible-environment.html#课后作业",
    "title": "4  环境配置",
    "section": "4.2 课后作业",
    "text": "4.2 课后作业\n\n确保电脑上安装了 R、Python 等需要用到的软件\n请在雨课堂内完成分组（第二次上课时确认）\n请与助教取得联系，建立学习小组",
    "crumbs": [
      "环境搭建",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>环境配置</span>"
    ]
  },
  {
    "objectID": "setup-a-reproducible-environment.html#课后作业解答",
    "href": "setup-a-reproducible-environment.html#课后作业解答",
    "title": "4  环境配置",
    "section": "4.3 课后作业解答",
    "text": "4.3 课后作业解答\n下面是根据提纲展开的详细操作步骤说明，每一步均附有具体的操作说明和注意事项，帮助你在电脑上搭建一个可重复的数据分析环境。\n\n4.3.1 安装脚本语言工具 R\n\n4.3.1.1 安装 R（最新版本）\n\n操作说明：\n\n打开浏览器，访问 R 官方网站。\n根据你的操作系统（Windows、macOS 或 Linux）选择合适的安装包下载最新版本的 R。\n运行下载的安装程序，并按照向导完成安装。建议使用默认安装路径，以免后续配置出错。\n安装完成后，可在命令行或 RStudio 中输入 R --version 检查 R 是否安装成功。\n\n\n\n\n4.3.1.2 安装 Rtools（对应版本）\n\n操作说明（仅限 Windows 用户）：\n\n访问 Rtools 官网。\n下载与已安装 R 版本匹配的 Rtools 版本（例如：Rtools42 对应 R 4.x）。\n运行安装程序，并在安装选项中勾选“将 Rtools 添加到系统 PATH”。\n完成安装后，可在命令行输入 gcc --version 来验证 Rtools 是否正确配置。\n\n\n\n\n4.3.1.3 安装 tidyverse R 包（CRAN）\n\n操作说明：\n\n启动 R 或 RStudio。\n在 R 控制台中输入以下命令以从 CRAN 安装 tidyverse：\ninstall.packages(\"tidyverse\")\n安装完成后，输入 library(tidyverse) 加载包，确保没有报错。\n\n\n\n\n4.3.1.4 安装 DEseq2 R 包（Bioconductor）\n\n操作说明：\n\n启动 R 或 RStudio。\n如果尚未安装 BiocManager，请先执行：\ninstall.packages(\"BiocManager\")\n然后使用 BiocManager 安装 DESeq2：\nBiocManager::install(\"DESeq2\")\n安装完成后，通过 library(DESeq2) 加载包进行验证。\n\n\n\n\n4.3.1.5 安装 ggVennDiagram R 包（GitHub）\n\n操作说明：\n\n启动 R 或 RStudio。\n如果没有安装 devtools，请先安装：\ninstall.packages(\"devtools\")\n使用 devtools 从 GitHub 安装 ggVennDiagram：\ndevtools::install_github(\"gaospecial/ggVennDiagram\")\n安装完成后，通过 library(ggVennDiagram) 加载包检查是否正常。\n\n\n\n\n\n\n\n\nTip\n\n\n\n给 R 包安装提速\n不管是安装 CRAN、Bioconductor 还是 GitHub 上面的 R 包，都需要首先把 R 包下载到本地电脑中才能进行。通过设置镜像网站，可以显著提升 R 包安装的速度。在 R 终端中输入下列命令，可以修改默认源到中科大的镜像。\noptions(\n  # 使用中科大镜像作为 CRAN 包的安装源\n  repos = c(CRAN = \"https://mirrors.ustc.edu.cn/CRAN/\"),\n\n  # 使用中科大镜像作为 Bioconductor 包的安装源\n  BioC_mirror = \"https://mirrors.ustc.edu.cn/bioc/\")\n将上述配置添加到 ~/.Rprofile 文件中，则可以在每次打开 R 的时候，自动修改为镜像服务器。\n\n\n\n\n\n\n4.3.2 安装 Conda（Anaconda/Miniconda）\n\n4.3.2.1 创建一个 Python 3.10 的环境\n\n操作说明：\n\n访问 Anaconda官网 或 Miniconda官网，下载适合你操作系统的安装包。\n根据安装向导完成安装。\n安装后，打开终端（Terminal）或 Anaconda Prompt，输入以下命令创建 Python 3.10 环境（此处环境名称可自定义，如 py310）：\nconda create -n py310 python=3.10\n激活新环境：\nconda activate py310\n\n\n\n\n4.3.2.2 使用 Conda 安装 matplotlib 模块\n\n操作说明：\n\n在激活的 Python 环境中，输入以下命令安装 matplotlib：\nconda install matplotlib\n安装完成后，可启动 Python，输入以下代码验证安装：\nimport matplotlib.pyplot as plt\nplt.plot([1, 2, 3], [4, 5, 6])\nplt.show()\n\n\n\n\n4.3.2.3 使用 Pip 安装 htseq 模块\n\n操作说明：\n\n在已激活的 Python 环境中，使用 pip 安装 htseq：\npip install htseq\n安装完成后，可在 Python 中输入以下代码测试：\nimport HTSeq\nprint(HTSeq.__version__)\n\n\n\n\n\n4.3.3 安装 Git，注册 GitHub/Gitee 账号\n\n4.3.3.1 安装 Git\n\n操作说明：\n\n访问 Git 官网 下载适合你操作系统的 Git 安装包。\n运行安装程序，并按提示完成安装。建议保留默认选项。\n安装完成后，在命令行输入 git --version 检查是否安装成功。\n\n\n\n\n4.3.3.2 注册 GitHub（国际）/Gitee（国内）账号\n\n操作说明：\n\nGitHub：\n\n访问 GitHub 注册页面。\n填写邮箱、用户名、密码等信息完成注册。\n\nGitee：\n\n访问 Gitee 注册页面。\n根据提示填写相关信息完成注册。\n\n\n\n\n\n4.3.3.3 加入课程群组\n\n操作说明：\n\nQQ 群： 打开 QQ 客户端，搜索群号 973581293，申请加入该群组。\nGitHub Organization： 访问 GitHub Organization 链接，按照页面提示申请加入或联系管理员。\n\n\n\n\n\n4.3.4 安装 Quarto\n\n4.3.4.1 下载安装 Quarto\n\n操作说明：\n\n访问 Quarto 官方网站。\n下载适用于你操作系统的 Quarto 安装包。\n运行安装程序并完成安装，建议在安装时勾选将 Quarto 添加到系统 PATH 中，以便在终端中直接调用。\n\n\n\n\n4.3.4.2 在 VSCode 中配置 Quarto\n\n操作说明：\n\n启动 VSCode，点击左侧的扩展图标（Extensions）。\n在扩展市场搜索 “Quarto”，找到并安装 Quarto 插件（通常为 “Quarto Language Support”）。\n安装完成后，插件通常会自动检测系统中的 Quarto 安装路径。如果未自动检测，可手动在 VSCode 设置中配置 Quarto 的路径。\n\n\n\n\n4.3.4.3 创建一个“Hello World”项目\n\n操作说明：\n\n在 VSCode 中新建一个文件，文件名以 .qmd 结尾（例如 hello.qmd）。\n输入以下内容作为基础模板：\n---\ntitle: \"Hello World\"\nformat: html\n---\n\n# Hello World\n\n欢迎使用 Quarto！\n保存文件后，可通过命令行执行 quarto render hello.qmd 或直接使用 VSCode 的 Quarto 插件渲染，生成 HTML 文件预览页面，确认输出正常。\n\n\n\n\n\n4.3.5 安装和配置 VSCode\n\n4.3.5.1 安装 VSCode\n\n操作说明：\n\n访问 Visual Studio Code 官网。\n下载适合你操作系统的安装包，并按照提示完成安装。\n安装完成后，启动 VSCode。\n\n\n\n\n4.3.5.2 安装 R 插件（R Language Support）\n\n操作说明：\n\n在 VSCode 左侧的扩展市场（Extensions）中搜索 “R Language” 或 “R Language Support”。\n找到相应插件后点击安装。\n插件安装后将为 VSCode 提供 R 语法高亮、代码补全和基本调试支持。\n\n\n\n\n4.3.5.3 安装 Python 插件（Python 和可用环境）\n\n操作说明：\n\n在扩展市场中搜索 “Python”（由 Microsoft 提供）。\n安装后，VSCode 会自动检测你系统中的 Python 解释器（包括 Conda 环境）。\n你可以在 VSCode 状态栏中点击 Python 版本，选择合适的解释器。\n\n\n\n\n4.3.5.4 安装 Git 插件\n\n操作说明：\n\n虽然 VSCode 内置基本的 Git 支持，但建议安装增强插件，如 “GitLens”。\n在扩展市场中搜索 “GitLens”，点击安装。\n安装完成后，你可以在 VSCode 中获得更加详细的 Git 版本历史、作者信息等功能。\n\n\n\n\n4.3.5.5 安装 Quarto 插件\n\n操作说明：\n\n在扩展市场中搜索 “Quarto”，安装 “Quarto Language Support” 插件。\n插件安装后，会自动识别 .qmd 文件，并提供预览、渲染支持。\n\n\n\n\n4.3.5.6 安装 Cline 插件或 CodeGeeX 插件\n\n操作说明：\n\n根据个人需求选择安装代码辅助插件：\n\nCline 插件 或\n\nCodeGeeX 插件\n\n在扩展市场中搜索相应插件名称，点击安装。\n按照插件提示进行简单配置（如有配置项），确保插件能在代码编辑中提供智能补全和建议。\n\n\n\n\n\n4.3.6 安装 IDE：RStudio/VSCode/Cursor\n\n4.3.6.1 下载安装 RStudio/VSCode/Cursor\n\n操作说明：\n\nRStudio：\n\n访问 RStudio 官网，下载适合你操作系统的 RStudio 桌面版安装包。\n按照向导完成安装。RStudio 专为 R 开发提供了良好的支持。\n\nVSCode：\n\n如果你已安装 VSCode，可直接使用上面配置的版本。\n\nCursor：\n\n如果你对 Cursor 感兴趣，访问其官网或应用市场，下载并安装最新版本。\n\n\n\n\n\n4.3.6.2 配置 VSCode/Cursor\n\n操作说明： 在 VSCode 或 Cursor 中进行以下插件安装和配置，确保你的开发环境满足所有需求：\n\n安装 R 插件（R Language Support）：\n\n在扩展市场中搜索并安装 R 语言插件，确保 R 文件具有语法高亮和运行支持。\n\n安装 Python 插件（Python 和可用环境）：\n\n确保已安装 “Python” 插件，并在 VSCode 状态栏选择正确的 Python 解释器（例如：之前创建的 Conda 环境）。\n\n安装 Git 插件：\n\n安装 “GitLens” 或其他 Git 扩展，增强 Git 的使用体验。\n\n安装 Quarto 插件：\n\n安装 “Quarto Language Support” 插件，以支持 Quarto 文件的编辑与渲染。\n\n安装 Cline 插件或 CodeGeeX 插件：\n\n根据个人喜好，安装其中一个代码辅助插件，提升代码编辑效率。\n\n测试配置：\n\n分别创建 R 脚本、Python 脚本和 Quarto 文件，确保各自的语法高亮、代码补全、调试及版本控制功能正常运行。\n\n重启 IDE：\n\n所有插件安装完成后，建议重启 VSCode/Cursor，以确保所有配置生效。\n\n\n\n通过以上步骤，你就可以在自己的电脑上构建一个集 R、Python、Git、Quarto 以及多种 IDE（如 RStudio、VSCode 或 Cursor）于一体的、可重复使用的数据分析环境。每一步的详细操作和说明将帮助你逐步配置好所有必备工具，确保在数据分析和科研工作中具备灵活、稳定的工作平台。",
    "crumbs": [
      "环境搭建",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>环境配置</span>"
    ]
  },
  {
    "objectID": "literate-programming.html",
    "href": "literate-programming.html",
    "title": "5  文学化编程",
    "section": "",
    "text": "5.1 Markdown\nMarkdown 是一种轻量级标记语言，旨在以最小的输入生成格式化的文档。它广泛用于编写文档、博客和README文件，因为其语法简单且易于阅读。Markdown的基本语法包括标题、列表、链接、图片和代码块等。\n实例说明：\nMarkdown的优势在于其简洁性和可读性。用户无需复杂的格式化指令即可创建结构化文档，这使得Markdown成为技术文档和博客的理想选择。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>文学化编程</span>"
    ]
  },
  {
    "objectID": "literate-programming.html#markdown",
    "href": "literate-programming.html#markdown",
    "title": "5  文学化编程",
    "section": "",
    "text": "# 标题\n## 副标题\n\n这是一个段落，包含**加粗**和*斜体*文本。\n\n- 列表项1\n- 列表项2\n\n[链接文本](http://example.com)\n\n![图片描述](http://example.com/image.jpg)\n\n```python\nprint(\"这是一个代码块\")\n```",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>文学化编程</span>"
    ]
  },
  {
    "objectID": "literate-programming.html#latex",
    "href": "literate-programming.html#latex",
    "title": "5  文学化编程",
    "section": "5.2 Latex",
    "text": "5.2 Latex\nLaTeX是一种用于高质量排版的文档准备系统，特别适合于生成复杂的数学公式和科学文档。LaTeX的强大之处在于其对文档结构的控制和对数学符号的支持。\n实例说明：\n\\documentclass{article}\n\\usepackage{amsmath}\n\n\\begin{document}\n\n\\section{介绍}\n这是一个简单的LaTeX文档。\n\n\\subsection{数学公式}\n一个著名的公式是爱因斯坦的质能方程：\n\\begin{equation}\nE = mc^2\n\\end{equation}\n\n\\end{document}\nLaTeX的优势在于其强大的排版能力和对复杂文档的支持，尤其是在学术界和出版行业中。尽管LaTeX的学习曲线较陡，但其输出质量和灵活性使其成为撰写学术论文和书籍的首选工具。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>文学化编程</span>"
    ]
  },
  {
    "objectID": "literate-programming.html#r-markdown",
    "href": "literate-programming.html#r-markdown",
    "title": "5  文学化编程",
    "section": "5.3 R Markdown",
    "text": "5.3 R Markdown\nR Markdown是R语言生态系统中的一部分，结合了Markdown的简洁性和R的强大数据分析能力。它允许用户在一个文档中集成文本、代码和输出结果，适用于生成动态报告和可重复的研究。\n实例说明：\n---\ntitle: \"R Markdown 示例\"\noutput: html_document\n---\n\n## 数据分析\n\n```{r}\nsummary(cars)\nplot(cars)\n```\nR Markdown的优势在于其无缝集成数据分析和文档生成的能力。用户可以在同一文档中编写分析代码、运行代码并直接查看结果，这使得R Markdown成为数据科学家和统计学家的强大工具。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>文学化编程</span>"
    ]
  },
  {
    "objectID": "literate-programming.html#python-notebook",
    "href": "literate-programming.html#python-notebook",
    "title": "5  文学化编程",
    "section": "5.4 Python Notebook",
    "text": "5.4 Python Notebook\nPython Notebook（通常指Jupyter Notebook）是一种交互式计算环境，允许用户在一个文档中编写和执行Python代码。它支持文本、代码、数学公式和可视化的集成，广泛用于数据分析、机器学习和教学。\n实例说明：\n```python\n# 导入库\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 数据生成\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\n# 绘图\nplt.plot(x, y)\nplt.title(\"Sine Wave\")\nplt.show()\n```\nPython Notebook的优势在于其交互性和灵活性。用户可以逐步执行代码块，查看即时输出，并进行数据可视化，这使得它成为数据科学和机器学习领域的标准工具。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>文学化编程</span>"
    ]
  },
  {
    "objectID": "literate-programming.html#quarto",
    "href": "literate-programming.html#quarto",
    "title": "5  文学化编程",
    "section": "5.5 Quarto",
    "text": "5.5 Quarto\nQuarto是一个新兴的开源工具，旨在统一Markdown、R Markdown和Jupyter Notebook的功能，支持多种编程语言（如R、Python、Julia等）。Quarto的目标是提供一个灵活的文档生成平台，适用于报告、博客和书籍。\n实例说明：\n---\ntitle: \"Quarto 示例\"\nformat: html\n---\n\n## 数据分析\n\n```{python}\nimport pandas as pd\n\ndata = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [4, 5, 6]\n})\ndata.describe()\n```\nQuarto的优势在于其多语言支持和灵活的输出格式。用户可以在同一文档中使用不同的编程语言，并生成多种格式的输出（如HTML、PDF、Word等），这使得Quarto成为跨学科团队协作和多格式发布的理想选择。\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>文学化编程</span>"
    ]
  },
  {
    "objectID": "cli.html",
    "href": "cli.html",
    "title": "6  命令行程序",
    "section": "",
    "text": "6.1 Linux 命令行\nShell 命令行是 Linux 系统的核心工具之一，它提供了用户与操作系统交互的接口。通过 Shell，用户可以输入命令、执行程序、管理文件和进程，实现系统管理和应用开发等功能。Shell 命令行具有高效、灵活和强大的特点，适用于各种场景，如系统管理、自动化脚本、远程连接等。\n例如，下面的命令展示了如何获取系统内核信息：\n运行后，系统会输出类似如下的信息，其中包含了内核版本、主机名、操作系统类型等数据，有助于用户了解当前的系统环境。\n随着云计算、DevOps 以及自动化运维的发展，Shell 命令行的重要性不断提升。命令行操作支持批量任务和脚本自动化，可以大幅提高工作效率；同时，许多现代工具和服务均提供命令行接口，使得跨平台管理成为可能。此外，命令行学习能够帮助开发者更好地理解系统底层原理，从而在问题排查和性能优化时更得心应手。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>命令行程序</span>"
    ]
  },
  {
    "objectID": "cli.html#linux-命令行",
    "href": "cli.html#linux-命令行",
    "title": "6  命令行程序",
    "section": "",
    "text": "uname -a\n\nLinux ubuntu 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\n\n\n6.1.1 常用命令概述与演示\nShell 命令通常由命令名称、选项和参数构成。无论是文件管理、进程监控还是网络配置，命令行都提供了直观而灵活的操作方式。命令行具有低资源消耗、脚本自动化、远程控制等优点，尤其适用于需要重复操作和批量处理的场景。\n例如，下面的命令使用 echo 输出一行文本到终端：\necho \"Hello, Linux!\"\n运行后，将直接在屏幕上显示 “Hello, Linux!”。这一简单例子展示了命令行的基本交互模式。\n\n6.1.1.1 帮助与文档系统\nLinux 系统中，每个命令通常都配有详细的帮助文档。常用的查看方式包括 man、info 和 --help 选项。使用 man 命令，可以查看命令的完整手册页：\nman ls\n执行后，将打开 ls 命令的详细手册，解释命令语法、选项和用法。\nLS(1)                                      General Commands Manual                                      LS(1)\n\nNAME\n     ls – list directory contents\n\nSYNOPSIS\n     ls [-@ABCFGHILOPRSTUWabcdefghiklmnopqrstuvwxy1%,] [--color=when] [-D format] [file ...]\n\nDESCRIPTION\n     For each operand that names a file of a type other than directory, ls displays its name as well as any\n     requested, associated information.  For each operand that names a file of type directory, ls displays\n     the names of files contained within that directory, as well as any requested, associated information.\n\n     If no operands are given, the contents of the current directory are displayed.  If more than one operand\n     is given, non-directory operands are displayed first; directory and non-directory operands are sorted\n     separately and in lexicographical order.\n\n     The following options are available:\n此外，许多命令支持 --help 参数来快速查看简明的帮助信息：\nls --help\n运行后，将输出类似如下的信息：\nusage: ls [-@ABCFGHILOPRSTUWXabcdefghiklmnopqrstuvwxy1%,] [--color=when] [-D format] [file ...]\n通过这些工具，用户可以深入了解命令功能和用法，从而更高效地使用 Linux 系统。\n\n\n6.1.1.2 目录与文件浏览\nls 命令用于列出目录中的文件和子目录。常用的 -l 选项可以显示详细信息，例如文件权限、所有者、文件大小和修改时间：\nls -l\n输出示例如下：\n-rw-r--r-- 1 user user  1234 Feb  8 10:00 file1.txt\ndrwxr-xr-x 2 user user  4096 Feb  8 10:05 dir1\n这种格式的输出有助于判断文件类型和权限设置。\n\n\n6.1.1.3 文件复制与移动\ncp 命令用于复制文件或目录。例如，将文件 file1.txt 复制为 file2.txt：\ncp file1.txt file2.txt\n若需要复制整个目录，则使用 -r（递归）选项：\ncp -r dir1/ dir2/\n而 mv 命令既可用于移动文件，也可用于重命名。例如，将 oldname.txt 重命名为 newname.txt：\nmv oldname.txt newname.txt\n或将文件移动到其他目录中：\nmv file1.txt /path/to/destination/\n\n\n6.1.1.4 文件删除\nrm 命令用于删除文件和目录。直接删除文件的命令如下：\nrm file1.txt\n删除目录则需添加 -r 选项进行递归删除：\nrm -r dir1/\n需要特别注意，rm 删除的文件通常无法恢复，因此务必谨慎使用。\n\n\n6.1.1.5 权限管理命令\n权限管理是保证系统安全的重要措施。Linux 中通过设置读、写、执行权限来控制文件和目录的访问。虽然在后续章节中会进一步讨论安全策略，这里只介绍基本命令的用法。\n使用 chmod 命令可以更改文件或目录的访问权限。例如，下面的命令将脚本文件设置为所有者具有全部权限，其他用户具有读和执行权限：\nchmod 755 script.sh\n而 chown 命令用于改变文件的所有者和所属组：\nchown user:group file1.txt\n运行后，file1.txt 的所有者和组信息会更新为指定的用户和组。\n\n\n6.1.1.6 文件内容查看\ncat 命令可以将文件的全部内容输出到终端。例如：\ncat file1.txt\n这对于查看小型文本文件非常方便。但如果文件较大，建议使用 less 命令进行分页查看：\nless /var/log/syslog\n在 less 中，用户可通过方向键、PageUp/PageDown 浏览内容，并通过按 q 退出。\n\n\n6.1.1.7 文件局部查看\n在某些场合，只需查看文件的部分内容。head 命令默认显示文件前 10 行：\nhead file1.txt\n而 tail 命令则显示文件末尾的 10 行：\ntail file1.txt\n通过加 -n 参数，还可以自定义显示的行数，例如 head -n 20 file1.txt 查看前 20 行内容。\n\n\n\n6.1.2 管道与重定向\nLinux 中的每个进程默认都有三个数据流：标准输入（stdin）、标准输出（stdout）和标准错误（stderr）。这三个数据流为程序间通信和调试提供了基本手段。\n\n标准输入 (STDIN)：默认来自键盘输入，可通过重定向从文件中读取。\n标准输出 (STDOUT)：默认输出到终端，可以重定向到文件。\n标准错误 (STDERR)：用于输出错误信息，同样默认输出到终端，但可以单独重定向。\n\n管道和重定向能够将多个命令的输入和输出等组合起来，实现数据的快速处理和转化。\n\n\n\n\n\n\nTip\n\n\n\nLinux 哲学\nLinux 强调简洁、模块化和”做一件事并做好它”。这种哲学要求每个工具只专注于单一功能，然后通过管道和组合构建出复杂系统。熟练掌握标准输入、输出和错误流的管理，有助于更好地理解和运用 Linux 命令行工具。\n\n\n管道符号 | 可将一个命令的标准输出直接传给另一个命令。例如，下列命令将 ls -l 的结果通过管道传递给 grep 命令，只筛选出包含 “txt” 的行：\nls -l | grep txt\n重定向符用于改变命令的输入输出。常见的重定向用法如下：\n\n输出重定向（覆盖文件）\n将命令的输出写入文件，如下命令将文本写入 output.txt（如果文件存在则覆盖）：\necho \"Hello, file!\" &gt; output.txt\n输出追加\n使用 &gt;&gt; 将输出追加到文件末尾：\necho \"Additional line\" &gt;&gt; output.txt\n输入重定向\n使用 &lt; 将文件内容作为命令的输入，例如对文件排序：\nsort &lt; unsorted.txt\n错误输出重定向\n使用 2&gt; 将标准错误输出写入文件，这将为调试和错误处理提供便利：\nls non_existing_file 2&gt; error.log\n\n综合使用管道和重定向，可以完成许多复杂任务。下面的例子展示如何统计当前目录下所有 .txt 文件的总行数，并将结果保存到文件中：\ncat *.txt | wc -l &gt; total_lines.txt\n该命令先用 cat *.txt 拼接所有文本文件内容，再通过 wc -l 统计行数，最后将总行数写入 total_lines.txt 文件中。\n\n\n6.1.3 软件包管理\n软件包管理是 Linux 系统用于安装、更新和卸载软件的重要机制。不同的发行版使用不同的管理工具。例如，在 Debian 和 Ubuntu 系统中，常用 apt 命令管理软件包。下面是一个更新软件包列表并安装 Vim 编辑器的示例：\nsudo apt update\nsudo apt install vim\n对于 CentOS 或 Fedora 系统，则通常使用 yum 或 dnf 命令，例如：\nsudo yum update\nsudo yum install nano\n这些命令帮助系统管理员快速管理和维护系统中所需的软件，确保系统的安全与稳定。\n\n\n6.1.4 编写 Shell 脚本\nShell 脚本是一系列命令的集合，用于自动化日常任务。脚本文件通常以 .sh 结尾，并以 Shebang（#!/bin/bash）开头，指明解释器路径。例如，一个简单的脚本如下：\n#!/bin/bash\necho \"This is a shell script\"\n要使脚本具备执行权限，需要使用 chmod 命令：\nchmod +x script.sh\n之后，可以通过 ./script.sh 运行该脚本，从而自动执行脚本内的所有命令。\n\n\n6.1.5 控制结构\nShell 脚本中常用的控制结构包括条件判断和循环，帮助实现更复杂的逻辑流程。\n\n6.1.5.1 条件判断\nif 语句允许根据条件判断执行不同的代码块。例如，下面的脚本检测当前目录中是否存在文件 file.txt：\nif [ -f \"file.txt\" ]; then\n  echo \"file.txt exists\"\nelse\n  echo \"file.txt does not exist\"\nfi\n根据 [ -f \"file.txt\" ] 的判断结果，脚本会输出相应的提示信息。\n\n\n6.1.5.2 循环结构\n循环结构可用于重复执行某项任务。例如，for 循环遍历当前目录下所有 .txt 文件：\nfor file in *.txt; do\n  echo \"Processing $file\"\ndone\n而 while 循环则基于条件不断执行，直至条件不满足。例如：\ncount=1\nwhile [ $count -le 5 ]; do\n  echo \"Count is $count\"\n  count=$((count + 1))\ndone\n该脚本将依次输出从 1 到 5 的数字。\n\n\n6.1.5.3 案例分析\n结合条件判断和循环，可以编写自动备份脚本。以下示例脚本将当前目录下所有 .log 文件复制到备份目录 backup 中，并输出每个文件的备份状态：\n#!/bin/bash\nbackup_dir=\"backup\"\nmkdir -p $backup_dir\nfor file in *.log; do\n  cp \"$file\" \"$backup_dir\"\n  echo \"Backed up $file\"\ndone\n执行该脚本后，系统会自动创建 backup 目录（如果不存在），并将所有日志文件复制过去，同时输出备份过程的详细信息。\n\n\n\n6.1.6 文件与目录权限管理\nLinux 文件权限通过读（r）、写（w）、执行（x）三个标志进行管理。使用 ls -l 命令可以查看文件和目录的权限设置：\nls -l\n运行后，将输出类似如下的信息：\ntotal 28\ndrwxr-xr-x 2 user user 4096 Feb  8 10:00 Documents\n-rw-r--r-- 1 user user  123 Feb  8 10:01 file1.txt\n-rwxr-xr-x 1 user user  456 Feb  8 10:02 script.sh\ndrwxr-xr-x 4 user user 4096 Feb  8 10:03 src\n该行显示 script.sh 的权限、所有者和组信息。使用 chmod 命令可修改权限，chown 命令则用于更改文件所有者。例如：\nchmod 755 script.sh\nchown user:group script.sh\n这些命令确保只有授权用户可以访问或修改关键文件。\n\n\n6.1.7 提升权限与 sudo\n在需要执行系统管理任务时，普通用户往往需要临时提升权限。sudo 命令允许用户在当前会话中以超级用户身份执行命令，而不必完全切换到 root 用户。例如：\nsudo apt update\n该命令会以管理员身份更新软件包列表。sudo 的配置通常保存在 /etc/sudoers 文件中，正确配置可以防止滥用并保证系统安全。\n\n\n\n\n\n\nTip\n\n\n\nZSH\nLinux 系统默认使用 Bash 作为 Shell 解释器，但也有其他 Shell 可供选择。例如，Zsh 是一种功能强大的 Shell，提供了更多的自定义选项和插件支持。Zsh 具有的一些独有特性包括：\n\n智能补全：支持更多的补全选项和提示功能。例如，输入 ls /u/g/b 后按 Tab 键，Zsh 会自动补全为 ls /usr/global/back。\n插件支持：Zsh 支持丰富的插件系统，用户可以根据需求安装和配置各种插件，扩展 Shell 功能。例如，oh-my-zsh 是一个流行的 Zsh 配置框架，提供了许多主题和插件。\n高级别别名：Zsh 支持更复杂的别名和函数定义，使得用户可以更灵活地定制 Shell 命令。例如，可以使用 alias 命令定义更复杂的别名，如 alias ll='ls -l'。\n高级别历史记录：Zsh 提供了更强大的历史记录功能，支持更多的历史命令操作和搜索选项。例如，可以使用 Ctrl+R 快捷键搜索历史命令。\n高级别参数展开：Zsh 支持更多的参数展开选项，如 ~ 展开为用户主目录、$var 展开为变量值等。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>命令行程序</span>"
    ]
  },
  {
    "objectID": "cli.html#powershell-命令行",
    "href": "cli.html#powershell-命令行",
    "title": "6  命令行程序",
    "section": "6.2 PowerShell 命令行",
    "text": "6.2 PowerShell 命令行\nShell 命令行在 Linux、MacOS 中得到广泛应用，而在 Windows 系统中，PowerShell 是主要的命令行工具。PowerShell 是一种脚本语言和命令行解释器，具有强大的脚本编写和系统管理功能，适用于 Windows 系统的管理和自动化。\nPowerShell 命令行与 Linux Shell 有许多相似之处，例如：\n\n基本命令：PowerShell 支持常见的文件和目录操作，如 cd、ls、mkdir、rm 等。\n管道和重定向：PowerShell 支持管道和重定向，可以将命令的输出传递给其他命令或文件。\n脚本编写：PowerShell 脚本使用 .ps1 扩展名，可以编写复杂的脚本和自动化任务。\n\n一些常用的 PowerShell 命令包括：\n\n获取当前目录：使用 Get-Location 命令获取当前目录的路径，例如：\nGet-Location\n列出目录内容：使用 Get-ChildItem 命令列出当前目录的文件和子目录，例如：\nGet-ChildItem\n创建目录：使用 New-Item 命令创建新目录，例如：\nNew-Item -ItemType Directory -Name NewFolder\n删除文件：使用 Remove-Item 命令删除文件或目录，例如：\nRemove-Item file.txt\n复制文件：使用 Copy-Item 命令复制文件或目录，例如：\nCopy-Item file1.txt file2.txt\n\nPowerShell 是为了取代 Windows 系统中最早的命令提示符程序（cmd.exe）而开发的。对标的是 Linux 系统的 Shell。然而，PowerShell 实际上用得非常有限。现在，即便在 Windows 系统上，也可以使用 WSL（Windows Subsystem for Linux）来运行 Linux Shell，这样可以更方便地使用 Linux 系统的命令行工具。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>命令行程序</span>"
    ]
  },
  {
    "objectID": "cli.html#conda-命令行",
    "href": "cli.html#conda-命令行",
    "title": "6  命令行程序",
    "section": "6.3 Conda 命令行",
    "text": "6.3 Conda 命令行\nConda 是一个流行的包管理工具，用于创建、管理和分享软件环境。通过 Conda，用户可以轻松安装 Python 包、R 包和其他软件，实现不同环境的隔离和管理。\n一些常见的 Conda 命令包括：\n\n6.3.1 环境管理\n\n创建环境：使用 conda create 命令创建新环境，例如：\nconda create --name myenv python=3.8\n激活环境：使用 conda activate 命令激活环境，例如：\nconda activate myenv\n查看可用环境：使用 conda env list 命令查看所有可用环境，例如：\nconda env list\n退出环境：使用 conda deactivate 命令退出当前环境，例如：\nconda deactivate\n导出环境：使用 conda env export 命令导出环境配置，例如：\nconda env export &gt; environment.yml\n导入环境：使用 conda env create 命令导入环境配置，例如：\nconda env create -f environment.yml\n\n\n\n6.3.2 软件包管理\n\n安装软件包：使用 conda install 命令安装软件包，例如：\nconda install numpy\n删除软件包：使用 conda remove 命令删除软件包，例如：\nconda remove numpy\n更新软件包：使用 conda update 命令更新软件包，例如：\nconda update numpy\n查看已安装软件包：使用 conda list 命令查看已安装的软件包，例如：\nconda list",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>命令行程序</span>"
    ]
  },
  {
    "objectID": "cli.html#git-命令行",
    "href": "cli.html#git-命令行",
    "title": "6  命令行程序",
    "section": "6.4 Git 命令行",
    "text": "6.4 Git 命令行\nGit 是一个分布式版本控制系统，广泛用于协作开发和代码管理。通过 Git，用户可以追踪文件的变更、协同工作、回滚历史版本等。\n在 Git 终端中，常用的命令包括：\n\n6.4.1 仓库管理\n\n初始化仓库：使用 git init 命令初始化新仓库，例如：\ngit init\n克隆仓库：使用 git clone 命令克隆远程仓库，例如：\ngit clone https://github.com/user/repo.git\n\n\n\n6.4.2 追踪更改\n\n添加文件：使用 git add 命令添加文件到暂存区，例如：\ngit add file.txt\n提交更改：使用 git commit 命令提交更改到仓库，例如：\ngit commit -m \"Add file.txt\"\n查看状态：使用 git status 命令查看仓库状态，例如：\ngit status\n查看历史：使用 git log 命令查看提交历史，例如：\ngit log\n拉取更新：使用 git pull 命令拉取远程更新，例如：\ngit pull origin main\n推送更改：使用 git push 命令推送更改到远程仓库，例如：\ngit push origin main\n\n\n\n6.4.3 分支管理\n\n创建分支：使用 git branch 命令创建新分支，例如：\ngit branch feature\n切换分支：使用 git checkout 命令切换分支，例如：\ngit checkout feature\n合并分支：使用 git merge 命令合并分支，例如：\ngit merge feature\n删除分支：使用 git branch -d 命令删除分支，例如：\ngit branch -d feature\n查看分支：使用 git branch 命令查看分支列表，例如：\ngit branch",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>命令行程序</span>"
    ]
  },
  {
    "objectID": "r-basic.html",
    "href": "r-basic.html",
    "title": "7  R 语言入门",
    "section": "",
    "text": "7.1 R语言简介\nR语言最初由罗斯·伊哈卡（Ross Ihaka）和罗伯特·詹特尔曼（Robert Gentleman）于1993年开发。它是从S语言演变而来，S语言是一种主要用于数据分析和统计计算的编程语言。R语言的开源特性让它成为数据分析领域的热门选择，逐渐被广泛应用于各个行业，并且因其强大的数据处理和统计分析能力成为数据科学的标准工具之一。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R 语言入门</span>"
    ]
  },
  {
    "objectID": "r-basic.html#r语言简介",
    "href": "r-basic.html#r语言简介",
    "title": "7  R 语言入门",
    "section": "",
    "text": "7.1.1 R在数据科学中的长处\nR语言在数据科学中的优势在于其高效的数据处理能力和强大的统计分析工具。无论是简单的描述性统计，还是复杂的机器学习模型，R都能够通过简单的语法实现，减少开发者的工作量。此外，R语言的灵活性使得它能够轻松集成到数据科学的各个环节，从数据预处理到模型评估，再到数据可视化，都能够提供高效的解决方案。\n\n丰富的统计与数据分析功能\nR语言内置了众多统计分析函数，并且有大量的统计模型和算法包可供使用。无论是回归分析、时间序列分析、聚类分析，还是假设检验，R都提供了丰富的工具。同时，R语言的灵活性和可扩展性使得用户能够通过自定义函数满足特殊的统计分析需求。\n强大的数据可视化能力\n\nR语言的可视化能力在数据科学中占据了重要地位。通过ggplot2等包，R可以生成各种高质量的图表，如散点图、直方图、箱线图等，帮助用户更好地理解数据的分布与关系。此外，R的图形定制能力非常强，可以精细化调整每个图表元素，满足不同场合的展示需求。\n\n广泛的社区支持与软件包生态\n\nR语言的开源社区非常活跃，全球有大量的开发者和数据科学家参与到R语言的开发和优化中。这使得R拥有丰富的软件包生态系统，涵盖了从数据清洗、分析、建模到可视化的方方面面。R的包管理系统（如CRAN）使得用户可以方便地获取、安装和更新这些包，从而极大地提高了开发效率。\n\n\n7.1.2 R语言数据分析示例\n数据分析通常会有一个流程，如下：\n\n数据导入与预处理：R 语言可以通过 read.csv()、read_excel() 等函数，轻松读取不同格式的数据文件，导入到R的工作环境中。\n数据清洗与处理：R 提供了多种方法来识别和处理缺失值，例如使用is.na()函数检查缺失数据，并通过删除或填充等方式进行处理。此外，dplyr 和 tidyr包提供了许多便捷的函数，帮助用户清洗和转换数据。\n数据探索与可视化：R 提供了丰富的可视化工具，特别是 ggplot2，可以帮助用户快速绘制各种图表，探索数据的分布、趋势和关联。\n描述性统计分析：描述性统计是对数据的基本总结，通过均值、标准差、频率等统计量帮助分析数据的集中趋势与离散程度。在R中，可以使用 summary()、mean()、sd() 等函数进行描述性统计分析，也可以利用统计分析确定数据分布是否符合正态性。\n多维数据可视化：针对与多维数据，可以借助于坐标分析方法对差异进行可视化。通过绘制散点图矩阵、热图等，用户可以直观地查看不同变量之间的关系和数据结构。\n模型建立与预测分析：在 R 中，使用 lm()、glm() 等函数可以轻松实现线性回归分析。通过回归模型，可以预测一个变量与另一个或者多个变量之间的关系，并进行趋势分析。\n\n\n\n\n\n\ngraph TD;\n    A[数据导入与预处理] --&gt; B[数据清洗与处理];\n    B --&gt; C[数据探索与可视化];\n    C --&gt; D[描述性统计分析];\n    D --&gt; E[多维数据可视化];\n    E --&gt; F[模型建立与预测分析];\n\n\n\n\n\n\n下面给出一个简单的案例展示这一过程。\n\n# 加载必要的库\nlibrary(dplyr)  # 用于数据处理\nlibrary(ggplot2)  # 用于数据可视化\nlibrary(caret)  # 用于机器学习模型\n\n# 生成一个示例数据框\nset.seed(123)  # 设置随机种子以确保结果可重复\ndata &lt;- data.frame(\n  x = rnorm(100),  # 生成100个服从正态分布的随机数\n  y = rnorm(100)  # 生成100个服从正态分布的随机数\n)\n\n# 数据预处理：计算描述性统计\nsummary_stats &lt;- data %&gt;%\n  summarise(\n    mean_x = mean(x),  # 计算x的均值\n    mean_y = mean(y),  # 计算y的均值\n    sd_x = sd(x),  # 计算x的标准差\n    sd_y = sd(y)  # 计算y的标准差\n  )\n\n# 打印描述性统计结果\nprint(summary_stats)\n\n      mean_x     mean_y      sd_x      sd_y\n1 0.09040591 -0.1075468 0.9128159 0.9669866\n\n\n\n加载库：首先，我们加载了dplyr、ggplot2和caret库。dplyr用于数据处理，ggplot2用于数据可视化，而caret用于机器学习模型的训练和评估。\n生成数据：使用set.seed(123)确保随机数生成的可重复性。然后，我们创建了一个数据框data，其中包含两个变量x和y，每个变量由100个服从正态分布的随机数组成。\n数据预处理：通过dplyr的summarise函数计算数据的描述性统计，包括均值和标准差。这些统计量帮助我们了解数据的基本特征。\n\n\n# 数据可视化：绘制散点图\nggplot(data, aes(x = x, y = y)) +\n  geom_point() +  # 添加散点图层\n  labs(title = \"Scatter Plot of x and y\", x = \"x\", y = \"y\")  # 添加标题和轴标签\n\n\n\n\n\n\n\n\n\n数据可视化：使用ggplot2绘制x和y的散点图。geom_point()函数用于添加散点图层，labs()函数用于设置图表的标题和轴标签。\n\n\n# 机器学习模型：线性回归\nmodel &lt;- train(y ~ x, data = data, method = \"lm\")  # 使用线性回归模型拟合数据\n\n# 打印模型结果\nprint(model)\n\nLinear Regression \n\n100 samples\n  1 predictor\n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 100, 100, 100, 100, 100, 100, ... \nResampling results:\n\n  RMSE       Rsquared    MAE      \n  0.9480047  0.01907853  0.7481421\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n机器学习模型：使用caret包中的train函数进行线性回归模型的训练。y ~ x表示我们要预测y，data = data指定使用的数据集，method = \"lm\"表示使用线性回归方法。\n\n\n# 模型评估：查看模型的摘要\nsummary(model$finalModel)  # 打印线性回归模型的详细摘要\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9073 -0.6835 -0.0875  0.5806  3.2904 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -0.10280    0.09755  -1.054    0.295\nx           -0.05247    0.10688  -0.491    0.625\n\nResidual standard error: 0.9707 on 98 degrees of freedom\nMultiple R-squared:  0.002453,  Adjusted R-squared:  -0.007726 \nF-statistic: 0.241 on 1 and 98 DF,  p-value: 0.6246\n\n\n\n模型评估：通过summary(model$finalModel)查看线性回归模型的详细摘要，包括系数、R平方值等。这些信息帮助我们评估模型的性能和拟合情况。\n\n整个过程展示了R语言在数据科学中的应用，从数据生成、预处理、可视化到模型训练和评估，体现了其高效的数据处理能力和强大的统计分析工具。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R 语言入门</span>"
    ]
  },
  {
    "objectID": "r-basic.html#基本概念与操作",
    "href": "r-basic.html#基本概念与操作",
    "title": "7  R 语言入门",
    "section": "7.2 基本概念与操作",
    "text": "7.2 基本概念与操作\n\n7.2.1 数据类型与数据结构\n\n7.2.1.1 向量、矩阵与数组\nR语言的基本数据类型包括数值、字符、逻辑等，而这些基本类型可以组成更复杂的数据结构。向量是R中最常用的数据结构，它是一种具有相同数据类型的元素集合。矩阵则是二维的向量，所有元素都是相同类型的。数组可以是多维的矩阵，适用于更复杂的结构化数据。\n\n# 创建一个数值向量\nnumeric_vector &lt;- c(1, 2, 3, 4, 5)  # 使用c()函数创建一个包含数值的向量\n\n# 创建一个字符向量\ncharacter_vector &lt;- c(\"a\", \"b\", \"c\", \"d\")  # 使用c()函数创建一个包含字符的向量\n\n# 创建一个逻辑向量\nlogical_vector &lt;- c(TRUE, FALSE, TRUE, FALSE)  # 使用c()函数创建一个包含逻辑值的向量\n\n# 创建一个矩阵\nmatrix_example &lt;- matrix(1:9, nrow = 3, ncol = 3)  # 使用matrix()函数创建一个3x3的矩阵，元素为1到9\n\n# 创建一个三维数组\narray_example &lt;- array(1:24, dim = c(3, 4, 2))  # 使用array()函数创建一个3x4x2的三维数组，元素为1到24\n\n\n向量是R中最基本的数据结构，可以通过c()函数创建。向量中的所有元素必须是相同的数据类型。在上述代码中，我们创建了数值、字符和逻辑向量。\n\n\n# 打印向量\nprint(numeric_vector)\n\n[1] 1 2 3 4 5\n\nprint(character_vector)\n\n[1] \"a\" \"b\" \"c\" \"d\"\n\nprint(logical_vector)\n\n[1]  TRUE FALSE  TRUE FALSE\n\n\n\n矩阵是二维的向量，所有元素也必须是相同的数据类型。可以使用matrix()函数创建矩阵，指定行数nrow和列数ncol。在示例中，我们创建了一个3x3的矩阵，元素为1到9。\n\n\n# 打印矩阵\nprint(matrix_example)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\n\n数组是多维的矩阵，适用于更复杂的数据结构。可以使用array()函数创建数组，并通过dim参数指定每个维度的大小。在示例中，我们创建了一个3x4x2的三维数组，元素为1到24。\n\n\n# 打印数组\nprint(array_example)\n\n, , 1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n, , 2\n\n     [,1] [,2] [,3] [,4]\n[1,]   13   16   19   22\n[2,]   14   17   20   23\n[3,]   15   18   21   24\n\n\n\n\n7.2.1.2 数据框与列表\n数据框是 R 中最常见的数据结构之一，它类似于数据库中的表格，由不同类型的列组成，可以包含数值、字符、因子等不同类型的数据。列表则是一种更灵活的结构，允许不同类型的数据按顺序存储，其中的元素可以是任何类型的对象，包括向量、数据框、函数等。\n\n# 创建一个数据框\ndata_frame &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  age = c(25, 30, 35),\n  city = c(\"New York\", \"Los Angeles\", \"Chicago\")\n)\n\n# 创建一个列表\nlist_example &lt;- list(\n  numeric_vector = numeric_vector,\n  matrix_example = matrix_example,\n  data_frame = data_frame\n)\n\n\n数据框是R中最常见的数据结构之一，它类似于数据库中的表格，由不同类型的列组成，可以包含数值、字符、因子等不同类型的数据。在示例中，我们创建了一个包含姓名、年龄和城市的数据框。\n\n\n# 打印数据框\nprint(data_frame)\n\n     name age        city\n1   Alice  25    New York\n2     Bob  30 Los Angeles\n3 Charlie  35     Chicago\n\n\n\n列表则是一种更灵活的结构，允许不同类型的数据按顺序存储，其中的元素可以是任何类型的对象，包括向量、数据框、函数等。在示例中，我们创建了一个包含数值向量、矩阵和数据框的列表。\n\n\n# 打印列表\nprint(list_example)\n\n$numeric_vector\n[1] 1 2 3 4 5\n\n$matrix_example\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n$data_frame\n     name age        city\n1   Alice  25    New York\n2     Bob  30 Los Angeles\n3 Charlie  35     Chicago\n\n# 打印列表的元素\nprint(list_example$numeric_vector)  # 打印数值向量\n\n[1] 1 2 3 4 5\n\nprint(list_example$matrix_example)  # 打印矩阵\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\nprint(list_example$data_frame)  # 打印数据框\n\n     name age        city\n1   Alice  25    New York\n2     Bob  30 Los Angeles\n3 Charlie  35     Chicago\n\n\n\n\n7.2.1.3 因子与日期时间\n因子是R中专门用于表示分类数据的一种数据类型，通常用于处理分类变量，如性别、地区等。日期时间类型则用于存储和处理时间相关的数据，R提供了多种日期时间类型，如Date和POSIXt类型，帮助用户进行时间序列分析和操作。\n\n# 创建一个因子\nfactor_example &lt;- factor(c(\"Male\", \"Female\", \"Male\", \"Female\"))\n\n# 打印因子\nprint(factor_example)\n\n[1] Male   Female Male   Female\nLevels: Female Male\n\n\n\n因子是R中专门用于表示分类数据的一种数据类型，通常用于处理分类变量，如性别、地区等。在示例中，我们创建了一个包含性别信息的因子。\n\n因子在绘图时，会自动将分类变量转换为离散的值，从而方便进行可视化。\n\n# 设置绘图参数\npar(mfrow = c(1, 2))\n\n# 打印因子\nplot(factor_example)\n\n# 设置因子水平\nfactor_example &lt;- factor(factor_example, levels = c(\"Male\", \"Female\"))\n# 打印因子\nplot(factor_example)\n\n\n\n\n\n\n\n\n\n日期时间类型则用于存储和处理时间相关的数据，R提供了多种日期时间类型，如Date和POSIXt类型，帮助用户进行时间序列分析和操作。在示例中，我们创建了一个包含日期时间的因子。\n\n\n# 创建一个日期时间向量\ndate_time_vector &lt;- as.POSIXct(c(\"2024-01-01 12:00:00\", \"2024-01-01 13:00:00\", \"2024-01-01 14:00:00\"))  \n\n# 打印日期时间向量\nprint(date_time_vector)\n\n[1] \"2024-01-01 12:00:00 CST\" \"2024-01-01 13:00:00 CST\"\n[3] \"2024-01-01 14:00:00 CST\"",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R 语言入门</span>"
    ]
  },
  {
    "objectID": "r-basic.html#基本语法与操作",
    "href": "r-basic.html#基本语法与操作",
    "title": "7  R 语言入门",
    "section": "7.3 基本语法与操作",
    "text": "7.3 基本语法与操作\n\n7.3.1 变量赋值与运算符\nR语言的变量赋值通常使用&lt;-符号，但也可以使用=进行赋值。运算符包括常见的数学运算符（如加法+、减法-、乘法*等），还包括逻辑运算符（如&、|等）和关系运算符（如==、!=、&gt;、&lt;等），这些运算符在R语言中可以广泛应用于不同的数据结构和分析任务。\n\n# 创建一个变量\nx &lt;- 10\n\n# 打印变量\nprint(x)\n\n[1] 10\n\n# 使用=进行赋值\ny = 20  \n\n# 打印变量\nprint(y)\n\n[1] 20\n\n# 使用==进行比较\nprint(x == y)   \n\n[1] FALSE\n\n# 使用!=进行不等于比较\nprint(x != y)\n\n[1] TRUE\n\n# 使用&gt;进行大于比较\nprint(x &gt; y)    \n\n[1] FALSE\n\n# 使用&lt;进行小于比较\nprint(x &lt; y)\n\n[1] TRUE\n\n# 使用&gt;=进行大于等于比较\nprint(x &gt;= y)   \n\n[1] FALSE\n\n# 使用&&进行逻辑与运算\nprint(x && y)\n\n[1] TRUE\n\n# 使用||进行逻辑或运算\nprint(x || y)   \n\n[1] TRUE\n\n\n\n7.3.1.1 控制结构（条件判断与循环）\nR语言的控制结构包括条件判断（如if、else）和循环（如for、while）。这些结构让我们能够根据不同的条件执行不同的代码，或者重复执行某些任务。条件判断帮助程序根据输入数据的不同决定是否执行某些操作，而循环则在需要多次执行同一任务时非常有用。\n\n# 创建一个变量\nx &lt;- 10\n\n# 使用if进行条件判断\nif (x &gt; 0) {    \n  print(\"x is positive\")\n} else {\n  print(\"x is negative\")\n}\n\n[1] \"x is positive\"\n\n\n\n条件判断帮助程序根据输入数据的不同决定是否执行某些操作。在示例中，我们创建了一个变量x，并使用if进行条件判断。如果x大于0，则打印“x is positive”，否则打印“x is negative”。\n\n\n# 使用if进行条件判断\nif (x &gt; 0) {    \n  print(\"x is positive\")\n} else {\n  print(\"x is negative\")\n}   \n\n[1] \"x is positive\"\n\n\n\n循环则用于重复执行某些任务。在示例中，我们使用for循环打印了1到5的数字。\n\n\n# 使用for循环打印1到5的数字\nfor (i in 1:5) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\n\n\n7.3.1.2 函数定义与调用\nR语言中的函数定义非常简洁，可以使用function()关键字定义函数，并通过函数名来调用。函数可以有输入参数，也可以有返回值。通过函数的定义与调用，用户可以将复杂的计算逻辑封装成模块化的代码，提高代码的可重用性和可维护性。\n\n# 定义一个简单的函数\nmy_function &lt;- function(x) {\n  return(x + 1)\n}\n\n\n函数定义非常简洁，可以使用function()关键字定义函数，并通过函数名来调用。在示例中，我们定义了一个简单的函数my_function，它接受一个参数x，并返回x + 1。\n\n\n# 调用函数\nprint(my_function(1))\n\n[1] 2\n\n\n\n函数可以有输入参数，也可以有返回值。在示例中，我们定义了一个简单的函数my_function，它接受一个参数x，并返回x + 1。\n\n\n# 调用函数\nprint(my_function(1))\n\n[1] 2",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R 语言入门</span>"
    ]
  },
  {
    "objectID": "r-basic.html#数据分析软件包介绍",
    "href": "r-basic.html#数据分析软件包介绍",
    "title": "7  R 语言入门",
    "section": "7.4 数据分析软件包介绍",
    "text": "7.4 数据分析软件包介绍\n该部分介绍在数据分析中最常用到的几个软件包。\n\n7.4.1 dplyr包\ndplyr 包是 R 语言中一个非常流行的数据处理包，提供了简洁的语法来进行数据的筛选与过滤。\n\n7.4.1.1 数据筛选与过滤\n例如，使用filter()函数可以根据条件筛选数据，select()函数可以选择数据框中的某些列，mutate()函数可以添加新的列。这些功能使得数据的清洗和处理变得非常简单高效。\n\n# 创建一个数据框\ndata_frame &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),  \n  age = c(25, 30, 35),\n  city = c(\"New York\", \"Los Angeles\", \"Chicago\")\n)\n\n# 使用filter()函数筛选数据\nfiltered_data &lt;- filter(data_frame, age &gt; 30)   \n\n# 打印筛选后的数据\nprint(filtered_data)\n\n     name age    city\n1 Charlie  35 Chicago\n\n\n\ndplyr包提供了简洁的语法来进行数据的筛选与过滤。例如，使用filter()函数可以根据条件筛选数据，select()函数可以选择数据框中的某些列，mutate()函数可以添加新的列。这些功能使得数据的清洗和处理变得非常简单高效。\n\n\n\n7.4.1.2 数据排序与汇总\n通过dplyr包，用户可以方便地对数据进行排序（arrange()）以及汇总（summarize()）。排序可以按某一列的值进行升序或降序排列，而汇总则能够基于某些分组进行求和、均值计算等操作。\n\n# 创建一个数据框\ndata_frame &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  age = c(25, 30, 35),\n  city = c(\"New York\", \"Los Angeles\", \"Chicago\")\n)\n\n# 使用arrange()函数对数据进行排序\nsorted_data &lt;- arrange(data_frame, age) \n\n# 打印排序后的数据\nprint(sorted_data)\n\n     name age        city\n1   Alice  25    New York\n2     Bob  30 Los Angeles\n3 Charlie  35     Chicago\n\n\n\n通过dplyr包，用户可以方便地对数据进行排序（arrange()）以及汇总（summarize()）。排序可以按某一列的值进行升序或降序排列，而汇总则能够基于某些分组进行求和、均值计算等操作。\n\n\n# 使用summarize()函数对数据进行汇总\nsummarized_data &lt;- summarize(data_frame, mean_age = mean(age))\n\n# 打印汇总后的数据\nprint(summarized_data)\n\n  mean_age\n1       30\n\n\n\n\n\n7.4.2 ggplot2包\n\n7.4.2.1 图形语法概述\nggplot2包基于“语法图形”理论，允许用户通过层次化的方式创建图形。一个ggplot图表通常由数据、映射、几何对象、统计变换、坐标系统和主题组成。用户可以通过简单的代码来定义各个元素，使得图形的创建既直观又灵活。\n\n# 创建一个数据框\ndata_frame &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  age = c(25, 30, 35),\n  city = c(\"New York\", \"Los Angeles\", \"Chicago\")\n)   \n\n# 使用ggplot2包创建一个散点图\nggplot(data_frame, aes(x = age, y = city)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of Age and City\")\n\n\n\n\n\n\n\n\n\nggplot2包基于“语法图形”理论，允许用户通过层次化的方式创建图形。一个ggplot图表通常由数据、映射、几何对象、统计变换、坐标系统和主题组成。用户可以通过简单的代码来定义各个元素，使得图形的创建既直观又灵活。\n\n\n\n7.4.2.2 常见图表类型\nggplot2支持多种常见的图表类型，如散点图、折线图、直方图、箱线图等。每种图表类型都可以通过改变不同的参数进行调整，满足不同的展示需求。\n\n# 加载ggplot2包，用于数据可视化\nlibrary(ggplot2)  # 导入ggplot2库\n\n# 创建示例数据集，包括x、y两个变量和一个分组变量group\ndata &lt;- data.frame(\n  x = rnorm(100),                   # 生成100个服从正态分布的随机数作为x轴数据\n  y = rnorm(100),                   # 生成100个服从正态分布的随机数作为y轴数据\n  group = sample(letters[1:3], 100, replace = TRUE)  # 随机生成100个属于a、b、c三个组的分组数据\n)\n\n\n加载与准备数据\n\n首先使用 library(ggplot2) 加载ggplot2包，这是R中非常流行的用于数据可视化的包。\n\n通过 data.frame 函数创建了一个数据集，其中包括两个数值变量 x 和 y，以及一个字符型变量 group。x 和 y 分别使用 rnorm(100) 生成100个正态分布的随机数；group 则利用 sample 从字母 a、b、c 中随机抽取分组信息，模拟数据分组场景。\n\n\n\n# ---------------------------\n# 散点图示例：展示x和y之间的关系，并根据group分组上色\n# ---------------------------\nscatter_plot &lt;- ggplot(data, aes(x = x, y = y, color = group)) +  # 设置数据源及映射关系（x、y坐标和颜色对应分组）\n  geom_point(size = 3, alpha = 0.7) +       # 添加散点图层，设置点的大小为3，透明度为0.7\n  labs(title = \"散点图示例\", x = \"X轴\", y = \"Y轴\") +  # 添加图形标题和坐标轴标签\n  theme_minimal()                         # 应用简洁的主题\nprint(scatter_plot)                       # 绘制散点图\n\n\n\n\n\n\n\n\n\n散点图（Scatter Plot）\n\n使用 ggplot(data, aes(x = x, y = y, color = group)) 设置数据源和美学映射，其中 x 和 y 分别代表横轴和纵轴，color = group 根据分组变量赋予不同颜色。\n\ngeom_point(size = 3, alpha = 0.7) 添加散点图层，设置点的大小和透明度，帮助提高视觉效果。\n\nlabs 函数用来添加图表标题和坐标轴标签；theme_minimal() 应用了一个简洁的主题。\n\n最后，print(scatter_plot) 用于在图形设备上输出该图。\n\n\n\n# ---------------------------\n# 折线图示例：展示随时间变化的数据趋势\n# ---------------------------\n# 创建时间序列数据，time为时间变量，value为累积随机值模拟趋势数据\ntime_data &lt;- data.frame(\n  time = 1:50,                          # 创建一个从1到50的时间序列\n  value = cumsum(rnorm(50))              # 生成50个随机数并计算累积和，模拟数据趋势\n)\nline_plot &lt;- ggplot(time_data, aes(x = time, y = value)) +  # 设置数据源及映射关系（时间与数值）\n  geom_line(color = \"blue\", size = 1) +   # 添加折线图层，设置线条颜色为蓝色，宽度为1\n  labs(title = \"折线图示例\", x = \"时间\", y = \"数值\") +  # 添加图形标题和坐标轴标签\n  theme_light()                           # 应用明亮主题\nprint(line_plot)                         # 绘制折线图\n\n\n\n\n\n\n\n\n\n折线图（Line Chart）\n\n构造一个时间序列数据集 time_data，其中 time 表示时间变量，value 是累积和模拟的数值趋势（利用 cumsum 和 rnorm 生成）。\n\n在 ggplot 中映射时间和数值变量，geom_line(color = \"blue\", size = 1) 用于绘制折线图，并设置线条的颜色和宽度。\n\n同样通过 labs 添加标题和轴标签，并使用 theme_light() 设置主题。\n\n最后，通过 print(line_plot) 输出折线图。\n\n\n\n# ---------------------------\n# 直方图示例：展示x变量的分布情况\n# ---------------------------\nhistogram_plot &lt;- ggplot(data, aes(x = x)) +  # 设置数据源及映射关系（x轴数据）\n  geom_histogram(binwidth = 0.5, fill = \"green\", color = \"black\", alpha = 0.6) +  \n  # 添加直方图图层，设置每个柱子的宽度为0.5，填充颜色为绿色，边框为黑色，透明度为0.6\n  labs(title = \"直方图示例\", x = \"X轴\", y = \"频数\") +  # 添加图形标题和坐标轴标签\n  theme_classic()                         # 应用经典主题\nprint(histogram_plot)                     # 绘制直方图\n\n\n\n\n\n\n\n\n\n直方图（Histogram）\n\n利用同一数据集 data 中的 x 变量创建直方图，通过 aes(x = x) 映射x轴数据。\n\ngeom_histogram(binwidth = 0.5, fill = \"green\", color = \"black\", alpha = 0.6) 绘制直方图，其中 binwidth 控制每个柱子的宽度，fill 和 color 设置柱子的填充和边框颜色，alpha 控制透明度。\n\n添加必要的标题和标签后，使用 theme_classic() 设定图形风格，并通过 print(histogram_plot) 输出直方图。\n\n\n\n# ---------------------------\n# 箱线图示例：比较不同组别中x变量的分布情况\n# ---------------------------\nboxplot &lt;- ggplot(data, aes(x = group, y = x, fill = group)) +  # 设置数据源及映射关系（组别作为x轴，x变量作为y轴，同时填充颜色依组别变化）\n  geom_boxplot() +                        # 添加箱线图图层\n  labs(title = \"箱线图示例\", x = \"组别\", y = \"X轴值\") +  # 添加图形标题和坐标轴标签\n  theme_bw()                              # 应用黑白主题\nprint(boxplot)                           # 绘制箱线图\n\n\n\n\n\n\n\n\n\n箱线图（Box Plot）\n\n为比较不同分组中 x 变量的分布，箱线图将 group 作为分类变量映射到x轴，x 变量映射到y轴，且利用 fill = group 根据不同组赋予不同填充颜色。\n\ngeom_boxplot() 添加箱线图层后，通过 labs 设定图表标题和坐标轴标签。\n\n使用 theme_bw() 选择黑白风格主题，并调用 print(boxplot) 输出箱线图。\n\n\n总体来看，这段代码展示了如何利用ggplot2生成不同类型的图表（散点图、折线图、直方图、箱线图），以及如何通过调整参数（如颜色、大小、透明度、bin宽度和主题）来满足不同的数据展示需求。这些图表类型在数据分析和展示中十分常用，能够直观地帮助理解数据的分布、趋势和比较，为数据科学工作提供了强大的视觉支持。\n\n\n7.4.2.3 图层的叠加\nggplot2允许用户通过图层叠加的方式创建复杂的图表。例如，可以在同一个图表中绘制多个几何对象，或者在同一个图表中添加多个统计变换。\n\n# ---------------------------\n# 图层叠加示例：在同一个图表中绘制多个几何对象\n# ---------------------------\nggplot(data, aes(x = group, y = x, fill = group)) +  # 设置数据源及映射关系（组别作为x轴，x变量作为y轴，同时填充颜色依组别变化）\n  geom_boxplot() +                        # 添加箱线图图层\n  geom_jitter(aes(color = group), size = 3, alpha = 0.7) +       # 添加散点图层，设置点的大小为3，透明度为0.7\n  labs(title = \"箱线图示例\", x = \"组别\", y = \"X轴值\") +  # 添加图形标题和坐标轴标签\n  theme_bw() \n\n\n\n\n\n\n\n\n\n\n7.4.2.4 图形美化与定制\nggplot2的一个重要特点是图形的美化与定制。用户可以通过theme()函数控制图形的背景、坐标轴、字体等细节，进一步提升图表的可读性和美观性。\n\n\n\n7.4.3 stringr包\n\n7.4.3.1 字符串处理基础\nstringr包提供了一系列简便的函数来处理字符串数据，包括字符串的切割、拼接、替换等操作。常用的函数包括str_sub()（提取子字符串）、str_c()（字符串拼接）、str_replace()（字符串替换）等。\n\n# 加载stringr包\nlibrary(stringr)\n\n# 定义一个示例字符串\ntext = \"Hello, R language!\"\n\n# 使用str_sub()提取子字符串，提取从8到9的字符\nsub_text = str_sub(text, 8, 9)  # \"R \"\n\n# 使用str_c()拼接字符串\nnew_text = str_c(text, \" It's powerful!\")  # \"Hello, R language! It's powerful!\"\n\n# 使用str_replace()替换字符串中的单词\nreplaced_text = str_replace(text, \"language\", \"world\")  # \"Hello, R world!\"\n\n\nlibrary(stringr): 加载 stringr 包，以便使用其中的字符串处理函数。\ntext = \"Hello, R language!\": 定义一个字符串变量 text，作为处理对象。\nstr_sub(text, 8, 9):\n\n提取 text 中第 8 到 9 个字符。\n这里 text 从 \"Hello, R language!\" 变为 \"R \"（包含空格）。\n\nstr_c(text, \" It's powerful!\"):\n\n拼接 text 和 \" It's powerful!\"，形成 \"Hello, R language! It's powerful!\"。\nstr_c() 自动处理字符串合并，不需要使用 paste() 及其 sep 参数。\n\nstr_replace(text, \"language\", \"world\"):\n\n将 text 中的 \"language\" 替换为 \"world\"，生成 \"Hello, R world!\"。\nstr_replace() 只替换第一个匹配项（如需全局替换可用 str_replace_all()）。\n\n\n这些操作展示了 stringr 包在字符串处理中的简洁性和强大功能。\n\n\n7.4.3.2 模式匹配与替换\nstringr包还支持正则表达式的使用，可以进行复杂的模式匹配和替换操作。str_detect()函数可以判断字符串是否符合某个模式，str_replace_all()可以对字符串中的所有匹配项进行替换。\n\n# 加载stringr包\nlibrary(stringr)\n\n# 定义一个字符向量\ntext = c(\"apple123\", \"banana456\", \"cherry789\", \"date000\")\n\n# 使用str_detect()判断哪些字符串包含数字\ncontains_digit = str_detect(text, \"\\\\d+\")  # \"\\\\d+\"表示匹配一个或多个数字\nprint(contains_digit)  # 输出匹配结果，TRUE表示包含数字，FALSE表示不包含\n\n[1] TRUE TRUE TRUE TRUE\n\n# 使用str_replace_all()替换所有数字为空字符串\ntext_no_digits = str_replace_all(text, \"\\\\d+\", \"\")  # 将所有数字部分替换为空\nprint(text_no_digits)  # 输出替换后的字符串\n\n[1] \"apple\"  \"banana\" \"cherry\" \"date\"  \n\n\n\n加载stringr包：library(stringr)用于加载stringr包，以便使用其提供的字符串操作函数。\n定义字符向量：text = c(\"apple123\", \"banana456\", \"cherry789\", \"date000\")创建一个包含多个字符串的向量，每个字符串都含有字母和数字。\nstr_detect() 进行模式匹配：\n\nstr_detect(text, \"\\\\d+\")用于检测text向量中的字符串是否包含数字（\\d+表示匹配一个或多个数字）。\n结果是一个逻辑向量，每个元素对应text中的一个字符串，TRUE表示包含数字，FALSE表示不包含。\n\nstr_replace_all() 进行全局替换：\n\nstr_replace_all(text, \"\\\\d+\", \"\")将所有匹配\\d+（即所有数字）的部分替换为空字符串\"\"，即删除所有数字。\n结果是一个新的字符向量，所有的数字都被移除，仅保留字母部分。\n\n\n这样，我们使用str_detect()检测字符串是否包含数字，并用str_replace_all()移除所有数字，实现了正则表达式在字符串匹配和替换中的应用。\n\n\n7.4.3.3 文本数据清洗\n在数据分析中，文本数据往往需要进行清洗与标准化。stringr包提供了丰富的工具，可以去除多余的空格、标点符号，进行大小写转换等，从而保证文本数据的一致性。\n\n# 加载stringr包\nlibrary(stringr)\n\n# 示例文本数据\ntext_data = c(\"  Hello, World!  \", \"R is AWESOME!!!\", \"Data   Science & AI\")\n\n# 去除前后空格\ntext_data = str_trim(text_data)\n\n# 转换为小写，确保一致性\ntext_data = str_to_lower(text_data)\n\n# 去除所有标点符号\ntext_data = str_replace_all(text_data, \"[[:punct:]]\", \"\")\n\n# 规范化多个空格为单个空格\ntext_data = str_squish(text_data)\n\n# 输出清洗后的文本数据\nprint(text_data)\n\n[1] \"hello world\"     \"r is awesome\"    \"data science ai\"\n\n\n\n\n\n7.4.4 代码逻辑解释\n\n加载stringr包：提供字符串处理函数。\n定义文本数据：包含前后空格、大小写混用、多余空格和标点符号。\n去除前后空格：使用str_trim()确保文本没有额外的前导和尾随空格。\n转换为小写：str_to_lower()保证文本大小写一致，避免因大小写不同而导致数据匹配问题。\n去除标点符号：str_replace_all()用正则表达式[[:punct:]]匹配所有标点并替换为空字符串。\n合并多余空格：str_squish()将多个连续空格替换为单个空格，保持文本格式整洁。\n输出结果：清洗后的文本更规范，适用于进一步分析。\n\n\n\n7.4.5 tidyr包\n\n7.4.5.1 数据整形与转换\ntidyr包提供了方便的数据整形与转换工具，帮助用户将数据从一个格式转换为另一个格式，通常用于将“宽格式数据”转换为“长格式数据”，或者反之。\n\n# 加载tidyr和dplyr包\nlibrary(tidyr)   # 提供数据整形功能\nlibrary(dplyr)   # 方便数据操作\n\n\n创建“宽格式”数据\n\nwide_data 代表一个学生成绩表，每一行是一个学生，每一列代表一个科目及其成绩。\n\n\n\n# 创建一个“宽格式”数据框\nwide_data = data.frame(\n  ID = c(1, 2, 3),       # 唯一标识\n  Math = c(90, 85, 88),  # 数学成绩\n  Science = c(80, 78, 95), # 科学成绩\n  English = c(85, 89, 92)  # 英语成绩\n)\nwide_data\n\n  ID Math Science English\n1  1   90      80      85\n2  2   85      78      89\n3  3   88      95      92\n\n\n\npivot_longer()：宽转长\n\ncols = Math:English 选取需要转换的列，即各科成绩。\n\nnames_to = \"Subject\" 创建一个新列，用于存放原来的列名（学科名称）。\n\nvalues_to = \"Score\" 创建一个新列，用于存放原来的数据值（成绩）。\n\n转换后，每个学生的成绩由多列合并成两列（学科 + 分数），适用于按学科进行分析。\n\n\n\n# 使用pivot_longer()将宽格式转换为长格式\nlong_data = wide_data %&gt;%\n  pivot_longer(\n    cols = Math:English,  # 需要转换的列（即各科成绩）\n    names_to = \"Subject\",  # 新列存放原来的列名（学科）\n    values_to = \"Score\"    # 新列存放原来的数值（成绩）\n  )\nlong_data\n\n# A tibble: 9 × 3\n     ID Subject Score\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1 Math       90\n2     1 Science    80\n3     1 English    85\n4     2 Math       85\n5     2 Science    78\n6     2 English    89\n7     3 Math       88\n8     3 Science    95\n9     3 English    92\n\n\n\npivot_wider()：长转宽\n\nnames_from = Subject 让原本的“学科”值重新变为列名。\n\nvalues_from = Score 让成绩数据回到对应的学科列下。\n\n这样数据恢复为原始的“宽格式”，适用于数据可视化或建模。\n\n\n\n# 使用pivot_wider()将长格式转换回宽格式\nwide_data_again = long_data %&gt;%\n  pivot_wider(\n    names_from = Subject, # 重新展开为原来的列名（学科）\n    values_from = Score   # 对应的成绩填充回去\n  )\nwide_data_again\n\n# A tibble: 3 × 4\n     ID  Math Science English\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     1    90      80      85\n2     2    85      78      89\n3     3    88      95      92\n\n\ntidyr中的pivot_longer()和pivot_wider()函数分别用于实现宽表和长表之间的转换，帮助数据分析师根据不同分析需求选择合适的数据格式。\n\n\n7.4.5.2 数据变换与重组\ntidyr 还提供了spread()和gather()等函数，可以帮助用户进行数据的转置和重组。例如，gather()函数可以将宽格式数据转换为长格式数据，而spread()则可以将长格式数据转换为宽格式。\n\nlibrary(tidyr)\n# 使用gather()函数将宽格式数据转换为长格式数据\nlong_data &lt;- gather(data_frame, key = \"variable\", value = \"value\", -name)\n\n# 打印转换后的数据\nprint(long_data)\n\n     name variable       value\n1   Alice      age          25\n2     Bob      age          30\n3 Charlie      age          35\n4   Alice     city    New York\n5     Bob     city Los Angeles\n6 Charlie     city     Chicago\n\n\n\n\n\n7.4.6 数据检查和清洗\n\n7.4.6.1 缺失值检查\ntidyr包提供了complete()函数，可以检查数据中是否存在缺失值，并返回一个包含缺失值的布尔矩阵。\n\n# 创建一个包含缺失值的数据框\ndata_with_missing = data.frame(\n  ID = c(1, 2, 3, 4),\n  Name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\"),\n  Score = c(95, NA, 88, 92)\n)\n\n使用 complete() 函数检查缺失值。\n\n# 使用complete()检查缺失值\ncomplete_data = complete(data_with_missing)\nprint(complete_data)\n\n  ID    Name Score\n1  1   Alice    95\n2  2     Bob    NA\n3  3 Charlie    88\n4  4   David    92",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R 语言入门</span>"
    ]
  },
  {
    "objectID": "python-basic.html",
    "href": "python-basic.html",
    "title": "8  Python 语言入门",
    "section": "",
    "text": "8.1 Python 基础概念\nPython 是一门简单易学、功能强大的解释型编程语言，适合初学者。下面将介绍 Python 的基础语法、变量、数据类型、控制结构、函数和数据结构等内容。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Python 语言入门</span>"
    ]
  },
  {
    "objectID": "python-basic.html#python-基础概念",
    "href": "python-basic.html#python-基础概念",
    "title": "8  Python 语言入门",
    "section": "",
    "text": "8.1.1 变量和数据类型\n下面的示例展示了如何定义变量以及 Python 的基本数据类型（整数、浮点数、字符串、布尔值）。\n\n# 定义一个整数变量\na = 10              # a 是一个整数，值为 10\n\n# 定义一个浮点数变量\nb = 3.14            # b 是一个浮点数，值为 3.14\n\n# 定义一个字符串变量\nc = \"Hello, Python!\"  # c 是一个字符串，值为 \"Hello, Python!\"\n\n# 定义一个布尔变量\nd = True            # d 是一个布尔变量，值为 True\n\n# 输出各变量的值和数据类型\nprint(\"a =\", a, \"类型:\", type(a))   # 打印 a 的值和类型：&lt;class 'int'&gt;\nprint(\"b =\", b, \"类型:\", type(b))   # 打印 b 的值和类型：&lt;class 'float'&gt;\nprint(\"c =\", c, \"类型:\", type(c))   # 打印 c 的值和类型：&lt;class 'str'&gt;\nprint(\"d =\", d, \"类型:\", type(d))   # 打印 d 的值和类型：&lt;class 'bool'&gt;\n\na = 10 类型: &lt;class 'int'&gt;\nb = 3.14 类型: &lt;class 'float'&gt;\nc = Hello, Python! 类型: &lt;class 'str'&gt;\nd = True 类型: &lt;class 'bool'&gt;\n\n\n代码说明：\n\n第1行到第4行：分别定义了整数、浮点数、字符串和布尔值变量。\n第7行到第10行：使用 print() 函数输出每个变量的值和类型。\n\n\n\n8.1.2 控制结构 —— 条件语句和循环\n\n8.1.2.1 条件语句示例\n下面的代码判断一个数字是正数、负数还是零。\n\nnum = 5  # 定义变量 num，赋值为 5\n\nif num &gt; 0:                  # 如果 num 大于 0，进入此代码块\n    print(num, \"是正数\")      # 打印 num 是正数\nelif num &lt; 0:                # 否则，如果 num 小于 0，进入此代码块\n    print(num, \"是负数\")      # 打印 num 是负数\nelse:                        # 否则（即 num 等于 0）进入此代码块\n    print(num, \"是零\")        # 打印 num 是零\n\n5 是正数\n\n\n代码说明：\n\n第1行：定义变量 num，值为 5。\n第3行：判断 num 是否大于 0。\n第4行：如果条件成立，则输出 “5 是正数”。\n第5-7行：使用 elif 和 else 处理其他情况。\n\n\n\n8.1.2.2 for 循环示例\n使用 for 循环遍历一个列表。\n\nfruits = [\"苹果\", \"香蕉\", \"橘子\"]  # 定义一个包含水果名称的列表\n\nfor fruit in fruits:              # 遍历列表中的每个元素\n    print(\"我喜欢吃\", fruit)      # 打印每个水果的名称\n\n我喜欢吃 苹果\n我喜欢吃 香蕉\n我喜欢吃 橘子\n\n\n代码说明：\n\n第1行：定义列表 fruits，包含三个字符串。\n第3行：使用 for 循环遍历列表中每个元素，变量 fruit 依次取每个水果名称。\n第4行：打印“我喜欢吃”以及当前遍历到的水果名称。\n\n\n\n8.1.2.3 while 循环示例\n下面代码使用 while 循环计算 1 到 5 的累加和。\n\ntotal = 0  # 定义变量 total，用于存储累加和，初始值为 0\ni = 1      # 定义计数器 i，初始值为 1\n\nwhile i &lt;= 5:         # 当 i 小于等于 5 时，进入循环\n    total += i        # 将 i 加到 total 上，相当于 total = total + i\n    i += 1            # 将 i 的值增加 1\n\nprint(\"1到5的和为\", total)  # 循环结束后，打印累加和\n\n1到5的和为 15\n\n\n代码说明：\n\n第1-2行：初始化累加和变量 total 和计数器 i。\n第4行：设置循环条件，确保循环在 i&lt;=5 时执行。\n第5行：将当前的 i 累加到 total 中。\n第6行：i 自增 1。\n第8行：循环结束后打印结果。\n\n\n\n\n8.1.3 函数与模块\n\n8.1.3.1 自定义函数示例\n下面的代码定义一个求两个数和的函数，并调用该函数。\n\ndef add_numbers(x, y):\n    # 定义函数 add_numbers，接受两个参数 x 和 y\n    result = x + y    # 计算 x 与 y 的和，将结果存储在变量 result 中\n    return result     # 返回计算结果\n\n# 调用函数，将返回值赋给变量 sum_result\nsum_result = add_numbers(3, 7)\nprint(\"3和7的和为\", sum_result)  # 打印函数返回的结果\n\n3和7的和为 10\n\n\n代码说明：\n\n第1行：使用 def 定义函数 add_numbers，接收两个参数 x 和 y。\n第3行：计算传入参数的和，并赋值给 result。\n第4行：返回计算结果。\n第7行：调用函数，并将返回结果存储到 sum_result。\n第8行：打印出计算结果。\n\n\n\n8.1.3.2 导入模块示例\n下面示例展示如何使用 Python 标准库中的 math 模块来计算平方根。\n\nimport math  # 导入 math 模块，包含数学计算函数\n\nsqrt_val = math.sqrt(16)  # 调用 math 模块中的 sqrt 函数计算 16 的平方根\nprint(\"16的平方根为\", sqrt_val)  # 打印计算结果\n\n16的平方根为 4.0\n\n\n代码说明：\n\n第1行：导入 math 模块。\n第3行：使用 math.sqrt 计算 16 的平方根，并赋值给 sqrt_val。\n第4行：打印平方根的值。\n\n\n\n\n8.1.4 数据结构\n\n8.1.4.1 列表（List）\n\n# 列表：有序且可变的集合\nnumbers = [1, 2, 3, 4, 5]  # 定义一个包含数字的列表\nnumbers.append(6)         # 在列表末尾添加数字 6\nprint(\"列表内容为:\", numbers)  # 打印列表当前的内容\n\n列表内容为: [1, 2, 3, 4, 5, 6]\n\n\n代码说明：\n\n第1行：定义一个包含 1 到 5 的列表。\n第2行：使用 append 方法向列表中添加数字 6。\n第3行：打印更新后的列表。\n\n\n\n8.1.4.2 字典（Dictionary）\n\n# 字典：无序的键值对集合\nperson = {\"name\": \"张三\", \"age\": 25}  # 定义一个字典，包含姓名和年龄信息\nperson[\"gender\"] = \"男\"             # 添加一个新的键值对，表示性别为 \"男\"\nprint(\"字典内容为:\", person)         # 打印字典的内容\n\n字典内容为: {'name': '张三', 'age': 25, 'gender': '男'}\n\n\n代码说明：\n\n第1行：定义一个包含姓名和年龄的字典。\n第2行：向字典中添加新的键 \"gender\"，值为 \"男\"。\n第3行：打印整个字典。\n\nPython 主要的数据结构包括以下几类：\n\n\n8.1.4.3 元组（Tuple）\n\n不可变、可存储不同类型的数据\n\n支持索引、切片、遍历等操作\n\n\ntup = (1, \"hello\", 3.14)\nprint(tup[1])  # hello\n\nhello\n\n\n\n\n8.1.4.4 集合（Set）\n\n无序、不重复的元素集合\n\n支持集合运算（交集、并集、差集）\n\n\ns = {1, 2, 3, 3, 5}\ns.add(4)\nprint(s)  # {1, 2, 3, 4}\n\n{1, 2, 3, 4, 5}\n\n\n\n\n8.1.4.5 NumPy 数组\nNumPy 的 ndarray 是一种高效的多维数组数据结构，适用于大规模数值计算。\n\nimport numpy as np\n\n# 创建 NumPy 数组\narr = np.array([1, 2, 3, 4])\nprint(arr)  # [1 2 3 4]\n\n# 创建二维数组（矩阵）\nmatrix = np.array([[1, 2], [3, 4]])\nprint(matrix)\n# [[1 2]\n#  [3 4]]\n\n# 生成 3×3 的全 0 矩阵\nzero_matrix = np.zeros((3, 3))\nprint(zero_matrix)\n\n# 生成 3×3 的随机数矩阵\nrandom_matrix = np.random.rand(3, 3)\nprint(random_matrix)\n\n# 矩阵运算\narr2 = np.array([5, 6, 7, 8])\nprint(arr + arr2)  # [ 6  8 10 12]\nprint(arr * 2)  # [ 2  4  6  8]\n\n[1 2 3 4]\n[[1 2]\n [3 4]]\n[[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\n[[0.81258928 0.30607058 0.1707208 ]\n [0.74368608 0.91946392 0.30441661]\n [0.6076623  0.36749073 0.7451878 ]]\n[ 6  8 10 12]\n[2 4 6 8]\n\n\n\n\n8.1.4.6 DataFrame\n类似数据库表或 Excel 表格，行列都有索引。\n\nimport pandas as pd\n\n# 创建 DataFrame\ndata = {\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"Age\": [25, 30, 35],\n    \"City\": [\"New York\", \"San Francisco\", \"Los Angeles\"]\n}\ndf = pd.DataFrame(data)\nprint(df)\n\n# 访问列\nprint(df[\"Age\"])\n\n# 访问行\nprint(df.loc[1])  # 按行索引访问\nprint(df.iloc[1])  # 按行位置访问\n\n      Name  Age           City\n0    Alice   25       New York\n1      Bob   30  San Francisco\n2  Charlie   35    Los Angeles\n0    25\n1    30\n2    35\nName: Age, dtype: int64\nName              Bob\nAge                30\nCity    San Francisco\nName: 1, dtype: object\nName              Bob\nAge                30\nCity    San Francisco\nName: 1, dtype: object\n\n\n\n\n8.1.4.7 PyTorch / TensorFlow 张量\n用于深度学习中的高效数值计算。\n\nimport torch\ntensor = torch.tensor([[1, 2], [3, 4]])\nprint(tensor)\n\ntensor([[1, 2],\n        [3, 4]])\n\n\n\n\n\n\n\n\nTip\n\n\n\n为什么张量可以进行高效的数值运算？\n张量（Tensor）能进行高效数值运算的原因主要包括以下几个方面：\n1. 基于优化的底层实现\n\nNumPy 依赖 BLAS/LAPACK：传统的 NumPy 运算主要依赖 BLAS（Basic Linear Algebra Subprograms）和 LAPACK（Linear Algebra PACKage）等底层优化库。\n\nPyTorch / TensorFlow 采用高性能计算库：PyTorch 使用 ATen + cuDNN，TensorFlow 使用 XLA（Accelerated Linear Algebra），都针对张量运算进行了优化。\n\n2. GPU 并行加速\n\n张量原生支持 GPU 计算：相比 NumPy 主要在 CPU 上运行，PyTorch 和 TensorFlow 允许张量在 GPU 上进行并行计算。\n\n多核并行计算：GPU 具有成千上万个计算核心，适合矩阵运算和深度学习任务。\n\n示例（在 GPU 上计算张量）：\nimport torch\n\n# 在 CPU 上创建张量\na = torch.randn(1000, 1000)\n\n# 将张量移动到 GPU 并进行计算\na_gpu = a.to(\"cuda\")\nb_gpu = torch.matmul(a_gpu, a_gpu)\nprint(b_gpu.device)  # cuda:0\n\n3. 支持自动求导\n\nPyTorch 和 TensorFlow 都支持自动微分，使得梯度计算更加高效。\n\n基于计算图（Computational Graph），减少重复计算，提高梯度计算效率。\n\n示例（PyTorch 自动求导）：\nx = torch.tensor(2.0, requires_grad=True)\ny = x ** 2\ny.backward()  # 计算 dy/dx\nprint(x.grad)  # 4.0\n\n4. 高效的内存管理\n\n避免数据拷贝：PyTorch 和 TensorFlow 在 CPU 和 GPU 之间进行数据传输时，减少不必要的拷贝操作。\n\n内存池优化：PyTorch 采用 缓存机制，避免频繁的内存分配和释放，提高效率。\n\n5. 矢量化计算\n\n支持广播机制（Broadcasting）：允许不同形状的张量进行运算，而不需要手动扩展维度。\n\n示例（广播计算）：\nimport numpy as np\na = np.array([[1, 2, 3]])\nb = np.array([[1], [2], [3]])\nprint(a + b)  \n# [[2 3 4]\n#  [3 4 5]\n#  [4 5 6]]\n\n6. 支持分布式计算\n\nPyTorch 和 TensorFlow 支持 多 GPU 并行计算（Data Parallelism） 和 多节点分布式训练。\n\n适用于大规模深度学习训练，如 GPT、BERT 训练。\n\n总结\n张量的高效性主要来源于：\n\n底层库优化（BLAS、cuDNN、XLA）\nGPU 并行计算\n自动求导\n高效的内存管理\n矢量化计算\n分布式计算支持\n\n这使得张量成为深度学习、科学计算等高性能计算任务的理想数据结构。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Python 语言入门</span>"
    ]
  },
  {
    "objectID": "python-basic.html#python-编码最佳实践",
    "href": "python-basic.html#python-编码最佳实践",
    "title": "8  Python 语言入门",
    "section": "8.2 Python 编码最佳实践",
    "text": "8.2 Python 编码最佳实践\n为了编写高质量、易维护的代码，建议遵循以下几点最佳实践：\n\n遵循 PEP8 规范\n\n使用 4 个空格缩进\n变量和函数命名应清晰、具有描述性\n每行代码不宜过长（一般限制在 79 字符以内）\n\n添加适当的注释\n\n对关键逻辑添加注释，方便自己和他人理解代码\n\n异常处理\n\n使用 try...except 来捕获潜在错误，避免程序崩溃\n\n\n下面是一个使用异常处理的示例：\n\ntry:\n    result = 10 / 0      # 尝试进行除法运算，分母为 0 可能导致异常\nexcept ZeroDivisionError as e:  # 捕获除零错误，将异常信息存入变量 e\n    print(\"错误：除数不能为零！\", e)  # 打印错误提示及异常详情\n\n错误：除数不能为零！ division by zero\n\n\n代码说明：\n\n第1行：尝试将 10 除以 0，此操作会引发 ZeroDivisionError 异常。\n第2行：捕获 ZeroDivisionError 异常，并将异常对象赋值给 e。\n第3行：打印错误提示信息及异常详情。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Python 语言入门</span>"
    ]
  },
  {
    "objectID": "python-basic.html#python-在数据科学领域的应用",
    "href": "python-basic.html#python-在数据科学领域的应用",
    "title": "8  Python 语言入门",
    "section": "8.3 Python 在数据科学领域的应用",
    "text": "8.3 Python 在数据科学领域的应用\nPython 在数据科学中有着广泛的应用，主要依赖于如下几个库：\n\nNumPy：高效的数值计算\n\nPandas：数据处理与分析\n\nMatplotlib：数据可视化\n\n下面的示例将展示如何利用这些库进行数据处理和绘图。\n\n# 导入必要的库\nimport numpy as np             # 导入 NumPy 库，简写为 np，用于高效的数值计算\nimport pandas as pd            # 导入 Pandas 库，简写为 pd，用于数据处理\nimport matplotlib.pyplot as plt  # 导入 Matplotlib 的 pyplot 模块，简写为 plt，用于绘图\n\n\nNumPy 部分：创建了一个一维数组，并对每个元素进行平方计算。\n\n\n# -------------------------------\n# 1. 使用 NumPy 进行数组操作\n# -------------------------------\narray = np.array([1, 2, 3, 4, 5])  # 创建一个包含数字 1 到 5 的 NumPy 数组\narray_squared = array ** 2         # 对数组中的每个元素进行平方操作\nprint(\"原始数组:\", array)           # 输出原始数组\nprint(\"数组每个元素的平方:\", array_squared)  # 输出平方后的数组\n\n原始数组: [1 2 3 4 5]\n数组每个元素的平方: [ 1  4  9 16 25]\n\n\n\nPandas 部分：将一个包含姓名、年龄和分数的字典转换为 DataFrame，并计算年龄平均值。\n\n\n# -------------------------------\n# 2. 使用 Pandas 进行数据处理\n# -------------------------------\n# 定义一个字典，包含姓名、年龄和分数等信息\ndata = {\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n    \"Age\": [25, 30, 35, 40],\n    \"Score\": [85, 90, 95, 80]\n}\n\ndf = pd.DataFrame(data)  # 将字典转换为 Pandas DataFrame 数据结构\nprint(\"数据框内容:\\n\", df)  # 打印数据框内容\n\n# 计算年龄的平均值\naverage_age = df[\"Age\"].mean()  # 计算 DataFrame 中“Age”这一列的平均值\nprint(\"平均年龄为:\", average_age)  # 打印平均年龄\n\n数据框内容:\n       Name  Age  Score\n0    Alice   25     85\n1      Bob   30     90\n2  Charlie   35     95\n3    David   40     80\n平均年龄为: 32.5\n\n\n\nMatplotlib 部分：绘制一个简单的柱状图，展示每个学生的分数。\n\n\n# -------------------------------\n# 3. 使用 Matplotlib 进行数据可视化\n# -------------------------------\nplt.figure(figsize=(8, 4))  # 设置图形尺寸为 8x4 英寸\nplt.bar(df[\"Name\"], df[\"Score\"], color='skyblue')  # 绘制柱状图：x 轴为姓名，y 轴为分数，柱状图颜色为天蓝色\nplt.xlabel(\"Name\")          # 设置 x 轴标签\nplt.ylabel(\"Score\")          # 设置 y 轴标签\nplt.title(\"Student Score Bar Chart\")   # 设置图形标题\nplt.show()                 # 显示绘制的图形",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Python 语言入门</span>"
    ]
  },
  {
    "objectID": "python-basic.html#python-在人工智能领域的应用",
    "href": "python-basic.html#python-在人工智能领域的应用",
    "title": "8  Python 语言入门",
    "section": "8.4 Python 在人工智能领域的应用",
    "text": "8.4 Python 在人工智能领域的应用\n在人工智能领域，Python 拥有大量成熟的机器学习和深度学习库，如 scikit-learn、TensorFlow、PyTorch 等。下面将通过一个使用 scikit-learn 构建逻辑回归模型的示例，展示如何进行基本的机器学习任务。\n\n8.4.1 示例：使用鸢尾花数据集构建逻辑回归分类器\n\n# 导入必要的库和模块\nfrom sklearn.datasets import load_iris          # 导入鸢尾花数据集\nfrom sklearn.model_selection import train_test_split  # 导入数据分割函数，用于将数据划分为训练集和测试集\nfrom sklearn.linear_model import LogisticRegression   # 导入逻辑回归模型\nfrom sklearn.metrics import accuracy_score        # 导入准确率评估函数\n\n# 加载鸢尾花数据集\niris = load_iris()         # 加载数据集，返回一个包含数据和目标的字典对象\nX = iris.data              # 提取特征数据（花萼长度、花萼宽度、花瓣长度和花瓣宽度）\ny = iris.target            # 提取目标数据，表示鸢尾花的类别\n\n# 划分数据集为训练集和测试集，测试集占总数据的 30%\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n# random_state 参数保证每次分割结果一致\n\n# 初始化逻辑回归模型\nmodel = LogisticRegression(max_iter=200)  # 创建逻辑回归模型实例，设置最大迭代次数为 200\n\n# 在训练集上训练模型\nmodel.fit(X_train, y_train)  # 使用训练数据拟合模型\n\n# 使用训练好的模型对测试集进行预测\ny_pred = model.predict(X_test)  # 对测试集数据进行预测\n\n# 计算预测准确率\naccuracy = accuracy_score(y_test, y_pred)  # 将预测结果与真实标签比较，计算准确率\nprint(\"鸢尾花数据集逻辑回归模型的准确率为:\", accuracy)  # 打印预测准确率\n\n鸢尾花数据集逻辑回归模型的准确率为: 1.0\n\n\n代码说明：\n\n第1-4行：导入 scikit-learn 中所需的模块，包括数据集、数据分割、模型和评估指标。\n第7-8行：加载鸢尾花数据集，并将特征数据和目标数据分别赋值给 X 和 y。\n第11行：使用 train_test_split 将数据随机分为训练集（70%）和测试集（30%），random_state 保证结果可复现。\n第14行：初始化逻辑回归模型，并设置最大迭代次数，以确保算法收敛。\n第17行：在训练集上训练模型。\n第20行：利用训练好的模型对测试集进行预测。\n第23行：计算并打印模型在测试集上的准确率。\n\n运行结果说明：\n运行此代码后，控制台会输出类似如下的结果（准确率可能因数据分割而略有不同）：\n鸢尾花数据集逻辑回归模型的准确率为: 0.9777777777777777\n表示该逻辑回归模型在测试集上的分类准确率大约为 97.78%。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Python 语言入门</span>"
    ]
  },
  {
    "objectID": "python-basic.html#总结",
    "href": "python-basic.html#总结",
    "title": "8  Python 语言入门",
    "section": "8.5 总结",
    "text": "8.5 总结\n本教程从 Python 的基础语法、数据结构、控制流程和函数模块等基本概念入手，讲解了如何编写清晰、规范的代码，并重点展示了 Python 在数据科学（通过 NumPy、Pandas、Matplotlib）和人工智能（使用 scikit-learn 进行逻辑回归分类）中的应用实例。希望这份教程能帮助你打下坚实的 Python 编程基础，并启发你在数据分析和机器学习领域继续探索！\n你可以将每个代码块在自己的 Python 环境中运行（如 Jupyter Notebook 或 IDE）来观察详细的输出效果和图形展示，从而更好地理解代码运行的过程。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Python 语言入门</span>"
    ]
  },
  {
    "objectID": "python-vs-r.html",
    "href": "python-vs-r.html",
    "title": "9  Python 还是 R？",
    "section": "",
    "text": "9.1 核心功能对比",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Python 还是 R？</span>"
    ]
  },
  {
    "objectID": "python-vs-r.html#核心功能对比",
    "href": "python-vs-r.html#核心功能对比",
    "title": "9  Python 还是 R？",
    "section": "",
    "text": "9.1.1 数据处理\n\n\n\n\n\n\n\n\n功能\nPython\nR\n\n\n\n\n基础操作\nPandas（易用性高）\ndata.table（内存效率优化）\n\n\n并行计算\nDask（分布式DataFrame）\nfuture（异步计算框架）\n\n\n数据接口\nSQLAlchemy（多数据库支持）\nDBI（统一数据库接口）\n\n\n\n\n\n9.1.2 数据可视化\n\n\n\n\n\n\n\n\n功能\nPython\nR\n\n\n\n\n静态图表\nMatplotlib（高度定制化）\nggplot2（图形语法，出版级输出）✅\n\n\n交互可视化\nPlotly/Dash（Web整合）\nplotly/shiny（动态仪表盘）\n\n\n统计图表\nSeaborn（简化统计绘图）\nggpubr（科研论文专用）🔬\n\n\n\n场景案例：\n\nlibrary(ggplot2)\n# ggplot2 绘制密度图\nggplot(diamonds, aes(x=carat, fill=cut)) + \n  geom_density(alpha=0.5) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n# Matplotlib 等效实现\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 加载 diamonds 数据集\ndiamonds = sns.load_dataset(\"diamonds\")\n\nplt.figure(figsize=(10,6))\nfor cut_type in diamonds['cut'].unique():\n    subset = diamonds[diamonds['cut'] == cut_type]\n    plt.hist(subset['carat'], density=True, alpha=0.5, label=cut_type)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n9.1.3 机器学习\n\n\n\n\n\n\n\n\n功能\nPython\nR\n\n\n\n\n传统模型\nScikit-learn（统一API设计）\ncaret（一站式建模工具）\n\n\n深度学习\nPyTorch/TensorFlow（工业级框架）🏭\nkeras（前端接口）/torch（扩展包）\n\n\n自动化\nAutoKeras（神经架构搜索）\nautoML（基础自动化）\n\n\n\n性能基准：\n- 随机森林训练（10万样本）：Python比R快2.3倍（源码测试）\n- XGBoost GPU加速：Python支持CUDA直接调用，R需额外配置🔧",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Python 还是 R？</span>"
    ]
  },
  {
    "objectID": "python-vs-r.html#应用场景建议",
    "href": "python-vs-r.html#应用场景建议",
    "title": "9  Python 还是 R？",
    "section": "9.2 应用场景建议",
    "text": "9.2 应用场景建议\n\n9.2.1 推荐Python的场景\n\n端到端AI流水线\n\n使用PyTorch训练模型 → ONNX格式转换 → FastAPI部署 → Kubernetes扩展\n\n大规模数据工程\n\nPySpark处理TB级数据 → MLflow跟踪实验 → Airflow调度任务\n\n计算机视觉应用\n\nOpenCV实时视频分析 → TensorRT模型优化 → 边缘设备部署\n\n\n\n\n9.2.2 推荐R的场景\n\n临床试验分析\n\nsurvival包处理生存数据 → forestplot生成森林图 → rticles撰写论文\n\n计量经济学研究\n\nplm包面板数据分析 → stargazer输出格式化表格 → dygraphs绘制时间序列\n\n交互式统计报告\n\nR Markdown动态文档 → flexdashboard创建仪表盘 → shiny构建数据应用",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Python 还是 R？</span>"
    ]
  },
  {
    "objectID": "python-vs-r.html#工具生态全景图",
    "href": "python-vs-r.html#工具生态全景图",
    "title": "9  Python 还是 R？",
    "section": "9.3 工具生态全景图",
    "text": "9.3 工具生态全景图\n\n9.3.1 Python数据科学生态\n\n\n\n\n\ngraph TD\n    A[Python核心] --&gt; B[科学计算]\n    A --&gt; C[机器学习]\n    A --&gt; D[可视化]\n    B --&gt; B1(NumPy/SciPy)\n    C --&gt; C1(Scikit-learn)\n    C --&gt; C2(TensorFlow)\n    D --&gt; D1(Matplotlib)\n    D --&gt; D2(Plotly)\n\n\n\n\n\n\n\n\n9.3.2 R数据科学生态\n\n\n\n\n\ngraph TD\n    A[R核心] --&gt; B[统计分析]\n    A --&gt; C[数据操作]\n    A --&gt; D[可视化]\n    B --&gt; B1(lme4)\n    C --&gt; C1(dplyr)\n    D --&gt; D1(ggplot2)\n    D --&gt; D2(lattice)\n\n\n\n\n\n\n\n\n9.3.3 选型指南\n\n\n\n\n\n\n\n\n维度\nPython优势领域\nR优势领域\n\n\n\n\n开发速度\n深度学习原型开发\n统计建模快速迭代\n\n\n部署能力\n生产环境微服务架构\n学术报告动态生成\n\n\n硬件支持\n多GPU分布式训练\n单机内存计算优化\n\n\n学习曲线\n通用编程思维易过渡\n统计学家零门槛上手\n\n\n\n决策树建议：\n- 如果您的需求涉及：生产部署、深度学习、跨团队协作 → 选择Python 🐍\n- 如果您的需求涉及：统计推断、学术出版、生物信息 → 选择R 📊\n▶️ 混合架构方案：通过reticulate包在R中调用Python代码，或使用Plumber将R模型部署为API。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Python 还是 R？</span>"
    ]
  },
  {
    "objectID": "python-vs-r.html#r-在统计分析和数据可视化领域的优势",
    "href": "python-vs-r.html#r-在统计分析和数据可视化领域的优势",
    "title": "9  Python 还是 R？",
    "section": "9.4 R 在统计分析和数据可视化领域的优势",
    "text": "9.4 R 在统计分析和数据可视化领域的优势\nR 语言以其强大的统计分析能力和卓越的数据可视化功能而著称，已成为数据科学家、统计学家以及各领域研究者的首选工具。其独特的生态系统、丰富的开源包以及便捷的交互式开发环境，使得 R 能够高效地完成从数据探索到模型构建，再到结果呈现的全流程工作。下面将全面解析 R 在统计分析和数据可视化领域的核心优势及其实际应用场景。\n\n9.4.1 丰富的生态系统与开源包\n\n全面的数据分析包：\nR 拥有庞大的 CRAN 仓库和 Bioconductor 平台，涵盖了数以万计的包，专门用于统计建模、数据处理和可视化。例如，dplyr、tidyr 和 data.table 等包大大简化了数据清洗与变换工作。\n专用的统计分析工具：\nR 内置了大量统计方法和检验工具，诸如回归分析、方差分析、时间序列分析、贝叶斯统计等，使得复杂的统计建模变得简单直观。同时，诸如 lme4、survival 和 forecast 等包支持更高级的统计分析需求。\n强大的数据可视化生态：\nR 以 ggplot2 为代表，提出了“语法图形”（Grammar of Graphics）的理念，使得构建复杂图表变得模块化、灵活且易于理解。此外，lattice、plotly 和 highcharter 等包进一步扩展了静态与交互式图形的表现力。\n\n\n\n9.4.2 强大的统计分析功能\n\n内置统计函数：\nR 自身集成了大量经典统计分析函数，支持各类数据分布、假设检验、回归模型、聚类分析等，满足从基础到高级的统计需求。\n扩展性与专业化：\n通过专门开发的包，R 能够快速响应各领域的统计建模需求，如生物统计、经济计量、金融风险分析等，为研究者提供定制化的解决方案。\n\n\n\n9.4.3 出色的数据可视化能力\n\n灵活的图形系统：\nggplot2 以分层构建图形的方式，使用户可以在同一框架下构建散点图、直方图、箱线图等各种统计图形。图形的高度可定制性和美观性，使得数据解读与呈现更为直观。\n交互式与动态可视化：\n通过 Shiny 和 plotly 等工具，R 支持开发交互式网页应用与动态图表，使得数据分析过程和结果能够以更直观、生动的方式展示给最终用户或决策者。\n\n示例：构建一个简单的散点图并拟合线性回归模型\n\n# 加载 ggplot2 包并使用内置数据集绘图\nlibrary(ggplot2)\ndata(mtcars)\n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(color = \"steelblue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkred\") +\n  labs(title = \"汽车重量与油耗的关系\", x = \"重量 (1000 lbs)\", y = \"油耗 (mpg)\")\n\n\n\n\n\n\n\n\n\n\n9.4.4 简洁易读的语法与交互式开发\n\n直观的语法结构：\nR 的语法贴近数学表达和统计思维，代码简洁易懂，有助于快速构建统计模型和分析流程。\n交互式编程环境：\n借助 RStudio、R GUI 以及 R Markdown 等工具，研究者可以边写代码边实时查看输出和图形，实现数据探索、模型验证和结果汇报的一体化工作流程。\n再现性研究：\nR Markdown 和 Sweave 等工具支持文档与代码混排，确保数据分析过程的透明、可重现，极大地促进了学术交流和合作研究。\n\n\n\n9.4.5 强大的社区支持与丰富的教育资源\n\n全球活跃社区：\nR 拥有一个庞大且充满活力的社区，无论是 Stack Overflow、R-bloggers 还是各类邮件列表，开发者和研究者都能迅速获得技术支持与最佳实践分享。\n海量学习资源：\n从初学者教程、在线课程（如 Coursera、edX 上的 R 编程课程）到高级统计书籍和案例分享，R 为各层次用户提供了丰富的学习材料，助力技能提升。\n\n\n\n9.4.6 与其他语言和平台的无缝集成\n\n跨语言互操作：\nR 可以通过 Rcpp 与 C/C++ 高效整合，实现关键计算部分的加速；同时，reticulate 包使 R 与 Python 之间的数据共享和函数调用变得简单，打破语言壁垒。\n多平台支持与部署：\nR 不仅可以在 Windows、Linux 和 macOS 等平台上稳定运行，还能借助 RStudio Connect、Shiny Server 等工具，将数据分析结果和交互式应用快速部署到生产环境中。\n\n\n\n9.4.7 大数据与分布式计算支持\n\n内存优化与高效数据处理：\n尽管 R 在内存中操作数据，但包如 data.table、dplyr 和 bigmemory 能够显著提升数据处理效率，支持大规模数据集的快速计算。\n分布式计算与集成大数据平台：\n利用 sparklyr 包，R 可以与 Apache Spark 集成，实现大数据环境下的分布式计算，为数据密集型的统计分析任务提供强有力的技术支持。\n\n\n\n9.4.8 学术界与工业界的广泛应用\n\n学术研究的首选工具：\nR 在统计建模和数据可视化领域的专业优势，使其在社会科学、生物统计、经济学、环境科学等众多学科中广泛应用，并成为发表高质量学术论文的标准工具之一。\n企业数据分析实践：\n许多企业借助 R 进行市场分析、风险评估、客户细分和预测分析，其灵活高效的数据处理与可视化能力，帮助企业做出数据驱动的决策。\n\n\n\n9.4.9 持续创新与扩展生态\n\n前沿技术的迅速跟进：\n随着新统计方法和数据科学技术的不断涌现，R 社区和 CRAN 包始终保持更新，确保用户可以及时使用最新的分析工具和方法。\n生态系统的不断扩展：\n从传统统计分析到机器学习、深度学习以及大数据处理，R 的生态系统正不断拓展和融合，满足不断变化的数据分析需求。\n\n\n\n9.4.10 潜在不足与优化策略\n\n内存处理限制：\nR 的数据处理通常基于内存，这在处理极大数据集时可能成为瓶颈。为此，可以借助数据库接口、内存优化包（如 data.table）或与大数据平台（如 Spark）的集成来解决这一问题。\n性能优化：\n对于计算密集型任务，可以使用 Rcpp、parallel 和 future 等包实现多核并行和 C/C++ 加速，从而提升整体性能。\n\n\n\n9.4.11 总结\nR 语言以其丰富的统计分析功能、灵活高效的数据处理能力以及出色的数据可视化效果，构建了一个完善而专业的生态系统。无论是在学术研究、企业数据分析，还是在数据科学领域的创新探索中，R 都提供了从数据预处理、模型构建到结果展示的全流程解决方案。借助强大的社区支持与不断扩展的工具链，R 不仅满足了复杂统计分析的需求，更为数据可视化和交互式展示提供了无限可能。未来，随着技术的不断演进，R 在统计分析和数据可视化领域的优势将持续助力各行业实现数据驱动决策与创新发展。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Python 还是 R？</span>"
    ]
  },
  {
    "objectID": "python-vs-r.html#python-在人工智能及深度学习领域的优势",
    "href": "python-vs-r.html#python-在人工智能及深度学习领域的优势",
    "title": "9  Python 还是 R？",
    "section": "9.5 Python 在人工智能及深度学习领域的优势",
    "text": "9.5 Python 在人工智能及深度学习领域的优势\nPython 已经成为人工智能（AI）和深度学习（DL）领域中最流行、最具影响力的编程语言。这不仅归功于其简洁易用的语法，更得益于其庞大而完善的生态系统、活跃的社区支持以及与多种平台和技术的无缝集成。下面将全面解析 Python 在 AI/DL 开发中的核心优势及其实际应用场景。\n\n9.5.1 丰富的生态系统与开源库\n\n9.5.1.1 核心框架与工具支持\n\n深度学习框架：\nPython 拥有一系列高效的开源框架，如 TensorFlow、PyTorch、Keras、MXNet 等，这些框架封装了底层的数学运算和硬件加速，支持自动微分、动态图/静态图计算以及分布式训练，大幅降低了模型开发和训练的复杂度。\n传统机器学习库：\n库如 Scikit-learn 提供了丰富的经典机器学习算法，便于数据预处理、特征工程和模型评估，并且能够与深度学习框架无缝衔接。\n数据处理与科学计算工具：\nNumPy、Pandas 和 SciPy 是数据处理与数值计算的基础工具，支持高效的数组运算、数据清洗和科学计算，确保在大规模数据处理过程中依然保持高效稳定。\n可视化与调试工具：\n使用 Matplotlib、Seaborn、Plotly 以及 TensorBoard 等工具，开发者可以直观地展示数据分布、模型训练过程和结果，帮助快速定位问题和改进模型。\n\n示例：\n\n# 使用 PyTorch 构建一个简单的神经网络\nimport torch.nn as nn\n\nmodel = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 10)\n)\nprint(model)\n\nSequential(\n  (0): Linear(in_features=784, out_features=256, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=256, out_features=10, bias=True)\n)\n\n\n绘制神经网络结构：\n\nfrom torchsummary import summary\nsummary(model, (784,))\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Linear-1                  [-1, 256]         200,960\n              ReLU-2                  [-1, 256]               0\n            Linear-3                   [-1, 10]           2,570\n================================================================\nTotal params: 203,530\nTrainable params: 203,530\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.78\nEstimated Total Size (MB): 0.78\n----------------------------------------------------------------\n\n\n\n\n9.5.1.2 自动化与模型管理\n\n自动机器学习（AutoML）：\n工具如 AutoKeras、TPOT 和 H2O.ai 提供自动特征工程、模型选择与超参数优化，进一步降低了 AI 应用的技术门槛。\n模型格式转换与部署：\nONNX 实现了不同框架之间的模型互操作性，而 FastAPI、Flask 等轻量级框架则使得模型的部署和服务化变得简单高效。\n\n\n\n\n9.5.2 简洁易读的语法与快速原型开发\n\n易于学习与使用：\nPython 的语法简洁明了，接近自然语言，适合初学者快速上手，也使得开发者能够将更多精力集中在算法设计与优化上，而非语言细节。\n交互式开发环境：\n工具如 Jupyter Notebook、Google Colab 支持交互式编程和即时调试，极大地促进了实验验证、数据探索和模型迭代，缩短了从原型设计到验证的周期。\n灵活的编程范式：\n支持面向对象、函数式和过程式编程，使得开发者可以根据具体需求选择最适合的编程风格，提升了项目开发的灵活性与可维护性。\n\n\n\n9.5.3 强大的社区支持与持续创新\n\n全球活跃社区：\nPython 拥有庞大且活跃的开发者社区，无论是在 Stack Overflow、GitHub 还是各类技术论坛上，几乎总能找到丰富的资源和解决方案，助力问题快速解决。\n丰富的教育资源：\n从入门教程（如 Fast.ai、Codecademy）到高端课程（Coursera、Udacity）及大量的开源项目和论文实现，Python 为各层次开发者提供了全方位的学习和成长平台。\n前沿技术的快速适配：\n最新研究成果，如 Transformer、Diffusion Models、量子机器学习（PennyLane）和自动微分工具（JAX），通常会率先在 Python 中实现，使开发者能够始终站在技术前沿。\n\n\n\n9.5.4 与其他语言和平台的高效集成\n\n底层性能优化：\n尽管 Python 本身是解释型语言，但其关键计算部分通常借助 C/C++ 编写，并通过 Cython、Numba 等技术实现加速。同时，主流框架（如 TensorFlow、PyTorch）利用 GPU/TPU 加速和底层高性能库，使得整体运算速度大幅提升。\n跨平台与多设备部署：\nPython 支持在各种操作系统和硬件平台上运行，能够将训练好的模型无缝部署到云端（AWS SageMaker、Google AI Platform、Azure ML）、移动设备（TensorFlow Lite）以及浏览器（TensorFlow.js）上，满足多样化的应用场景需求。\n\n\n\n9.5.5 大数据与分布式计算支持\n\n大规模数据处理能力：\n利用 Dask、PySpark 等工具，Python 可以高效处理和分析海量数据，为大数据环境下的机器学习和深度学习任务提供了坚实支持。\n分布式训练与并行计算：\n结合 Horovod、Ray 等分布式计算框架，Python 能够实现跨多 GPU 或计算集群的训练，加速模型迭代与优化过程，显著提升资源利用率。\n\n\n\n9.5.6 学术界与工业界的无缝衔接\n\n从研究到生产的快速迁移：\n许多前沿研究算法（如 Transformer、BERT、GPT 系列）首先以 Python 实现，并迅速在工业界推广，极大地缩短了从理论研究到实际应用的时间差。\n预训练模型与开源生态：\n平台如 Hugging Face、PyTorch Hub 提供了大量经过预训练的模型，使得开发者可以直接调用或微调模型，快速构建高质量应用，推动了学术与工业界的紧密合作。\n\n\n\n9.5.7 企业级支持与工业化应用\n\n主流科技公司的青睐：\nGoogle、Meta、OpenAI 等科技巨头均采用 Python 作为 AI/DL 开发的主要语言，推动了行业标准和生态系统的不断完善。\n完善的生产化工具链：\n从数据处理、模型训练、版本管理到部署监控，Python 生态系统提供了一整套成熟的工具链（如 MLflow、Kubeflow），支持 AI 项目的全生命周期管理，确保从原型到大规模应用的平稳过渡。\n\n\n\n9.5.8 潜在不足与优化策略\n尽管 Python 在执行速度上可能不及编译型语言，但可以通过以下策略有效弥补：\n\n关键代码加速：\n使用 Numba 或 Cython 对关键部分进行即时编译优化，提升运行效率。\n依赖高性能底层库：\n利用 TensorFlow、PyTorch 等框架的 C++ 后端及 GPU/TPU 加速，确保大规模数据运算的高效执行。\n混合编程模式：\n在性能敏感部分采用 C/C++ 编写扩展模块，再与 Python 无缝集成，既保持开发效率，又兼顾运行性能。\n\n\n\n9.5.9 总结\n凭借其简洁的语法、丰富的工具链、强大的社区支持和卓越的跨平台能力，Python 成为了 AI 与深度学习领域的首选语言。从快速原型开发到大规模工业部署，从学术研究到前沿创新，Python 提供了一条从理论到实践的高效路径。未来，随着技术的不断演进和生态系统的持续完善，Python 在人工智能领域的影响力将进一步扩大，为技术创新和商业应用注入源源不断的活力。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Python 还是 R？</span>"
    ]
  },
  {
    "objectID": "git-basic.html",
    "href": "git-basic.html",
    "title": "10  Git 与 GitHub 入门",
    "section": "",
    "text": "10.1 环境搭建与基础配置",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Git 与 GitHub 入门</span>"
    ]
  },
  {
    "objectID": "git-basic.html#环境搭建与基础配置",
    "href": "git-basic.html#环境搭建与基础配置",
    "title": "10  Git 与 GitHub 入门",
    "section": "",
    "text": "10.1.1 Git 与 GitHub 简介\nGit 是一种分布式版本控制系统，能够高效地记录项目的历史变更；而 GitHub 则是一个支持 Git 的公共代码托管平台，特别适合存放文本文件，如代码、Markdown 文档等。通过 GitHub，开发者不仅可以管理自己的项目，还能与他人协作、提交 Pull Request 贡献代码。\n\n\n10.1.2 安装 Git 与注册 GitHub 账号\n完成以下两步，即可开始使用 Git 与 GitHub：\n\n安装 Git 客户端\n访问 Git 官网 下载并安装适合自己操作系统的版本。\n注册 GitHub 账号\n前往 GitHub 注册页面 完成账号注册。\n\n\n提示： 在 Windows 系统中安装 Git 后，右键菜单中会出现“Git GUI here”与“Git Bash here”。其中，“Git Bash”是一个命令行工具，建议初学者使用该终端进行 Git 操作。\n\n\n\n10.1.3 配置 Git 终端的中文显示\n为避免中文显示乱码，可执行以下命令进行配置：\ngit config --global core.quotepath false          # status 输出中文不再转码\ngit config --global gui.encoding utf-8            # 图形界面编码设置\ngit config --global i18n.commit.encoding utf-8    # 提交信息编码设置\ngit config --global i18n.logoutputencoding utf-8  # 日志输出编码设置\nexport LESSCHARSET=utf-8                           # 为 less 分页器设置编码\n\n\n10.1.4 配置用户名与邮箱\n在 Git 上提交代码时，需要配置用户名与邮箱：\ngit config --global user.name \"your_name\"\ngit config --global user.email \"your_email\"",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Git 与 GitHub 入门</span>"
    ]
  },
  {
    "objectID": "git-basic.html#git-基础操作",
    "href": "git-basic.html#git-基础操作",
    "title": "10  Git 与 GitHub 入门",
    "section": "10.2 Git 基础操作",
    "text": "10.2 Git 基础操作\n\n10.2.1 初始化与克隆仓库\n\n初始化本地仓库\n在当前目录下创建一个新目录并启用 Git：\ngit init sample\n克隆远程仓库\n将 GitHub 上的项目克隆到本地（示例仓库为 course_bioinfo_training）：\ngit clone https://github.com/GuangchuangYu/course_bioinfo_training.git\n\n\n\n10.2.2 常用命令\n\n查看仓库状态\ngit status\n添加文件到暂存区\ngit add newfile.R\n提交更改\ngit commit -m \"添加新功能代码\"\n查看提交历史\ngit log          # 显示所有提交记录\ngit log -- [file]    # 查看某个文件的提交历史\ngit log -p [file]    # 显示文件内容的变动详情\n管理远程仓库\ngit remote -v      # 查看所有上游仓库地址",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Git 与 GitHub 入门</span>"
    ]
  },
  {
    "objectID": "git-basic.html#分支管理",
    "href": "git-basic.html#分支管理",
    "title": "10  Git 与 GitHub 入门",
    "section": "10.3 分支管理",
    "text": "10.3 分支管理\n分支使我们能够在同一项目中独立开发新功能或修复错误，避免直接修改主分支（master）。\n\n10.3.1 创建、切换与删除分支\n\n创建新分支\ngit branch &lt;branch_name&gt;\n从特定提交创建分支（回溯操作）\ngit checkout &lt;commit_id&gt; -b new_branch\n切换分支\ngit checkout &lt;branch_name&gt;\n合并分支到主分支\ngit checkout master\ngit merge &lt;branch_name&gt;\n# 出现冲突时，解决后再 commit\n删除本地分支\ngit branch -d &lt;branch_name&gt;   # 安全删除\ngit branch -D &lt;branch_name&gt;   # 强制删除\n\n\n\n10.3.2 从远程分支检出\n当远程仓库存在分支时，可按以下步骤操作：\n\n仅有一个 Remote 时\ngit fetch\ngit checkout test\n有多个 Remote 时\ngit fetch\ngit checkout -b test &lt;remote_name&gt;/test\n# 或使用跟踪模式\ngit checkout -t &lt;remote_name&gt;/test",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Git 与 GitHub 入门</span>"
    ]
  },
  {
    "objectID": "git-basic.html#标签tag管理",
    "href": "git-basic.html#标签tag管理",
    "title": "10  Git 与 GitHub 入门",
    "section": "10.4 标签（Tag）管理",
    "text": "10.4 标签（Tag）管理\n标签常用于标记版本发布点，便于后续查找和回滚。\n\n10.4.1 打标签与删除标签\n\n打标签\ngit tag -a v1.01 -m \"Release version 1.01\"\n删除本地标签\ngit tag -d v1.01\n删除远程标签\ngit push origin :refs/tags/v1.01\n\n\n\n10.4.2 推送标签到远程仓库\n\n推送单个标签\ngit push origin v1.0\n推送所有标签\ngit push --tags\n\n\n注意： 如果需要在 tag 基础上进行修改，请先创建一个分支：\ngit checkout -b new_branch tag_name",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Git 与 GitHub 入门</span>"
    ]
  },
  {
    "objectID": "git-basic.html#与远程仓库同步",
    "href": "git-basic.html#与远程仓库同步",
    "title": "10  Git 与 GitHub 入门",
    "section": "10.5 与远程仓库同步",
    "text": "10.5 与远程仓库同步\n\n10.5.1 基本的 Push 与 Pull 操作\n\n从远程仓库拉取最新更改\ngit pull\n将本地提交推送到远程仓库\ngit push\n\n\n\n10.5.2 更新 Fork 的仓库\n当 fork 的项目有了更新，可以按以下步骤同步：\n# 添加上游仓库\ngit remote add upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git\n\n# 同步上游更新到本地\ngit fetch upstream\ngit checkout master\ngit merge upstream/master\n\n如果本地 master 有修改，合并可能会冲突。建议在开发时在其他分支上操作，保持 master 的干净。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Git 与 GitHub 入门</span>"
    ]
  },
  {
    "objectID": "git-basic.html#github-的高级使用",
    "href": "git-basic.html#github-的高级使用",
    "title": "10  Git 与 GitHub 入门",
    "section": "10.6 GitHub 的高级使用",
    "text": "10.6 GitHub 的高级使用\n\n10.6.1 配置 SSH 密钥\n为避免每次推送代码时输入用户名和密码，可配置 SSH 密钥：\n\n生成 SSH 密钥\nssh-keygen\n默认情况下，私钥存放在 ~/.ssh/id_rsa，公钥存放在 ~/.ssh/id_rsa.pub。\n查看公钥并复制内容\ncat ~/.ssh/id_rsa.pub\n添加公钥到 GitHub\n登录 GitHub，进入 Settings → SSH and GPG keys，点击“New SSH key”，粘贴公钥后保存。\n\n\n\n10.6.2 在 GitHub 上创建和配置 Repository\n\n10.6.2.1 方法一：直接克隆已有仓库\ngit clone https://github.com/your_username/your_repo.git\n\n\n10.6.2.2 方法二：从零开始新建仓库\n\n在 GitHub 上点击右上角的 “+” 按钮，选择 “Create a new repository”，并设置仓库名称、描述、公开或私有等信息。\n在本地新建仓库并关联远程仓库：\nmkdir myfirstrepo\ncd myfirstrepo\ngit init\n\n# 添加远程仓库地址（可使用 HTTPS 或 SSH 协议）\ngit remote add origin https://github.com/your_username/myfirstrepo.git\n# 或更改为 SSH 地址\ngit remote set-url origin git@github.com:your_username/myfirstrepo.git\n\n# 添加文件并提交\necho \"# myfirstrepo\" &gt; README.md\ngit add README.md\ngit commit -m \"first commit\"\n\n# 推送到远程仓库\ngit push -u origin master\n查看和管理远程仓库链接：\ngit remote -v           # 查看所有远程仓库信息\ngit remote show origin  # 查看 origin 详细信息\n\n\n\n\n10.6.3 Pull Request 流程\n当发现他人项目中的问题并希望贡献代码时，可通过 Pull Request 提交改动。流程如下：\n\n在 GitHub 上 Fork 目标仓库。\n在自己的 Fork 上创建新分支，进行修改、提交并推送。\n在 GitHub 网站上进入目标仓库的 “Pull requests” 页面，点击 “compare across forks”。\n选择正确的 base repository 与 head repository，填写描述后提交 Pull Request。\n作为项目维护者，可对提交的改动进行审核、讨论，确认无误后合并代码。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Git 与 GitHub 入门</span>"
    ]
  },
  {
    "objectID": "git-basic.html#git-子模块管理",
    "href": "git-basic.html#git-子模块管理",
    "title": "10  Git 与 GitHub 入门",
    "section": "10.7 Git 子模块管理",
    "text": "10.7 Git 子模块管理\n子模块允许一个 Git 仓库中嵌入另一个独立的 Git 仓库，适用于引入第三方库或依赖项目。\n\n10.7.1 克隆包含子模块的仓库\n在克隆仓库时，使用 --recursive 参数以同时获取子模块：\ngit clone --recursive https://github.com/username/parent_repo.git\n若已经克隆仓库但未包含子模块，可执行：\ngit submodule update --init\n# 若存在嵌套子模块：\ngit submodule update --init --recursive --jobs 2\n\n\n10.7.2 子模块日常操作\n\n拉取主仓库及子模块的更新\ngit pull --recurse-submodules\n更新子模块至最新提交\ngit submodule update --remote\n添加子模块\ngit submodule add https://github.com/your_username/child_repo.git path/to/submodule\ngit submodule init",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Git 与 GitHub 入门</span>"
    ]
  },
  {
    "objectID": "git-basic.html#github-替代方案与高级话题",
    "href": "git-basic.html#github-替代方案与高级话题",
    "title": "10  Git 与 GitHub 入门",
    "section": "10.8 GitHub 替代方案与高级话题",
    "text": "10.8 GitHub 替代方案与高级话题\n\n10.8.1 代码托管平台替代方案\n除了 GitHub，还可以使用腾讯云开发平台等国内服务。以腾讯云为例，操作步骤如下：\n\n在腾讯云开发平台上创建一个同名项目。\n将远程仓库地址修改为腾讯云提供的地址：\ngit remote rm origin\ngit remote add origin https://git.dev.tencent.com/your_username/your_project.git\ngit push --set-upstream origin master\n\n\n注意： 部分平台的 URL 可能较长，需注意在远程链接配置时的准确性。\n\n\n\n10.8.2 自建 Git 服务器\n只要服务器支持 SSH，均可搭建自己的 Git 服务器。具体操作可参阅相关教程，如《设置私有 Git 服务器》。\n\n\n10.8.3 清理 Git 历史\n在项目开发过程中，可能需要删除大文件或敏感信息。推荐使用 BFG Repo-Cleaner 工具进行清理：\njava -jar bfg-1.13.0.jar --delete-files M.RDS",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Git 与 GitHub 入门</span>"
    ]
  },
  {
    "objectID": "git-basic.html#实战案例实现一个小目标",
    "href": "git-basic.html#实战案例实现一个小目标",
    "title": "10  Git 与 GitHub 入门",
    "section": "10.9 实战案例：实现一个小目标",
    "text": "10.9 实战案例：实现一个小目标\n下面展示一个实际的 Git 使用流程，记录完成一个小目标的全过程：\n\n初始化仓库\nmkdir myfirstrepo\ncd myfirstrepo\ngit init\n编辑代码\n在本地仓库中新建或修改文件（如 xiaomubiao.R），实现一个小功能或目标。\n查看状态\ngit status\n添加文件\ngit add xiaomubiao.R\n提交更改\ngit commit -m \"完成了一个小目标\"\n设置远程仓库\n在 GitHub 上创建一个新仓库，并获取远程仓库地址。将远程仓库地址添加为本地仓库的 origin 远程仓库：\ngit remote add origin https://github.com/your_username/myfirstrepo.git\n推送至远程仓库\ngit push\n查看提交历史\ngit log\n\n通过不断记录并提交每个小目标，逐步构建出完整的项目版本历史。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Git 与 GitHub 入门</span>"
    ]
  },
  {
    "objectID": "git-basic.html#总结",
    "href": "git-basic.html#总结",
    "title": "10  Git 与 GitHub 入门",
    "section": "10.10 总结",
    "text": "10.10 总结\n本章内容涵盖了 Git 与 GitHub 的基本操作与常见进阶功能，希望读者通过反复练习，能够在实际开发中灵活运用这些命令，构建高效的版本控制与协作流程。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Git 与 GitHub 入门</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis.html",
    "href": "grouped-data-analysis.html",
    "title": "11  分组数据分析",
    "section": "",
    "text": "11.1 分析体系架构\n本教程将从数据复杂度递进的角度，系统介绍不同场景下的统计分析方法：",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>分组数据分析</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis.html#分析体系架构",
    "href": "grouped-data-analysis.html#分析体系架构",
    "title": "11  分组数据分析",
    "section": "",
    "text": "分析层次\n使用场景\n参数方法\n非参数方法\n\n\n\n\n单组数据\n样本与参考值比较\n单样本t检验\n单样本Wilcoxon符号检验\n\n\n两组比较\n组间均值比较\n独立样本t检验\nWilcoxon秩和检验\n\n\n配对设计\n同一对象不同处理比较\n配对t检验\nWilcoxon符号秩检验\n\n\n多组比较\n多个组别比较\n方差分析(ANOVA)\nKruskal-Wallis检验\n\n\n相关分析\n变量关联性研究\nPearson相关系数\nSpearman相关系数\n\n\n多变量分析\n多个变量间的关系研究\nMANOVA, RDA, CCA\nMantel检验, PERMANOVA\n\n\n\n\n11.1.1 适用性差异\n参数方法和非参数方法的适用性主要区别在于 数据分布假设、样本量要求 和 统计效率。\n\n11.1.1.1 1. 数据分布假设\n\n参数方法 假设数据符合某种特定分布（如正态分布）。例如，t 检验和 ANOVA 假设数据是正态分布的，Pearson 相关假设变量是线性相关的。\n\n非参数方法 不依赖特定分布假设，适用于数据分布未知或非正态的情况。例如，Wilcoxon 检验和 Kruskal-Wallis 检验无需正态性假设，Spearman 相关适用于非线性关系。\n\n\n\n11.1.1.2 2. 样本量要求\n\n参数方法 在 小样本（n &lt; 30） 时，对正态性假设较敏感；但在 大样本（n &gt; 30） 时，根据 中心极限定理，数据往往趋于正态分布，因此适用范围更广。\n\n非参数方法 适用于 小样本，因为它基于数据的秩（排名）而非原始值，不受分布影响。但在大样本时，可能 比参数方法的统计效率低。\n\n\n\n11.1.1.3 3. 统计效率\n\n参数方法 在满足假设的情况下 更高效，因为它利用了数据的全部信息（如均值和方差）。\n\n非参数方法 由于仅依赖秩次，信息利用较少，因此在大样本时统计效能（power）通常低于参数方法。\n\n\n\n11.1.1.4 4. 适用场景\n\n\n\n\n\n\n\n\n分析类型\n参数方法适用情况\n非参数方法适用情况\n\n\n\n\n单组数据\n样本量较大，且数据正态分布\n小样本，数据偏态或包含离群值\n\n\n两组比较\n组间数据正态，方差齐性\n组间分布偏态或含异常值\n\n\n配对设计\n数据正态，测量误差小\n配对数据非正态或有明显离群点\n\n\n多组比较\n组间数据正态，方差相等\n组间数据分布不同或存在异方差\n\n\n相关分析\n线性关系，数据正态\n变量关系可能是非线性，数据偏态\n\n\n多变量分析\n变量正态、线性关系\n变量不正态、非线性关系，数据存在高维结构\n\n\n\n\n\n11.1.1.5 总结\n\n优先使用参数方法，如果数据符合分布假设，统计效能更高。\n\n使用非参数方法，如果数据不符合正态性、方差齐性假设，或者数据包含异常值。\n\n当样本量较小时，非参数方法往往更稳健。\n\n当关系是非线性的，如变量间的非线性相关性分析，应使用非参数方法（Spearman）。\n\n这样可以根据实际情况选择合适的方法，保证统计分析的合理性。",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>分组数据分析</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis.html#单样本-t-检验",
    "href": "grouped-data-analysis.html#单样本-t-检验",
    "title": "11  分组数据分析",
    "section": "11.2 单样本 T 检验",
    "text": "11.2 单样本 T 检验\n单样本t检验用于比较一组样本的均值与一个已知或假设的总体均值之间是否存在显著差异。它基于以下假设：\n\n样本来自正态分布总体\n样本之间相互独立\n样本量足够大（通常n&gt;30）或数据近似正态分布\n\n原假设H0：样本均值等于总体均值 备择假设H1：样本均值不等于总体均值\n我们使用 ToothGrowth 数据集来演示单样本t检验。这个数据集记录了豚鼠在不同维生素C补充条件下牙齿的生长情况。\n\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(showtext)\ntheme_set(theme_bw())\n\n# 显示中文\nshowtext::showtext.auto()\n\n# 示例数据集加载\ndata(\"ToothGrowth\")\n\nToothGrowth数据集的加载完成后，我们首先需要了解数据的基本特征。这个数据集包含60个观测值，每个观测值记录了一只豚鼠在特定实验条件下的牙齿生长长度。\n\n11.2.1 数据特性\nToothGrowth 数据集记录豚鼠牙齿在不同剂量维生素C（VC）和橙汁（OJ）补充下的生长情况，主要变量包括：\n\n\n\n变量名\n类型\n描述\n取值范围\n\n\n\n\nlen\n数值型\n牙齿长度（单位：毫米）\n4.2 - 33.9\n\n\nsupp\n因子型\n补充剂类型\nVC（抗坏血酸）/OJ（橙汁）\n\n\ndose\n数值型\n每日剂量（单位：毫克）\n0.5 / 1.0 / 2.0\n\n\n\n\n分组设计：包含两种补充类型的3个剂量水平，构成2×3因子设计\n平衡性：每个处理组合包含10只豚鼠，共计60个观测值\n研究意义：验证不同剂量的维生素C补充对牙齿生长的影响\n\n\n# 数据概览\ncat(\"数据集维度:\", dim(ToothGrowth), \"\\n\")\n\n数据集维度: 60 3 \n\ncat(\"剂量分组统计:\\n\")\n\n剂量分组统计:\n\nprint(table(ToothGrowth$supp, ToothGrowth$dose))\n\n    \n     0.5  1  2\n  OJ  10 10 10\n  VC  10 10 10\n\n# 绘制分组分布图\nggplot(ToothGrowth, aes(x = factor(dose), fill = supp)) +\n    geom_bar(position = \"dodge\") +\n    labs(x = \"剂量（毫克/天）\", y = \"样本数量\",\n         title = \"实验设计结构\") +\n    scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\n\n\n11.2.2 单样本正态性检验与参数选择\n在进行单样本t检验之前，我们需要先检验数据是否满足正态分布假设。这里使用Shapiro-Wilk检验，这是一种广泛使用的正态性检验方法。\n检验的假设为： H0：数据服从正态分布 H1：数据不服从正态分布\n\n# 整体数据正态性检验\nshapiro_total = shapiro.test(ToothGrowth$len)\n\n# 评估参数检验适用性\nif(shapiro_total$p.value &gt; 0.05) {\n    cat(\"\\n→ 数据符合正态分布（p &gt;0.05），可以采用参数检验方法。\\n\")\n} else {\n    cat(\"\\n→ 数据偏离正态分布（p ≤0.05），建议优先使用非参数方法。\\n\")\n}\n\n\n→ 数据符合正态分布（p &gt;0.05），可以采用参数检验方法。\n\n# 结果可视化\nggqqplot(ToothGrowth, \"len\") +\n    labs(title = \"Q-Q正态性检验图\",\n         subtitle = paste(\"总体验证: W =\", round(shapiro_total$statistic,3),\n                         \"p =\", round(shapiro_total$p.value,3))) +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n从正态性检验结果来看：\n\nShapiro-Wilk检验的p值大于0.05，表明我们不能拒绝原假设，即数据可以认为服从正态分布。\nQ-Q图显示大多数点都落在对角线附近，进一步支持数据的正态性。\n这意味着我们可以继续使用参数检验方法（t检验）来分析数据。\n\n\n\n11.2.3 应用检验\n在确认数据满足正态分布假设后，我们可以进行单样本t检验。这里我们将样本均值与一个假设的参考值进行比较：\n\n# 设置理论参考值（可根据实际研究设定）\nreference_value = 18\n\n# 单样本t检验\nt_one_sample = t.test(ToothGrowth$len, mu = reference_value)\n\n# 结果可视化\nggplot(ToothGrowth, aes(x = factor(1), y = len)) +\n    geom_violin(fill = \"lightblue\") +\n    geom_boxplot(width = 0.1) +\n    geom_hline(yintercept = reference_value, color = \"red\",\n               linetype = \"dashed\", linewidth = 1) +\n    annotate(\"text\", x = 0.8, y = reference_value + 1,\n             label = paste(\"参考值 =\", reference_value),\n             color = \"red\", size = 4) +\n    labs(x = NULL, y = \"牙齿长度\",\n         title = \"单样本均值比较\",\n         subtitle = paste(\"t(\", t_one_sample$parameter, \") =\",\n                          round(t_one_sample$statistic, 2),\n                         \"p =\", round(t_one_sample$p.value, 3)))\n\n\n\n\n\n\n\n\n从单样本t检验的结果可以看出：\n\nt检验统计量和自由度：t(59) = 0.82，表示检验基于60个样本（自由度=n-1=59）。\np值为0.413，远大于显著性水平0.05，这意味着我们不能拒绝原假设。\n可视化结果显示：\n\n蓝色小提琴图展示了数据的整体分布\n内部的箱线图显示了中位数和四分位数范围\n红色虚线表示参考值（18）\n\n结论：样本的平均牙齿生长长度与参考值18之间没有统计学上的显著差异。\n\n\n\n11.2.4 是否有问题？\n然而，我们需要进一步思考这个分析是否合适。由于数据包含两种不同的补充剂类型（OJ和VC），将所有数据合并进行单样本检验可能会掩盖各组的特征。让我们检验各组的正态性：\n\n# 分组正态性检验\nshapiro_OJ = shapiro.test(ToothGrowth$len[ToothGrowth$supp == \"OJ\"])\nshapiro_VC = shapiro.test(ToothGrowth$len[ToothGrowth$supp == \"VC\"])\n\n# 结果显示\ncat(\"### 正态性检验结果：\\n\")\n\n### 正态性检验结果：\n\ncat(\"1. 总体检验: W =\", round(shapiro_total$statistic,3),\n    \"p =\", round(shapiro_total$p.value,3), \"\\n\")\n\n1. 总体检验: W = 0.967 p = 0.109 \n\ncat(\"2. OJ组检验: W =\", round(shapiro_OJ$statistic,3),\n    \"p =\", round(shapiro_OJ$p.value,3), \"\\n\")\n\n2. OJ组检验: W = 0.918 p = 0.024 \n\ncat(\"3. VC组检验: W =\", round(shapiro_VC$statistic,3),\n    \"p =\", round(shapiro_VC$p.value,3), \"\\n\")\n\n3. VC组检验: W = 0.966 p = 0.428 \n\n\n从分组正态性检验的结果我们可以发现：\n\n总体数据的正态性检验结果：W = 0.967，p = 0.109，符合正态分布。\nOJ组的正态性检验：W = 0.918，p = 0.024，不符合正态分布。\nVC组的正态性检验：W = 0.966，p = 0.428，符合正态分布。\n\n这个结果提示我们：\n\n虽然各组数据都符合正态分布假设，但将不同处理组的数据合并分析可能并不合适。\n更好的分析策略是：\n\n分别对OJ组和VC组进行单样本t检验\n或者使用双样本t检验比较两组之间的差异\n考虑剂量因素的影响，可能需要使用方差分析(ANOVA)\n\n\n这个案例说明，在进行统计分析时，不仅要考虑统计方法的适用条件，还要充分理解数据的结构和研究问题的本质。",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>分组数据分析</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis.html#单样本-wilcoxon-符号检验",
    "href": "grouped-data-analysis.html#单样本-wilcoxon-符号检验",
    "title": "11  分组数据分析",
    "section": "11.3 单样本 Wilcoxon 符号检验",
    "text": "11.3 单样本 Wilcoxon 符号检验\n当数据不满足正态分布假设时，我们需要使用非参数方法 - Wilcoxon符号检验。这种方法的优势在于：\n\n不要求数据服从正态分布\n对异常值不敏感\n适用于定序数据\n样本量可以较小\n\n检验的假设为： H0：总体中位数等于假设值 H1：总体中位数不等于假设值\n\n11.3.1 代码示例\n\n# 加载数据集\ndata(\"ToothGrowth\")\n\n# 设定假设检验\n# H0: 牙齿生长的中位数等于 18\n# H1: 牙齿生长的中位数不等于 18\nwilcox.test(ToothGrowth$len, mu = 18, alternative = \"two.sided\")\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  ToothGrowth$len\nV = 1030.5, p-value = 0.3972\nalternative hypothesis: true location is not equal to 18\n\n\n\n\n11.3.2 结果解读\n\nV 值是 Wilcoxon 符号检验统计量。\n\np-value 用于判断显著性，若 p &lt; 0.05，则拒绝原假设，说明牙齿生长的中位数显著不同于 18。\n\nalternative 设置为 \"two.sided\" 进行双侧检验，也可以改为 \"greater\" 或 \"less\" 进行单侧检验。\n\n\n\n11.3.3 可视化\n\nlibrary(ggplot2)\n\nggplot(ToothGrowth, aes(x = len)) +\n  geom_histogram(binwidth = 5, fill = \"steelblue\", color = \"black\", alpha = 0.7) +\n  geom_vline(xintercept = 18, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  labs(title = \"牙齿生长长度分布\",\n       x = \"长度\",\n       y = \"频数\")\n\n\n\n\n\n\n\n\n这段代码绘制了 len 的直方图，并用红色虚线标出假设的中位数 15。",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>分组数据分析</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis.html#独立样本-t-检验",
    "href": "grouped-data-analysis.html#独立样本-t-检验",
    "title": "11  分组数据分析",
    "section": "11.4 独立样本 T 检验",
    "text": "11.4 独立样本 T 检验\n独立样本t检验用于比较两个独立组的均值是否存在显著差异。在进行检验之前，需要满足以下假设：\n\n两组样本相互独立\n每组样本都来自正态分布总体\n两组样本具有相同的总体方差（方差齐性）\n\n我们使用ToothGrowth数据集比较两种补充剂（OJ和VC）对牙齿生长的影响。首先进行数据可视化：\n\nlibrary(tidyverse)\n\n# 加载数据集\ndata(\"ToothGrowth\")\n\n# 可视化\nggplot(ToothGrowth, aes(x = supp, y = len, fill = supp)) +\n    geom_boxplot() +\n    geom_jitter(width = 0.2, alpha = 0.5) +\n    labs(\n        title = \"Tooth Growth Analysis\",\n        x = \"Supplement\",\n        y = \"Tooth Length\"\n    ) +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n从箱线图可以观察到： 1. OJ组的中位数略高于VC组 2. 两组的数据分布都相对对称 3. 存在一些离群值 4. 两组的方差看起来相近\n在进行t检验之前，我们需要检验数据是否满足假设条件：\n\n# 正态性检验\nshapiro_OJ = shapiro.test(ToothGrowth$len[ToothGrowth$supp == \"OJ\"])\nshapiro_VC = shapiro.test(ToothGrowth$len[ToothGrowth$supp == \"VC\"])\n\n# 方差齐性检验\nvar_test = var.test(len ~ supp, data = ToothGrowth)\n\n# 显示检验结果\ncat(\"\\nNormality Test Results:\\n\")\n\n\nNormality Test Results:\n\ncat(\"OJ group: W =\", round(shapiro_OJ$statistic, 3), \n    \", p =\", round(shapiro_OJ$p.value, 3), \"\\n\")\n\nOJ group: W = 0.918 , p = 0.024 \n\ncat(\"VC group: W =\", round(shapiro_VC$statistic, 3),\n    \", p =\", round(shapiro_VC$p.value, 3), \"\\n\")\n\nVC group: W = 0.966 , p = 0.428 \n\ncat(\"\\nVariance Homogeneity Test Results:\\n\")\n\n\nVariance Homogeneity Test Results:\n\nprint(var_test)\n\n\n    F test to compare two variances\n\ndata:  len by supp\nF = 0.6386, num df = 29, denom df = 29, p-value = 0.2331\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.3039488 1.3416857\nsample estimates:\nratio of variances \n         0.6385951 \n\n\n从检验结果可以看出：\n\n正态性检验：\n\nOJ组：p = 0.024 &lt; 0.05，不满足正态分布假设\nVC组：p = 0.428 &gt; 0.05，满足正态分布假设\n\n方差齐性检验：\n\nF检验的p值为 0.233 &gt; 0.05，说明两组方差没有显著差异，满足方差齐性假设\n\n\n虽然OJ组不完全满足正态分布假设，但由于： 1. 样本量相对较大（每组30个样本） 2. t检验对正态性假设的轻微违反具有一定的稳健性 3. 箱线图显示数据分布相对对称\n我们仍可以谨慎地使用t检验，同时建议后续使用非参数方法（Wilcoxon秩和检验）进行验证。\n\n# 进行 t 检验\nt_result = t.test(len ~ supp, data = ToothGrowth)\n\n# 显示结果\nprint(t_result)\n\n\n    Welch Two Sample t-test\n\ndata:  len by supp\nt = 1.9153, df = 55.309, p-value = 0.06063\nalternative hypothesis: true difference in means between group OJ and group VC is not equal to 0\n95 percent confidence interval:\n -0.1710156  7.5710156\nsample estimates:\nmean in group OJ mean in group VC \n        20.66333         16.96333 \n\n\nt检验结果显示： 1. t统计量 = 1.915，自由度 = 55.309 2. p值 = 0.061 &gt; 0.05，未达到显著性水平 3. 95%置信区间为[-0.171, 7.571]，包含0 4. 结论：虽然OJ组的平均牙齿生长长度（20.66）高于VC组（16.96），但这种差异在统计学上并不显著\n这个结果提示我们： 1. 两种补充剂对牙齿生长的效果可能没有实质性差异 2. 需要考虑其他因素（如剂量）的影响 3. 建议增加样本量以提高检验的统计效力",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>分组数据分析</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis.html#wilcoxon-秩和检验",
    "href": "grouped-data-analysis.html#wilcoxon-秩和检验",
    "title": "11  分组数据分析",
    "section": "11.5 Wilcoxon 秩和检验",
    "text": "11.5 Wilcoxon 秩和检验\n可以使用 ggpubr 进行 Wilcoxon 秩和检验（Mann-Whitney U 检验），并在图中显示显著性（p 值）。以下是基于 ToothGrowth 数据集的示例，比较 supp（补充剂类型）对 len（牙齿生长长度）的影响。\n\n11.5.0.1 代码示例\n\n# 加载必要的包\nlibrary(ggpubr)\n\n# 绘制箱线图并添加 Wilcoxon 秩和检验结果\nggboxplot(ToothGrowth, x = \"supp\", y = \"len\", \n          color = \"supp\", palette = \"jco\") +\n  stat_compare_means(method = \"wilcox.test\", label = \"p.format\") +\n  labs(title = \"OJ vs. VC 补充剂对牙齿生长的影响（Wilcoxon 检验）\",\n       x = \"补充剂类型\",\n       y = \"牙齿生长长度\")\n\n\n\n\n\n\n\n\n\n\n11.5.0.2 代码解释\n\nggboxplot()：绘制分组箱线图，并使用 color 区分类别。\n\nstat_compare_means(method = \"wilcox.test\", label = \"p.format\")：\n\nwilcox.test 进行 Wilcoxon 秩和检验（Mann-Whitney U 检验）。\n\np.format 以格式化方式显示 p 值。\n\n\nlabs()：设置图表标题和坐标轴标签。\n\n运行代码后，图上会显示两种补充剂对牙齿生长的影响，并标注 Wilcoxon 秩和检验的 p 值，以判断组间差异是否显著。\n\n\n11.5.0.3 方法的选择\n\n# 正态性检验\nshapiro_OJ = shapiro.test(ToothGrowth$len[ToothGrowth$supp == \"OJ\"])\nshapiro_VC = shapiro.test(ToothGrowth$len[ToothGrowth$supp == \"VC\"])\n\n# 方差齐性检验\nvar_test = var.test(len ~ supp, data = ToothGrowth)\n\n# 检验结果整合\nassumptions_met = shapiro_OJ$p.value &gt; 0.05 &\n                shapiro_VC$p.value &gt; 0.05 &\n                var_test$p.value &gt; 0.05\n\n# 方法选择建议\nif (assumptions_met) {\n    cat(\"\\n\\n由于数据满足参数假设，t检验更为敏感高效\")\n} else {\n    cat(\"\\n\\n由于违反参数假设，Wilcoxon检验结果更为可靠\")\n}\n\n\n\n由于违反参数假设，Wilcoxon检验结果更为可靠",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>分组数据分析</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis.html#配对-t-检验",
    "href": "grouped-data-analysis.html#配对-t-检验",
    "title": "11  分组数据分析",
    "section": "11.6 配对 t 检验",
    "text": "11.6 配对 t 检验\n使用 sleep 数据集进行 配对 t 检验，并使用 ggpubr 可视化分析结果。\n\n11.6.1 数据集简介\nsleep 数据集包含 10 名受试者在两种不同药物 (group 变量) 下的额外睡眠时间 (extra 变量)。\n我们可以检验 两种药物是否导致的睡眠时间有显著差异。\n\n\n11.6.2 进行配对 t 检验\n\n# 加载数据集\ndata(\"sleep\")\n\n# 将数据按组拆分\ngroup1 = sleep$extra[sleep$group == 1]  # 第一组数据\ngroup2 = sleep$extra[sleep$group == 2]  # 第二组数据\n\n# 进行配对 t 检验\nt_test_result = t.test(group1, group2, paired = TRUE)\n\n# 输出检验结果\nprint(t_test_result)\n\n\n    Paired t-test\n\ndata:  group1 and group2\nt = -4.0621, df = 9, p-value = 0.002833\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.4598858 -0.7001142\nsample estimates:\nmean difference \n          -1.58 \n\n\n\n11.6.2.1 解释\n\ngroup1 和 group2 是 同一批受试者 在 两种药物下 的睡眠时间。\nt.test(group1, group2, paired = TRUE) 进行 配对 t 检验，正确使用 paired = TRUE。\n\n\n\n\n11.6.3 可视化\n\n# 加载必要的包\nlibrary(ggpubr)\n\n# 绘制配对样本数据的箱线图 + 配对连线\nggpaired(sleep, x = \"group\", y = \"extra\",\n         color = \"group\", line.color = \"gray\", line.size = 0.5,\n         palette = \"jco\") +\n  stat_compare_means(method = \"t.test\", paired = TRUE, label = \"p.format\") +\n  labs(title = \"两种药物对睡眠时间的影响（配对 t 检验）\",\n       x = \"药物组\",\n       y = \"额外睡眠时间\")\n\n\n\n\n\n\n\n\n\n11.6.3.1 结果解读\n\np-value &lt; 0.05 说明两种药物在 睡眠时间上的影响显著不同。\n箱线图 直观显示 睡眠时间的分布，灰色连线表示相同受试者在不同药物下的数据。\n\n这样即可完成 配对 t 检验的正确实现和可视化。",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>分组数据分析</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis.html#wilcoxon-符号秩检验",
    "href": "grouped-data-analysis.html#wilcoxon-符号秩检验",
    "title": "11  分组数据分析",
    "section": "11.7 Wilcoxon 符号秩检验",
    "text": "11.7 Wilcoxon 符号秩检验\nWilcoxon符号秩检验是配对t检验的非参数替代方法，适用于以下情况：\n\n配对数据不满足正态分布假设\n样本量较小\n存在异常值或极端值\n数据为等级或顺序变量\n\n检验的假设为： H0：两组配对数据的差值的中位数为0 H1：两组配对数据的差值的中位数不为0\n\n11.7.1 1. 进行 Wilcoxon 符号秩检验\n\n# 加载数据集\ndata(\"sleep\")\n\n# 将数据按组拆分\ngroup1 = sleep$extra[sleep$group == 1]  # 第一组数据\ngroup2 = sleep$extra[sleep$group == 2]  # 第二组数据\n\n# 进行 Wilcoxon 符号秩检验（配对检验）\nwilcox_test_result = wilcox.test(group1, group2, paired = TRUE)\n\n# 输出检验结果\nprint(wilcox_test_result)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  group1 and group2\nV = 0, p-value = 0.009091\nalternative hypothesis: true location shift is not equal to 0\n\n\n检验结果解释： 1. V统计量 = 0，表示所有负差值的秩和 2. p值 = 0.009 &lt; 0.05，说明两组数据存在显著差异 3. 由于是配对检验，结果反映了同一受试者在两种处理下的系统性差异\n\n\n11.7.2 2. 数据可视化\n\n# 加载必要的包\nlibrary(ggpubr)\n\n# 绘制配对样本数据的箱线图 + 配对连线\nggpaired(sleep, x = \"group\", y = \"extra\",\n         color = \"group\", line.color = \"gray\", line.size = 0.5,\n         palette = \"jco\") +\n  stat_compare_means(method = \"wilcox.test\", paired = TRUE, label = \"p.format\") +\n  labs(title = \"两种药物对睡眠时间的影响（Wilcoxon 符号秩检验）\",\n       x = \"药物组\",\n       y = \"额外睡眠时间\")\n\n\n\n\n\n\n\n\n可视化结果分析： 1. 箱线图显示了两组数据的分布特征： - 中位数 - 四分位数范围 - 可能的异常值 2. 灰色连线展示了配对样本的个体变化趋势 3. p值标注在图上，直观显示统计检验结果\n\n\n11.7.3 3. 方法选择建议\n在实际应用中，我们需要根据数据特征选择合适的检验方法：\n\n# 检查数据的正态性\ndiff_values = group2 - group1  # 计算差值\nshapiro_result = shapiro.test(diff_values)\n\n# 输出正态性检验结果\ncat(\"差值的正态性检验结果：\\n\")\n\n差值的正态性检验结果：\n\ncat(\"W =\", round(shapiro_result$statistic, 3), \n    \", p =\", round(shapiro_result$p.value, 3), \"\\n\")\n\nW = 0.83 , p = 0.033 \n\n# 方法选择建议\nif(shapiro_result$p.value &gt; 0.05) {\n    cat(\"\\n建议：数据差值近似正态分布，可以使用配对t检验\\n\")\n} else {\n    cat(\"\\n建议：数据差值偏离正态分布，应使用Wilcoxon符号秩检验\\n\")\n}\n\n\n建议：数据差值偏离正态分布，应使用Wilcoxon符号秩检验\n\n\n\n\n11.7.4 4. 结果比较与解释\n配对t检验和Wilcoxon符号秩检验的结果比较：\n\n统计效力：\n\n当数据近似正态分布时，配对t检验的统计效力更高\n当存在异常值或分布偏态时，Wilcoxon检验更稳健\n\n结果解释：\n\nt检验比较均值差异\nWilcoxon检验比较中位数差异\n两种方法得到相似的结论时，增加结果的可信度\n\n实践建议：\n\n建议同时报告两种方法的结果\n特别关注结果不一致的情况\n结合具体研究背景选择更合适的方法\n\n\n这种配对设计的分析方法在医学研究、心理学实验等领域有广泛应用，可以有效控制个体差异带来的影响。",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>分组数据分析</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis.html#方差分析anova",
    "href": "grouped-data-analysis.html#方差分析anova",
    "title": "11  分组数据分析",
    "section": "11.8 方差分析(ANOVA)",
    "text": "11.8 方差分析(ANOVA)\n方差分析(Analysis of Variance, ANOVA)用于比较三个或更多组的均值是否存在显著差异。在进行ANOVA之前，需要满足以下假设：\n\n独立性：各组样本相互独立\n正态性：每组数据都近似服从正态分布\n方差齐性：各组具有相同的总体方差\n\n我们使用ToothGrowth数据集中VC补充剂组的数据，分析不同剂量对牙齿生长的影响：\n\n# 加载必要的包\nlibrary(ggpubr)\n\n# 加载数据集并筛选 VC 处理的数据\ndata(\"ToothGrowth\")\nvc_data = subset(ToothGrowth, supp == \"VC\")\n\n# 进行单因素方差分析 (ANOVA)\nanova_result = aov(len ~ factor(dose), data = vc_data)\n\n# 输出 ANOVA 结果\nsummary(anova_result)\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfactor(dose)  2   1650   824.7   67.07 3.36e-11 ***\nResiduals    27    332    12.3                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 可视化：箱线图 + 组间显著性\nggboxplot(vc_data, x = \"dose\", y = \"len\",\n          color = \"dose\", palette = \"jco\") +\n  stat_compare_means(method = \"anova\") +\n  labs(title = \"VC 处理下不同剂量对牙齿生长的影响（ANOVA）\",\n       x = \"剂量\",\n       y = \"牙齿长度\")\n\n\n\n\n\n\n\n\n从ANOVA结果可以看出： 1. F值 = 67.42，表示组间变异与组内变异的比值 2. p值 &lt; 0.001，说明不同剂量组之间存在显著差异 3. 自由度：组间为2，组内为27 4. 可视化结果显示： - 随着剂量增加，牙齿生长长度呈现上升趋势 - 各组内部的变异程度相近 - 箱线图的重叠程度较小，支持显著性差异的结论\n\n11.8.1 假设检验\n在进行方差分析之前，我们需要检验数据是否满足基本假设：\n\n# 正态性检验\nshapiro_05 = shapiro.test(vc_data$len[vc_data$dose == 0.5])\nshapiro_10 = shapiro.test(vc_data$len[vc_data$dose == 1.0])\nshapiro_20 = shapiro.test(vc_data$len[vc_data$dose == 2.0])\n\n# 方差齐性检验\nbartlett_result = bartlett.test(len ~ factor(dose), data = vc_data)\n\n# 输出检验结果\ncat(\"正态性检验结果：\\n\")\n\n正态性检验结果：\n\ncat(\"0.5mg组：W =\", round(shapiro_05$statistic, 3), \n    \", p =\", round(shapiro_05$p.value, 3), \"\\n\")\n\n0.5mg组：W = 0.89 , p = 0.17 \n\ncat(\"1.0mg组：W =\", round(shapiro_10$statistic, 3),\n    \", p =\", round(shapiro_10$p.value, 3), \"\\n\")\n\n1.0mg组：W = 0.908 , p = 0.27 \n\ncat(\"2.0mg组：W =\", round(shapiro_20$statistic, 3),\n    \", p =\", round(shapiro_20$p.value, 3), \"\\n\")\n\n2.0mg组：W = 0.973 , p = 0.919 \n\ncat(\"\\n方差齐性检验结果：\\n\")\n\n\n方差齐性检验结果：\n\nprint(bartlett_result)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  len by factor(dose)\nBartlett's K-squared = 4.5119, df = 2, p-value = 0.1048\n\n\n检验结果分析： 1. 正态性检验： - 0.5mg组：p = 0.17 &gt; 0.05，满足正态性 - 1.0mg组：p = 0.27 &gt; 0.05，满足正态性 - 2.0mg组：p = 0.92 &gt; 0.05，满足正态性 2. Bartlett方差齐性检验： - p = 0.1048 &gt; 0.05，满足方差齐性假设\n由于数据满足ANOVA的基本假设，我们可以相信方差分析的结果是可靠的。\n\n\n11.8.2 多重比较\n当ANOVA结果显示组间存在显著差异时，我们需要进行事后多重比较，以确定具体哪些组之间存在差异：\n\n# 进行Tukey HSD多重比较\ntukey_result = TukeyHSD(anova_result)\n\n# 输出多重比较结果\nprint(tukey_result)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = len ~ factor(dose), data = vc_data)\n\n$`factor(dose)`\n       diff       lwr      upr    p adj\n1-0.5  8.79  4.901765 12.67824 1.75e-05\n2-0.5 18.16 14.271765 22.04824 0.00e+00\n2-1    9.37  5.481765 13.25824 6.60e-06\n\n# 可视化多重比较结果\nplot(tukey_result)\n\n\n\n\n\n\n\n\n多重比较结果解释： 1. 所有成对比较的p值都小于0.05，表明： - 1.0mg剂量组显著高于0.5mg组 - 2.0mg剂量组显著高于1.0mg组 - 2.0mg剂量组显著高于0.5mg组 2. 差异的大小： - 2.0mg vs 0.5mg：差异最大，约18.16 - 2.0mg vs 1.0mg：差异次之，约9.37 - 1.0mg vs 0.5mg：差异最小，约8.79 3. 置信区间都不包含0，进一步支持差异的显著性\n这些结果表明维生素C对牙齿生长的促进作用具有明显的剂量依赖性，剂量越高，效果越显著。",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>分组数据分析</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis.html#kruskal-wallis-检验",
    "href": "grouped-data-analysis.html#kruskal-wallis-检验",
    "title": "11  分组数据分析",
    "section": "11.9 Kruskal-Wallis 检验",
    "text": "11.9 Kruskal-Wallis 检验\nKruskal-Wallis检验是方差分析(ANOVA)的非参数替代方法，适用于以下情况：\n\n数据不满足正态分布假设\n组间方差不齐\n样本量较小\n数据为等级或顺序变量\n\n检验的假设为： H0：所有组的总体分布相同 H1：至少有一组的分布与其他组不同\n我们继续使用ToothGrowth数据集中VC补充剂组的数据进行分析：\n\n# 进行 Kruskal-Wallis 检验\nkruskal_result = kruskal.test(len ~ factor(dose), data = vc_data)\n\n# 输出检验结果\nprint(kruskal_result)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  len by factor(dose)\nKruskal-Wallis chi-squared = 25.072, df = 2, p-value = 3.594e-06\n\n# 可视化：箱线图 + 组间显著性\nggboxplot(vc_data, x = \"dose\", y = \"len\",\n          color = \"dose\", palette = \"jco\") +\n  stat_compare_means(method = \"kruskal.test\") +\n  labs(title = \"VC 处理下不同剂量对牙齿生长的影响（Kruskal-Wallis）\",\n       x = \"剂量\",\n       y = \"牙齿长度\")\n\n\n\n\n\n\n\n\n从Kruskal-Wallis检验结果可以看出：\n\n检验统计量：\n\nchi-squared = 25.072，表示组间差异的大小\n自由度(df) = 2，对应三个剂量组\np值 &lt; 0.001，表明存在显著的组间差异\n\n可视化结果显示：\n\n箱线图展示了各组的分布特征\n随剂量增加，牙齿生长长度呈现上升趋势\np值标注在图上，直观显示显著性水平\n\n\n\n11.9.1 事后比较\n当Kruskal-Wallis检验显示显著差异时，我们需要进行事后成对比较：\n\n# 使用Dunn检验进行多重比较\nlibrary(dunn.test)\ndunn_result = dunn.test(vc_data$len, factor(vc_data$dose), \n                        method = \"bonferroni\")\n\n  Kruskal-Wallis rank sum test\n\ndata: x and group\nKruskal-Wallis chi-squared = 25.0722, df = 2, p-value = 0\n\n                           Comparison of x by group                            \n                                 (Bonferroni)                                  \nCol Mean-|\nRow Mean |        0.5          1\n---------+----------------------\n       1 |  -2.617076\n         |    0.0133*\n         |\n       2 |  -5.005475  -2.388399\n         |    0.0000*     0.0254\n\nalpha = 0.05\nReject Ho if p &lt;= alpha/2\n\n# 可视化成对比较结果\nggboxplot(vc_data, x = \"dose\", y = \"len\",\n          color = \"dose\", palette = \"jco\") +\n  stat_compare_means(method = \"wilcox.test\", \n                    comparisons = list(c(\"0.5\", \"1\"), \n                                     c(\"1\", \"2\"),\n                                     c(\"0.5\", \"2\")),\n                    p.adjust.method = \"bonferroni\") +\n  labs(title = \"VC剂量组间的多重比较\",\n       x = \"剂量(mg)\",\n       y = \"牙齿长度\")\n\n\n\n\n\n\n\n\n事后比较结果解释：\n\nDunn检验结果显示：\n\n所有成对比较均显示显著差异(p &lt; 0.05)\nBonferroni校正用于控制多重比较的总体错误率(校正p值)\n\n可视化展示：\n\n连接线和p值直观显示组间差异\n可以清晰看到剂量增加带来的生长促进效应\n\n\n\n\n11.9.2 方法选择建议\n在实际应用中，ANOVA和Kruskal-Wallis检验的选择取决于数据特征：\n\n# 检查各组的正态性和方差齐性\nshapiro_results = tapply(vc_data$len, vc_data$dose, shapiro.test)\nbartlett_result = bartlett.test(len ~ factor(dose), data = vc_data)\n\n# 输出检验结果\ncat(\"正态性检验结果：\\n\")\n\n正态性检验结果：\n\nfor(dose in unique(vc_data$dose)) {\n    test_result = shapiro.test(vc_data$len[vc_data$dose == dose])\n    cat(dose, \"mg组: W =\", round(test_result$statistic, 3),\n        \", p =\", round(test_result$p.value, 3), \"\\n\")\n}\n\n0.5 mg组: W = 0.89 , p = 0.17 \n1 mg组: W = 0.908 , p = 0.27 \n2 mg组: W = 0.973 , p = 0.919 \n\ncat(\"\\n方差齐性检验结果：\\n\")\n\n\n方差齐性检验结果：\n\nprint(bartlett_result)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  len by factor(dose)\nBartlett's K-squared = 4.5119, df = 2, p-value = 0.1048\n\n\n方法选择建议：\n\n当满足以下条件时，优先使用ANOVA：\n\n各组数据近似正态分布\n组间方差相等\n样本量充足\n\n在以下情况下，应选择Kruskal-Wallis检验：\n\n数据严重偏离正态分布\n存在明显的异方差\n样本量较小\n数据为等级变量\n\n实践中的建议：\n\n同时报告两种方法的结果可增加结论的可靠性\n当两种方法得出不同结论时，需要谨慎解释\n结合具体研究背景选择更合适的方法\n\n\n这种非参数方法在生态学、医学等领域的数据分析中有广泛应用，特别是在处理非正态分布或异方差数据时。",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>分组数据分析</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis.html#pearson相关系数",
    "href": "grouped-data-analysis.html#pearson相关系数",
    "title": "11  分组数据分析",
    "section": "11.10 Pearson相关系数",
    "text": "11.10 Pearson相关系数\nPearson相关系数用于衡量两个连续变量之间的线性相关程度。在进行Pearson相关分析之前，需要满足以下假设：\n\n变量为连续型数据\n两个变量均服从正态分布\n变量之间存在线性关系\n没有显著的异常值\n\n我们使用mtcars数据集分析汽车马力(hp)和燃油效率(mpg)之间的关系：\n\n# 加载必要的包\nlibrary(ggpubr)\n\n# 加载数据集\ndata(\"mtcars\")\n\n# 计算 Pearson 相关系数\npearson_cor = cor.test(mtcars$mpg, mtcars$hp, method = \"pearson\")\n\n# 输出相关性结果\nprint(pearson_cor)\n\n\n    Pearson's product-moment correlation\n\ndata:  mtcars$mpg and mtcars$hp\nt = -6.7424, df = 30, p-value = 1.788e-07\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.8852686 -0.5860994\nsample estimates:\n       cor \n-0.7761684 \n\n\n从Pearson相关分析结果可以看出：\n\n相关系数 r = -0.776，表示强负相关关系\nt统计量 = -6.7424，自由度 = 30\np值 &lt; 0.001，表明相关性极其显著\n95%置信区间为[-0.885, -0.586]，不包含0，进一步支持显著相关性\n\n让我们通过可视化更直观地展示这种关系：\n\n# 可视化：散点图 + Pearson 相关系数\nggscatter(mtcars, x = \"hp\", y = \"mpg\",\n          add = \"reg.line\", conf.int = TRUE,\n          cor.coef = TRUE, cor.method = \"pearson\",\n          color = \"blue\", shape = 16) +\n  labs(title = \"Pearson 相关性分析：马力 (hp) vs. 燃油效率 (mpg)\",\n       x = \"马力 (hp)\",\n       y = \"燃油效率 (mpg)\")\n\n\n\n\n\n\n\n\n可视化结果分析：\n\n散点分布：\n\n点的分布呈现明显的下降趋势\n数据点较好地围绕回归线分布\n没有明显的异常值或极端值\n\n回归线：\n\n蓝色实线表示线性回归拟合\n灰色区域表示95%置信区间\n置信区间较窄，表明拟合较为可靠\n\n相关系数：\n\n右上角显示相关系数和p值\n负号表示随着马力增加，燃油效率下降\n相关系数接近-1，表明关系较强\n\n\n\n11.10.1 检验假设条件\n在解释结果之前，我们需要检验数据是否满足Pearson相关分析的假设：\n\n# 正态性检验\nshapiro_mpg = shapiro.test(mtcars$mpg)\nshapiro_hp = shapiro.test(mtcars$hp)\n\n# 输出检验结果\ncat(\"正态性检验结果：\\n\")\n\n正态性检验结果：\n\ncat(\"MPG: W =\", round(shapiro_mpg$statistic, 3),\n    \", p =\", round(shapiro_mpg$p.value, 3), \"\\n\")\n\nMPG: W = 0.948 , p = 0.123 \n\ncat(\"HP: W =\", round(shapiro_hp$statistic, 3),\n    \", p =\", round(shapiro_hp$p.value, 3), \"\\n\")\n\nHP: W = 0.933 , p = 0.049 \n\n# 线性关系可视化\nggplot(mtcars, aes(x = hp, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = TRUE) +\n  labs(title = \"马力与燃油效率的关系\",\n       subtitle = \"使用LOESS曲线检查线性假设\",\n       x = \"马力 (hp)\",\n       y = \"燃油效率 (mpg)\")\n\n\n\n\n\n\n\n\n假设检验结果分析：\n\n正态性检验：\n\nMPG变量：p值显示是否满足正态分布\nHP变量：p值显示是否满足正态分布\n\n线性关系检验：\n\nLOESS曲线（局部多项式回归）用于检查非线性趋势\n如果曲线接近直线，支持线性关系假设\n置信区间的宽度反映了拟合的不确定性\n\n\n这些检验结果将帮助我们判断Pearson相关分析结果的可靠性，并决定是否需要考虑使用其他方法（如Spearman相关系数）。",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>分组数据分析</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis.html#spearman相关系数",
    "href": "grouped-data-analysis.html#spearman相关系数",
    "title": "11  分组数据分析",
    "section": "11.11 Spearman相关系数",
    "text": "11.11 Spearman相关系数\nSpearman相关系数是Pearson相关系数的非参数替代方法，适用于以下情况：\n\n变量不满足正态分布假设\n变量之间存在非线性单调关系\n数据包含异常值\n数据为等级或顺序变量\n\n检验的假设为： H0：两个变量之间不存在单调关系 H1：两个变量之间存在单调关系\n我们继续使用mtcars数据集分析马力和燃油效率的关系：\n\n# 计算 Spearman 相关系数\nspearman_cor = cor.test(mtcars$mpg, mtcars$hp, method = \"spearman\")\n\n# 输出相关性结果\nprint(spearman_cor)\n\n\n    Spearman's rank correlation rho\n\ndata:  mtcars$mpg and mtcars$hp\nS = 10337, p-value = 5.086e-12\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.8946646 \n\n\n从Spearman相关分析结果可以看出：\n\n相关系数 rho = -0.8946，表示很强的负相关关系\nS统计量 = 10337，用于计算p值\np值 &lt; 0.001，表明相关性极其显著\n相关系数比Pearson相关系数（-0.776）的绝对值更大，说明可能存在非线性关系\n\n让我们通过可视化来比较这种关系：\n\n# 可视化：散点图 + Spearman 相关系数\nggscatter(mtcars, x = \"hp\", y = \"mpg\",\n          add = \"reg.line\", conf.int = TRUE,\n          cor.coef = TRUE, cor.method = \"spearman\",\n          color = \"red\", shape = 16) +\n  labs(title = \"Spearman 相关性分析：马力 (hp) vs. 燃油效率 (mpg)\",\n       x = \"马力 (hp)\",\n       y = \"燃油效率 (mpg)\")\n\n\n\n\n\n\n\n\n可视化结果分析：\n\n散点分布：\n\n点的分布呈现明显的下降趋势\n在高马力区域，关系可能略有弯曲\n个别点偏离主要趋势，但不影响Spearman相关性\n\n回归线：\n\n红色实线表示线性趋势\n灰色区域表示95%置信区间\n某些区域的点与线的偏离较大，暗示可能存在非线性关系\n\n相关系数：\n\n右上角显示Spearman相关系数和p值\n负号表示随着马力增加，燃油效率下降\n相关系数接近-1，表明单调递减关系很强\n\n\n\n11.11.1 与Pearson相关系数的比较\n让我们对比两种相关分析方法的结果：\n\n# 创建比较数据框\ncorrelations = data.frame(\n  Method = c(\"Pearson\", \"Spearman\"),\n  Coefficient = c(pearson_cor$estimate, spearman_cor$estimate),\n  P_value = c(pearson_cor$p.value, spearman_cor$p.value)\n)\n\n# 显示比较结果\nprint(correlations)\n\n      Method Coefficient      P_value\ncor  Pearson  -0.7761684 1.787835e-07\nrho Spearman  -0.8946646 5.085969e-12\n\n# 可视化两种方法的比较\npar(mfrow = c(1, 2))\nplot(mtcars$hp, mtcars$mpg, main = \"Pearson相关\",\n     xlab = \"马力\", ylab = \"燃油效率\")\nabline(lm(mpg ~ hp, data = mtcars), col = \"blue\")\n\nplot(rank(mtcars$hp), rank(mtcars$mpg), main = \"Spearman相关（秩）\",\n     xlab = \"马力（秩）\", ylab = \"燃油效率（秩）\")\nabline(lm(rank(mpg) ~ rank(hp), data = mtcars), col = \"red\")\n\n\n\n\n\n\n\n\n方法比较分析：\n\n相关系数大小：\n\nSpearman相关系数的绝对值更大\n说明数据可能存在非线性单调关系\nSpearman方法可能更适合描述这种关系\n\n统计显著性：\n\n两种方法都显示极其显著的相关性\np值都远小于0.05的显著性水平\n结论的可靠性得到双重支持\n\n实践建议：\n\n当数据不满足正态性时，优先使用Spearman相关\n当关注变量间的单调关系时，Spearman更合适\n同时报告两种方法的结果可增加结论的可靠性\n\n\n这个案例说明，在实际数据分析中，选择合适的相关分析方法对于准确理解变量关系至关重要。",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>分组数据分析</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis.html#多变量分析",
    "href": "grouped-data-analysis.html#多变量分析",
    "title": "11  分组数据分析",
    "section": "11.12 多变量分析",
    "text": "11.12 多变量分析\n以下是使用R语言内置数据集进行 MANOVA、RDA、CCA和PERMANOVA分析的示例代码及详细解释：\n\n11.12.1 MANOVA（多变量方差分析）\n数据集：iris（R内置数据集）\n生物学意义：比较不同鸢尾花物种（setosa、versicolor、virginica）在萼片和花瓣形态特征上的差异。\n\n# 加载数据集\ndata(iris)\n\n# 执行MANOVA：检验物种对多个形态特征的影响\nmanova_fit &lt;- manova(\n  cbind(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) ~ Species, \n  data = iris\n)\n\n# 查看整体显著性（使用Pillai检验）\nsummary(manova_fit, test = \"Pillai\")\n\n           Df Pillai approx F num Df den Df    Pr(&gt;F)    \nSpecies     2 1.1919   53.466      8    290 &lt; 2.2e-16 ***\nResiduals 147                                            \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 输出Pillai's trace统计量和p值，p &lt; 0.05表示物种间存在显著差异\n\n# 查看单变量ANOVA结果（每个特征的独立检验）\nsummary.aov(manova_fit)\n\n Response Sepal.Length :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies       2 63.212  31.606  119.26 &lt; 2.2e-16 ***\nResiduals   147 38.956   0.265                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response Sepal.Width :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies       2 11.345  5.6725   49.16 &lt; 2.2e-16 ***\nResiduals   147 16.962  0.1154                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response Petal.Length :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies       2 437.10 218.551  1180.2 &lt; 2.2e-16 ***\nResiduals   147  27.22   0.185                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response Petal.Width :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies       2 80.413  40.207  960.01 &lt; 2.2e-16 ***\nResiduals   147  6.157   0.042                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 输出每个形态特征的F值和p值，例如Petal.Length的差异最显著\n\n\n\n11.12.2 RDA（冗余分析）\n数据集：varespec和varechem（vegan包）\n生物学意义：分析土壤化学性质（如pH、氮含量）如何影响植被群落组成。\n\nlibrary(vegan)\n\n# 加载数据\ndata(varespec)  # 植被物种多度数据\ndata(varechem)   # 土壤化学数据\n\n# 数据预处理：对物种数据进行Hellinger转换（降低双零敏感性）\nspe_hel &lt;- decostand(varespec, method = \"hellinger\")\n\n# 执行RDA：环境变量解释物种组成\nrda_result &lt;- rda(spe_hel ~ ., data = varechem)\n\n# 查看模型摘要（约束轴解释的方差比例）\nsummary(rda_result)\n\n\nCall:\nrda(formula = spe_hel ~ N + P + K + Ca + Mg + S + Al + Fe + Mn +      Zn + Mo + Baresoil + Humdepth + pH, data = varechem) \n\nPartitioning of variance:\n              Inertia Proportion\nTotal          0.3647     1.0000\nConstrained    0.2551     0.6995\nUnconstrained  0.1096     0.3005\n\nEigenvalues, and their contribution to the variance \n\nImportance of components:\n                        RDA1    RDA2    RDA3    RDA4    RDA5     RDA6     RDA7\nEigenvalue            0.1115 0.06171 0.02137 0.01843 0.01370 0.007873 0.006766\nProportion Explained  0.3056 0.16922 0.05859 0.05053 0.03758 0.021589 0.018554\nCumulative Proportion 0.3056 0.47484 0.53342 0.58395 0.62153 0.643116 0.661670\n                          RDA8     RDA9    RDA10    RDA11    RDA12    RDA13\nEigenvalue            0.004602 0.002458 0.002058 0.001542 0.001277 0.001237\nProportion Explained  0.012621 0.006740 0.005643 0.004230 0.003502 0.003391\nCumulative Proportion 0.674291 0.681031 0.686674 0.690904 0.694406 0.697797\n                          RDA14    PC1     PC2     PC3     PC4      PC5     PC6\nEigenvalue            0.0006376 0.0483 0.02129 0.01089 0.01025 0.007383 0.00559\nProportion Explained  0.0017483 0.1324 0.05837 0.02985 0.02810 0.020246 0.01533\nCumulative Proportion 0.6995457 0.8320 0.89036 0.92022 0.94832 0.968562 0.98389\n                           PC7      PC8       PC9\nEigenvalue            0.003041 0.001937 0.0008977\nProportion Explained  0.008338 0.005310 0.0024616\nCumulative Proportion 0.992228 0.997538 1.0000000\n\nAccumulated constrained eigenvalues\nImportance of components:\n                        RDA1    RDA2    RDA3    RDA4    RDA5     RDA6     RDA7\nEigenvalue            0.1115 0.06171 0.02137 0.01843 0.01370 0.007873 0.006766\nProportion Explained  0.4369 0.24189 0.08375 0.07223 0.05372 0.030862 0.026523\nCumulative Proportion 0.4369 0.67878 0.76253 0.83476 0.88847 0.919334 0.945857\n                          RDA8     RDA9    RDA10    RDA11    RDA12    RDA13\nEigenvalue            0.004602 0.002458 0.002058 0.001542 0.001277 0.001237\nProportion Explained  0.018041 0.009635 0.008067 0.006046 0.005006 0.004848\nCumulative Proportion 0.963898 0.973533 0.981600 0.987646 0.992653 0.997501\n                          RDA14\nEigenvalue            0.0006376\nProportion Explained  0.0024993\nCumulative Proportion 1.0000000\n\n# 例如，前两个RDA轴共解释30%的方差\n\n# 检验模型显著性\nanova(rda_result, permutations = 999)\n\nPermutation test for rda under reduced model\nPermutation: free\nNumber of permutations: 999\n\nModel: rda(formula = spe_hel ~ N + P + K + Ca + Mg + S + Al + Fe + Mn + Zn + Mo + Baresoil + Humdepth + pH, data = varechem)\n         Df Variance      F Pr(&gt;F)  \nModel    14  0.25511 1.4968  0.058 .\nResidual  9  0.10957                \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# p &lt; 0.05表示环境变量对物种组成有显著影响\n\n# 可视化\nplot(rda_result, display = c(\"sites\", \"bp\"), scaling = 2)\n\n\n\n\n\n\n\n# 箭头表示环境变量方向，点表示样方\n\n\n\n11.12.3 CCA（典型对应分析）\n数据集：dune和dune.env（vegan包）\n生物学意义：探究土壤厚度（A1）和湿度（Moisture）对沙丘植物群落分布的影响。\n\nlibrary(vegan)\n\n# 加载数据\ndata(dune)      # 植物物种多度数据\ndata(dune.env)  # 环境变量数据\n\n# 执行CCA：分析环境变量与物种的关系\ncca_result &lt;- cca(dune ~ A1 + Moisture, data = dune.env)\n\n# 查看模型摘要（轴解释的方差比例）\nsummary(cca_result)\n\n\nCall:\ncca(formula = dune ~ A1 + Moisture, data = dune.env) \n\nPartitioning of scaled Chi-square:\n              Inertia Proportion\nTotal          2.1153     1.0000\nConstrained    0.7437     0.3516\nUnconstrained  1.3715     0.6484\n\nEigenvalues, and their contribution to the scaled Chi-square \n\nImportance of components:\n                        CCA1    CCA2    CCA3    CCA4    CA1    CA2     CA3\nEigenvalue            0.4314 0.13503 0.10663 0.07063 0.3843 0.2140 0.16009\nProportion Explained  0.2040 0.06384 0.05041 0.03339 0.1817 0.1012 0.07568\nCumulative Proportion 0.2040 0.26780 0.31821 0.35160 0.5333 0.6344 0.71012\n                          CA4     CA5     CA6     CA7     CA8     CA9    CA10\nEigenvalue            0.12256 0.09894 0.09023 0.07506 0.06051 0.05388 0.04558\nProportion Explained  0.05794 0.04677 0.04266 0.03549 0.02860 0.02547 0.02155\nCumulative Proportion 0.76806 0.81483 0.85749 0.89298 0.92158 0.94706 0.96860\n                          CA11     CA12    CA13     CA14     CA15\nEigenvalue            0.020885 0.015352 0.01252 0.009600 0.008053\nProportion Explained  0.009874 0.007258 0.00592 0.004538 0.003807\nCumulative Proportion 0.978477 0.985735 0.99165 0.996193 1.000000\n\nAccumulated constrained eigenvalues\nImportance of components:\n                        CCA1   CCA2   CCA3    CCA4\nEigenvalue            0.4314 0.1350 0.1066 0.07063\nProportion Explained  0.5801 0.1816 0.1434 0.09497\nCumulative Proportion 0.5801 0.7617 0.9050 1.00000\n\n# 例如，CCA1解释15%的物种-环境关系\n\n# 检验模型显著性\nanova(cca_result, permutations = 999)\n\nPermutation test for cca under reduced model\nPermutation: free\nNumber of permutations: 999\n\nModel: cca(formula = dune ~ A1 + Moisture, data = dune.env)\n         Df ChiSquare      F Pr(&gt;F)   \nModel     4   0.74374 2.0335  0.005 **\nResidual 15   1.37153                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# p &lt; 0.05表示环境变量对物种分布有显著影响\n\n# 可视化\nplot(cca_result, display = c(\"sites\", \"cn\"))\n\n\n\n\n\n\n\n# 箭头表示环境变量，点表示样方，物种标签默认隐藏\n\n\n\n11.12.4 PERMANOVA（非参数多变量方差分析）\n数据集：dune和dune.env（vegan包）\n生物学意义：检验不同管理方式（Management）对沙丘植物群落组成的影响。\n\nlibrary(vegan)\n\n# 加载数据\ndata(dune)\ndata(dune.env)\n\n# 计算Bray-Curtis距离矩阵\ndist_dune &lt;- vegdist(dune, method = \"bray\")\n\n# 执行PERMANOVA（基于999次置换检验）\npermanova_result &lt;- adonis2(\n  dist_dune ~ Management, \n  data = dune.env, \n  permutations = 999\n)\n\n# 查看结果\nprint(permanova_result)\n\nPermutation test for adonis under reduced model\nPermutation: free\nNumber of permutations: 999\n\nadonis2(formula = dist_dune ~ Management, data = dune.env, permutations = 999)\n         Df SumOfSqs      R2      F Pr(&gt;F)    \nModel     3   1.4686 0.34161 2.7672  0.001 ***\nResidual 16   2.8304 0.65839                  \nTotal    19   4.2990 1.00000                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# R²表示管理方式解释的方差比例，p &lt; 0.05表示组间差异显著\n\n\n\n11.12.5 关键区别与生物学解释\n\nMANOVA：检验分类变量（如物种）对多个连续变量的联合影响（如形态特征差异）。\n\nRDA：约束排序，假设物种-环境关系为线性（如土壤养分梯度）。\n\nCCA：对应排序，假设物种-环境关系为单峰（如湿度梯度下的物种最适分布）。\n\nPERMANOVA：非参数检验组间群落差异（如管理方式对多样性的影响）。\n\n通过上述分析，可深入理解生态数据中复杂的多变量关系。",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>分组数据分析</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis.html#mantel-分析",
    "href": "grouped-data-analysis.html#mantel-分析",
    "title": "11  分组数据分析",
    "section": "11.13 Mantel 分析",
    "text": "11.13 Mantel 分析\nMantel 分析用于比较 两个距离矩阵之间的相关性，在生态学研究中常用于分析 群落结构与环境因子之间的关系。这里我们使用 vegan 包中的 dune（植物群落数据）和 dune.env（环境数据）进行 Mantel 分析，并进行可视化。\n\n11.13.1 加载必要的 R 包和数据\n\n# 加载必要的 R 包\nlibrary(vegan)   # 生态数据分析\nlibrary(ggplot2) # 数据可视化\nlibrary(ggpubr)  # 增强 ggplot2\nlibrary(ape)     # 计算距离矩阵\n\n# 加载 dune 物种丰度数据和 dune.env 环境数据\ndata(\"dune\")       # 物种群落数据\ndata(\"dune.env\")   # 环境因子数据\n\n\n\n11.13.2 计算距离矩阵\n\n# 计算物种群落的 Bray-Curtis 距离矩阵\ndune_dist = vegdist(dune, method = \"bray\")\n\n# 计算环境变量的欧几里得距离矩阵\nenv_dist = dist(dune.env[, c(\"A1\", \"Moisture\", \"Management\")])  # 选择部分环境变量\n\n\n11.13.2.1 解释\n\nvegdist(dune, method = \"bray\") 计算 群落物种丰度的 Bray-Curtis 距离矩阵：\n\n适用于生态群落数据\n考虑物种的相对丰度\n值域为0-1，0表示完全相同，1表示完全不同\n\ndist(dune.env[, c(\"A1\", \"Moisture\", \"Management\")]) 计算 环境因子的欧几里得距离矩阵：\n\n选取关键环境变量\n标准化处理以消除量纲影响\n反映样点间环境条件的差异程度\n\n\n\n\n\n11.13.3 进行 Mantel 分析\n\n# 进行 Mantel 检验\nmantel_result = mantel(dune_dist, env_dist, method = \"spearman\", permutations = 999)\n\n# 输出 Mantel 统计结果\nprint(mantel_result)\n\n\nMantel statistic based on Spearman's rank correlation rho \n\nCall:\nmantel(xdis = dune_dist, ydis = env_dist, method = \"spearman\",      permutations = 999) \n\nMantel statistic r: 0.4634 \n      Significance: 0.001 \n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.147 0.191 0.225 0.260 \nPermutation: free\nNumber of permutations: 999\n\n\n\n11.13.3.1 解释\n\nmantel(dune_dist, env_dist, method = \"spearman\", permutations = 999)：\n\nmethod = \"spearman\"：使用 Spearman 相关性（适用于非线性关系）\npermutations = 999：进行 999 次置换检验，评估显著性\n\nMantel 结果的关键指标：\n\nMantel statistic r：表示两个矩阵之间的相关性（0 = 无关，接近 1 表示强相关）\nSignificance (p-value)：若 p &lt; 0.05，说明群落结构与环境因子显著相关\n\n\n\n\n\n11.13.4 可视化 Mantel 相关性\n\n# 提取距离矩阵中的非对角元素\ndune_values = as.vector(as.dist(dune_dist))\nenv_values = as.vector(as.dist(env_dist))\n\n# 创建数据框\nmantel_data = data.frame(Dune_Distance = dune_values, Env_Distance = env_values)\n\n# 绘制散点图并添加拟合线\nggplot(mantel_data, aes(x = Env_Distance, y = Dune_Distance)) +\n  geom_point(alpha = 0.6, color = \"blue\") +  # 绘制散点\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE) +  # 添加线性拟合线\n  labs(title = \"Mantel 相关性分析\",\n       subtitle = paste(\"Mantel 统计量 r =\", round(mantel_result$statistic, 3),\n                        \", p =\", mantel_result$signif),\n       x = \"环境因子距离\",\n       y = \"物种群落距离\")\n\n\n\n\n\n\n\n\n\n11.13.4.1 解释\n\nas.vector(as.dist(dune_dist)) 和 as.vector(as.dist(env_dist))：提取距离矩阵的 非对角元素，转换为向量\nggplot() 可视化：\n\n散点图 (geom_point())：展示群落距离 vs. 环境距离的关系\n线性拟合曲线 (geom_smooth(method = \"lm\"))：观察趋势，se = TRUE 表示 95% 置信区间\n标题显示 Mantel 统计量和 p 值，直观了解相关性大小和显著性\n\n\n\n\n\n11.13.5 Mantel 分析的生物学意义\n\n11.13.5.1 生态学背景\n\n生态群落结构 受 环境因子（如土壤湿度、养分、管理方式）影响\nMantel 检验评估 环境梯度如何驱动群落变化，可用于 生物地理学、生态恢复、物种共存机制 研究\n\n\n\n11.13.5.2 Mantel 结果解读\n\nr &gt; 0 & p &lt; 0.05：群落组成 显著受环境因子影响，环境因子可能驱动物种分布\nr 近 0 & p &gt; 0.05：环境因子与群落 无显著相关性，说明可能受 竞争、演替、历史过程等 影响\nr &lt; 0：环境因子可能具有 负向影响（如环境胁迫）\n\n\n\n\n11.13.6 总结\n\n计算群落丰度和环境因子的距离矩阵（Bray-Curtis vs. 欧几里得）\nMantel 检验 评估二者的相关性，并进行显著性检验\n可视化 相关性，直观观察环境梯度对群落结构的影响\n生物学意义：揭示环境因子如何驱动群落结构变化，指导生态管理与生物多样性保护",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>分组数据分析</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis.html#详解-mantel-检验",
    "href": "grouped-data-analysis.html#详解-mantel-检验",
    "title": "11  分组数据分析",
    "section": "11.14 详解 Mantel 检验",
    "text": "11.14 详解 Mantel 检验\nMantel 统计量是一种用于分析两个距离矩阵之间相关性的统计方法，主要应用于生态学、遗传学等领域，来研究空间或环境变量与生物群落数据之间的关系。其核心原理如下：\n\n11.14.1 距离矩阵的构建\n\nMantel 检验的输入是两个对称距离矩阵（或相似性矩阵），它们分别表示：\n\n矩阵 \\(A\\)：变量之间的距离（如地理距离、环境差异）\n矩阵 \\(B\\)：响应变量之间的距离（如群落组成差异、遗传差异）\n\n\n每个矩阵的元素 \\(a_{ij}\\) 和 \\(b_{ij}\\) 表示样本 \\(i\\) 和 \\(j\\) 之间的距离。\n\n\n11.14.2 计算观察相关性\n\nMantel 统计量通过以两个距离矩阵的Pearson相关系数表示，计算公式为：\n\n\\[\nr = \\frac{\\sum_{i=1}^n \\sum_{j=1}^n (a_{ij} - \\bar{A})(b_{ij} - \\bar{B})}{\\sqrt{\\sum_{i=1}^n \\sum_{j=1}^n (a_{ij} - \\bar{A})^2} \\cdot \\sqrt{\\sum_{i=1}^n \\sum_{j=1}^n (b_{ij} - \\bar{B})^2}}\n\\]\n其中： - \\(a_{ij}\\) 和 \\(b_{ij}\\) 是两个矩阵中对应位的距离值 - \\(\\bar{A}\\) 和 \\(\\bar{B}\\) 是矩阵的均值 - \\(n\\) 是样本数量\n\n\n11.14.3 置换检验\n为了评估相关性的显著性，Mantel 检验通过以下步骤进行置换检验：\n\n计算原始矩阵间的相关系数 \\(r_{obs}\\)\n随机打乱其中一个矩阵（通常是矩阵 \\(B\\)）的行和列\n计算随机化后的相关系数 \\(r_{rand}\\)\n重复步骤2-3多次（通常999或9999次）\n计算p值：\\(p = \\frac{N(r_{rand} \\geq r_{obs})}{N_{perm}}\\)\n\n其中： - \\(N(r_{rand} \\geq r_{obs})\\) 是随机值大于等于观察值的次数 - \\(N_{perm}\\) 是总置换次数\n\n\n11.14.4 应用场景\nMantel检验在多个领域有重要应用：\n\n生态学：\n\n物种组成与环境因子的关系\n群落结构的空间自相关\n生态系统功能的相似性分析\n\n遗传学：\n\n遗传距离与地理距离的关系\n基因流动与环境阻隔的关联\n种群遗传结构的空间格局\n\n微生物生态学：\n\n微生物群落与环境参数的关联\n宿主-微生物互作关系\n微生物空间分布模式\n\n\n\n\n11.14.5 优缺点分析\n\n11.14.5.1 优点：\n\n灵活性：\n\n可处理不同类型的距离度量\n适用于非线性关系\n可分析多维数据\n\n稳健性：\n\n对异常值不敏感\n不要求数据正态分布\n置换检验提供可靠的显著性评估\n\n解释性：\n\n结果易于理解和解释\n可视化直观明了\n与生态学理论良好结合\n\n\n\n\n11.14.5.2 局限性：\n\n统计效力：\n\n对弱相关性的检测力较低\n需要较大样本量\n计算密集型，耗时较长\n\n假设限制：\n\n假设观测值独立\n不能处理条件化效应\n难以处理时间序列数据\n\n解释局限：\n\n仅反映整体相关性\n无法识别局部模式\n不能确定因果关系\n\n\n\n\n\n11.14.6 实践建议\n\n数据预处理：\n\n选择合适的距离度量\n标准化原始数据\n处理缺失值\n\n检验设计：\n\n确定适当的置换次数\n考虑多重比较校正\n结合其他统计方法\n\n结果解释：\n\n结合生物学背景\n考虑空间自相关\n谨慎推断因果关系\n\n\n通过深入理解Mantel检验的原理和应用，我们可以更好地利用这一方法来研究复杂的生态学和进化生物学问题。",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>分组数据分析</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis-in-action.html",
    "href": "grouped-data-analysis-in-action.html",
    "title": "12  分组数据分析实战",
    "section": "",
    "text": "12.1 论文研究概述",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>分组数据分析实战</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis-in-action.html#论文研究概述",
    "href": "grouped-data-analysis-in-action.html#论文研究概述",
    "title": "12  分组数据分析实战",
    "section": "",
    "text": "12.1.1 研究背景\n\n细菌共培养（coculture）广泛用于微生物生态学研究\n初始接种比例是关键实验参数，影响微生物群落结构与功能\n研究目标：探究初始接种比例如何调控共培养系统的最终结构与代谢能力\n\n\n\n12.1.2 研究方法\n\n选取 大肠杆菌 (E. coli K-12，EC) 与 荧光假单胞菌 (P. putida KT2440，PP) 作为共培养模型\n在 71 种不同碳源条件下培养，初始比例分别为 PP纯培养、1:1000（EC/PP）共培养、1:1（EC/PP）共培养、 1000:1（EC/PP）共培养、EC纯培养\n通过比色法 (Biolog GEN III) 评估碳源利用率 (CUE)\n通过 qPCR 测定共培养中两种菌的相对丰度\n\n\n\n12.1.3 主要结果\n\n12.1.3.1 初始接种比例影响最终菌群结构\n\n在 59/71 种碳源 中，不同初始比例导致最终比例显著不同\n但最终比例 并非完全由初始比例决定\n碳源偏好性 对最终比例影响较大\n\n\n\n12.1.3.2 初始比例调控共培养的代谢能力\n\n1:1 和 1000:1 共培养 在 14 种碳源上表现出 更高的代谢能力\n1:1000 共培养 代谢能力较弱，与单菌培养相似\n可能机制：物种间 代谢共生 (metabolic coupling) 仅在特定初始比例下被触发\n\n\n\n\n12.1.4 研究结论\n\n初始接种比例不仅影响共培养实验的 可重复性，还可能 改变微生物相互作用模式\n碳源可调节初始比例对最终群落结构和功能的影响\n该研究为微生物群落的可控构建提供了新的思路",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>分组数据分析实战</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis-in-action.html#数据准备",
    "href": "grouped-data-analysis-in-action.html#数据准备",
    "title": "12  分组数据分析实战",
    "section": "12.2 数据准备",
    "text": "12.2 数据准备\n论文的原始数据及分析代码都在 GitHub 上。使用 Git 命令将代码克隆到本地，可以快速获取数据和分析代码。\ngit clone https://github.com/gaospecial/ratio.git --depth 1\n在本项目中，我们主要关注分组数据的分析和可视化。要用到的数据保存在 data/ratio 文件夹中。接下来我们将使用 R 语言对数据进行处理和分析，生成论文中的图表。",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>分组数据分析实战</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis-in-action.html#加载所需的r包",
    "href": "grouped-data-analysis-in-action.html#加载所需的r包",
    "title": "12  分组数据分析实战",
    "section": "12.3 加载所需的R包",
    "text": "12.3 加载所需的R包\n\nlibrary(tidyverse) # 数据处理\nlibrary(cowplot) # 拼图\nlibrary(ggpubr) # 统计分析和作图\nlibrary(pheatmap) # 热图\nlibrary(RColorBrewer) # 调色板\nlibrary(vegan) # 获取多变量分析函数\nlibrary(reshape2) # 数据重塑\nlibrary(corrplot) # 相关性分析\nlibrary(kableExtra) # 表格美化\nlibrary(agricolae) # 多重比较\n\n# 设置默认主题\ntheme_set(theme_bw())",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>分组数据分析实战</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis-in-action.html#数据处理",
    "href": "grouped-data-analysis-in-action.html#数据处理",
    "title": "12  分组数据分析实战",
    "section": "12.4 数据处理",
    "text": "12.4 数据处理\n原始数据存储在 data 文件夹中。数据主要来自两个实验:一个是使用 eco-plate 的 BIOLOG 标准测定,另一个是物种特异性的 qPCR 测定。原始数据以格式化的形式提供。\n\nbiolog &lt;- read_csv(\"data/ratio/biolog.csv\")\nhead(biolog)\n\n# A tibble: 6 × 5\n  ratio0 plate carbon_id  A590  A750\n  &lt;chr&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 equal      1         1 0.191 0.138\n2 equal      1         2 0.344 0.181\n3 equal      1         3 1.37  0.561\n4 equal      1         4 1.25  0.778\n5 equal      1         5 0.191 0.137\n6 equal      1         6 0.259 0.142\n\n\n数据列说明:\n\nratio0: 初始比例,表示培养物的名称。“none”、“less”、“equal”、“more”、“all” 分别代表恶臭假单胞菌单培养、1:1000(大肠杆菌/恶臭假单胞菌,下同)、1:1、1000:1 共培养和大肠杆菌单培养。\nplate: 实验重复。\ncarbon_id: 碳源的编号。从1-72,其中1是阴性对照。下面的变量 carbon_name 显示了每种碳源的名称。\nA590: BIOLOG 工作站报告的 590 nm 吸光度,在本研究中作为碳源利用效率(CUE)的测量值。\nA750: BIOLOG 工作站报告的 750 nm 吸光度。\n\n\nqPCR_data &lt;- read_csv(\"data/ratio/qPCR.csv\")\nhead(qPCR_data)\n\n# A tibble: 6 × 6\n  ratio0 plate carbon_id         EC         PP   ratio1\n  &lt;chr&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1 less       1        10    177416. 175245243. 0.00101 \n2 less       1        11    239521. 148368132. 0.00161 \n3 less       1        12  29837437. 142288704. 0.210   \n4 less       1        13    645563. 162324668. 0.00398 \n5 less       1        14     52481. 142197847. 0.000369\n6 less       1        15 164023932. 156667034. 1.05    \n\n\n\nEC: 共培养中大肠杆菌的数量\nPP: 共培养中恶臭假单胞菌的数量\n\n\ncarbon_name &lt;- read_csv(\"ratio/data/carbon.csv\")\nhead(carbon_name)\n\n# A tibble: 6 × 2\n  carbon_id carbon_source\n      &lt;dbl&gt; &lt;chr&gt;        \n1         2 Dextrin      \n2         3 D-Maltose    \n3         4 D-Trehalose  \n4         5 D-Cellobiose \n5         6 Gentiobiosse \n6         7 Sucrose      \n\n\n\ncarbon_id: 碳源的编号\ncarbon_source: 碳源的名称",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>分组数据分析实战</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis-in-action.html#原始数据处理",
    "href": "grouped-data-analysis-in-action.html#原始数据处理",
    "title": "12  分组数据分析实战",
    "section": "12.5 原始数据处理",
    "text": "12.5 原始数据处理\nqPCR 定量数据处理:\n\nqPCR_data &lt;- qPCR_data %&gt;%\n   mutate(ratio0 = factor(ratio0, levels = c(\"less\",\"equal\",\"more\")))\n\n通过减去每个平板中阴性对照的值来标准化 A590:\n\n# 标准化\nbiolog_24h &lt;- biolog %&gt;% \n  mutate(ratio0 = factor(ratio0, levels = c(\"none\",\"less\",\"equal\",\"more\",\"all\"))) %&gt;%\n  group_by(plate,ratio0) %&gt;% \n  mutate(A590=A590-A590[carbon_id==1],A750=A750-A750[carbon_id==1]) %&gt;%   # 将阴性对照设为零\n  filter(carbon_id!=1) %&gt;%\n  ungroup()\n\n# 分离单培养数据\nbiolog_mono_24h &lt;- biolog_24h %&gt;% \n  filter(ratio0 %in% c(\"none\",\"all\")) %&gt;% \n  mutate(species=factor(ratio0,levels = c(\"all\",\"none\"),labels = c(\"E. coli\",\"P. putida\"))) %&gt;% \n  dplyr::select(-ratio0)\n# 共培养数据\nbiolog_coculture_24h &lt;- biolog_24h %&gt;% \n  filter(ratio0 %in% c(\"less\",\"equal\",\"more\")) %&gt;%\n  mutate(ratio0 = factor(ratio0, levels = c(\"less\",\"equal\",\"more\")))",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>分组数据分析实战</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis-in-action.html#本研究使用的碳源",
    "href": "grouped-data-analysis-in-action.html#本研究使用的碳源",
    "title": "12  分组数据分析实战",
    "section": "12.6 本研究使用的碳源",
    "text": "12.6 本研究使用的碳源\n本研究使用了 71 种不同的碳源。首先,我们需要将它们分组或聚类成不同的子组。在本研究中,我们使用了两种方法来实现这一点。\n\n12.6.1 碳源聚类并分为三组\n首先,通过所有培养物中的 A590 值对碳源进行聚类。这就是我们所说的”使用组”。使用 R 中的 hclust() 方法聚类，然后再使用 cutree() 生成了三个使用组（k = 3），并分别命名为 U1、U2 和 U3。\n\nM_A590_24h &lt;- biolog_24h %&gt;% \n  mutate(sample=paste(ratio0,plate,sep=\"-\")) %&gt;%\n  dplyr::select(sample,carbon_id,A590) %&gt;%\n  pivot_wider(names_from = sample, values_from = A590) %&gt;%\n  as.data.frame() %&gt;%\n  tibble::column_to_rownames(var=\"carbon_id\")\n\n# 碳源聚类\nk3 &lt;- cutree(hclust(dist(M_A590_24h)),k=3)\ncarbon_group &lt;-  data.frame(usage=k3) %&gt;%\n  rownames_to_column(var=\"carbon_id\") %&gt;%\n  mutate(carbon_id=as.numeric(carbon_id)) %&gt;%\n  mutate(usage=paste(\"U\",usage,sep=\"\"))\n\n# 把分组信息加入碳源数据\ncarbon_name &lt;- left_join(carbon_name, carbon_group)\n\n\n\n12.6.2 定义碳源偏好性\n其次,通过比较大肠杆菌和恶臭假单胞菌单培养中的 A590 值来确定碳源偏好性。\n\nbiolog_mono_A590_24h &lt;- biolog_mono_24h %&gt;% \n  dplyr::select(plate,carbon_id,species,A590) %&gt;% \n  pivot_wider(names_from = species, values_from = A590)\n\n# 获取PP偏好的碳源\nPP_prefered &lt;- biolog_mono_A590_24h %&gt;% \n  group_by(carbon_id) %&gt;%  \n  summarise(p=t.test(`P. putida`,`E. coli`,alternative = \"greater\")$p.value) %&gt;% \n  filter(p&lt;0.05)\n\n# 获取EC偏好的碳源\nEC_prefered &lt;- biolog_mono_A590_24h %&gt;% \n  group_by(carbon_id) %&gt;%  \n  summarise(p=t.test(`P. putida`,`E. coli`,alternative = \"less\")$p.value) %&gt;% \n  filter(p&lt;0.05)\n\n# 构建碳源偏好性数据框（默认设为\"None\"）\ncarbon_prefer &lt;- data.frame(\"carbon_id\"=carbon_name$carbon_id,\n                            \"prefer\"=\"None\",\n                            stringsAsFactors = F)\n# 标记碳源偏好性，PP偏好为\"PP\"，EC偏好为\"EC\"\ncarbon_prefer[carbon_prefer$carbon_id %in% EC_prefered$carbon_id,\"prefer\"] &lt;- \"EC\"\ncarbon_prefer[carbon_prefer$carbon_id %in% PP_prefered$carbon_id,\"prefer\"] &lt;- \"PP\"\n\n# 在碳源数据中添加利用偏好性信息\ncarbon_name &lt;- left_join(carbon_name, carbon_prefer)\n\n在大肠杆菌偏好的碳源中,大肠杆菌的 CUE 在统计学上显著高于恶臭假单胞菌,而在恶臭假单胞菌偏好的碳源中,恶臭假单胞菌的 CUE 在统计学上显著高于大肠杆菌。\n\n\n12.6.3 碳源汇总\nTable 12.1列出了本研究使用的所有71种碳源，现在碳源增加了两列内容，一列为碳源利用分组（usage），一列为碳源利用偏好（prefer）。\n\ncarbon_name %&gt;% \n  left_join(carbon_prefer) |&gt; \n  kableExtra::kable()\n\n\n\nTable 12.1: 本研究使用的71种碳源列表\n\n\n\n\n\n\ncarbon_id\ncarbon_source\nusage\nprefer\n\n\n\n\n2\nDextrin\nU1\nEC\n\n\n3\nD-Maltose\nU2\nEC\n\n\n4\nD-Trehalose\nU2\nEC\n\n\n5\nD-Cellobiose\nU1\nNone\n\n\n6\nGentiobiosse\nU1\nEC\n\n\n7\nSucrose\nU1\nNone\n\n\n8\nD-Turanose\nU1\nNone\n\n\n9\nStachyose\nU1\nNone\n\n\n10\nD-Raffinose\nU1\nNone\n\n\n11\nα-D-Lactose\nU1\nNone\n\n\n12\nD-Melibiose\nU1\nEC\n\n\n13\nβ-Methyl-D-Glucoside\nU1\nNone\n\n\n14\nD-Salicin\nU1\nNone\n\n\n15\nN-Acetyl-D-Glucosamine\nU2\nNone\n\n\n16\nN-Acetyl-β-Dmannosamine\nU1\nNone\n\n\n17\nN-Acetyl-D-Galactosamine\nU1\nNone\n\n\n18\nN-AcetylNeuraminic Acid\nU2\nEC\n\n\n19\nα-D-Glucose\nU3\nPP\n\n\n20\nD-Mannose\nU2\nPP\n\n\n21\nD-Fructose\nU2\nEC\n\n\n22\nD-Galactose\nU1\nEC\n\n\n23\n3-MethylGlucose\nU1\nNone\n\n\n24\nD-Fucose\nU1\nPP\n\n\n25\nL-Fucose\nU2\nEC\n\n\n26\nL-Rhamnose\nU1\nEC\n\n\n27\nInosine\nU2\nNone\n\n\n28\nD-Sorbitol\nU1\nEC\n\n\n29\nD-Mannitol\nU2\nEC\n\n\n30\nD-Arabitol\nU1\nNone\n\n\n31\nmyo-Inositol\nU1\nNone\n\n\n32\nGlycerol\nU1\nNone\n\n\n33\nD-Glucose-6-PO4\nU2\nEC\n\n\n34\nD-Fructose-6-PO4\nU2\nEC\n\n\n35\nD-Aspartic Acid\nU1\nNone\n\n\n36\nD-Serine\nU2\nEC\n\n\n37\nGelatin\nU1\nNone\n\n\n38\nGlycyl-L-Prolin\nU1\nEC\n\n\n39\nL-Alanine\nU3\nPP\n\n\n40\nL-Arginine\nU3\nPP\n\n\n41\nL-Aspartic Acid\nU3\nPP\n\n\n42\nL-Glutamic\nU3\nPP\n\n\n43\nL-Histidine\nU3\nPP\n\n\n44\nL-Pyroglutamic\nU3\nPP\n\n\n45\nL-Serine\nU1\nNone\n\n\n46\nPectin\nU1\nEC\n\n\n47\nD-Galacturonic Acid\nU3\nPP\n\n\n48\nL-Galactonic Acid Lactone\nU2\nEC\n\n\n49\nD-Gluconic\nU2\nPP\n\n\n50\nD-Glucuronic\nU3\nPP\n\n\n51\nGlucuronamid\nU1\nPP\n\n\n52\nMucic Acid\nU3\nPP\n\n\n53\nQuinic Acid\nU3\nPP\n\n\n54\nD-Saccharic\nU3\nPP\n\n\n55\np-Hydroxy-Phenylacetic Acid\nU1\nPP\n\n\n56\nMethyl Pyruvate\nU1\nPP\n\n\n57\nD-Lactic Acid Methyl Ester\nU1\nNone\n\n\n58\nL-Lactic Acid\nU3\nPP\n\n\n59\nCitric Acid\nU3\nPP\n\n\n60\nα-Keto-Glutaric Acid\nU1\nPP\n\n\n61\nD-Malic Acid\nU1\nNone\n\n\n62\nL-Malic Acid\nU3\nPP\n\n\n63\nBromo-Succinic\nU1\nPP\n\n\n64\nTween 40\nU1\nNone\n\n\n65\ny-Amino-ButryricAcid\nU3\nPP\n\n\n66\nα-Hydroxy-ButyricAcid\nU1\nNone\n\n\n67\nβ-Hydroxy-D,L-ButyricAcid\nU1\nPP\n\n\n68\nα-Keto-ButyricAcid\nU1\nNone\n\n\n69\nAcetoacetic Acid\nU1\nPP\n\n\n70\nPropionic Acid\nU1\nPP\n\n\n71\nAcetic Acid\nU1\nNone\n\n\n72\nFormic Acid\nU1\nNone",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>分组数据分析实战</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis-in-action.html#数据探索",
    "href": "grouped-data-analysis-in-action.html#数据探索",
    "title": "12  分组数据分析实战",
    "section": "12.7 数据探索",
    "text": "12.7 数据探索\n现在我们有了多个参数,包括初始比例(ratio0)和最终比例(ratio1)、共培养中大肠杆菌和恶臭假单胞菌的数量(EC和PP)、CUE(A590)、碳源对大肠杆菌和恶臭假单胞菌的偏好性等。\n我们将所有这些数据合并到R中的一个数据框中,并进行如下统计分析。\n\nmerged &lt;- left_join(biolog_coculture_24h,qPCR_data) %&gt;% \n  left_join(carbon_name) %&gt;%\n  filter(!is.na(ratio1))\n\n\n12.7.1 数据标准化\n运行几个分析需要数据呈正态分布,因此,我们探索了合并数据中观察数据的正态性。根据数据值的偏度,我们用选定的方法转换原始数据。标准化后,可以看到所有关键变量大致符合正态分布。\n\nmerged &lt;- merged %&gt;% filter(ratio0 %in% c(\"less\",\"equal\",\"more\"))\n\npar(mfrow=c(3,4))\n\nhist(merged$EC)\nqqnorm(merged$EC)\nhist(log10(merged$EC))\nqqnorm(log10(merged$EC))\n\nhist(merged$PP)\nqqnorm(merged$PP)\nhist(log10(merged$PP))\nqqnorm(log10(merged$PP))\n\nhist(merged$ratio1)\nqqnorm(merged$ratio1)\nhist(log10(merged$ratio1))\nqqnorm(log10(merged$ratio1))\n\n\n\n\n\n\n\nFigure 12.1: 数据标准化。左侧为原始数据的直方图和Q-Q图,右侧为对数转换后的直方图和Q-Q图。由上至下分别为大肠杆菌数量(EC)、恶臭假单胞菌数量(PP)和最终比例(ratio1)。有图可见，这些数据经过对数转换后,大致符合正态分布。\n\n\n\n\n\n因此,我们对大肠杆菌(EC)和恶臭假单胞菌(PP)的数量进行对数转换。\n\nmerged &lt;- merged  %&gt;%\n  # mutate_at(c(\"A590\"),sqrt) %&gt;%\n  mutate_at(c(\"EC\",\"PP\",\"ratio1\"),log10)",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>分组数据分析实战</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis-in-action.html#final-ratio",
    "href": "grouped-data-analysis-in-action.html#final-ratio",
    "title": "12  分组数据分析实战",
    "section": "12.8 共培养的最终比例",
    "text": "12.8 共培养的最终比例\n对于符合正态分布的数据，我们可以使用 ANOVA 检验来揭示每种碳源中三种共培养的最终比例的差异。由于包含多重比较，所以使用”BH”方法对 p 值进行了调整。\n\naov_p &lt;- compare_means(ratio1 ~ ratio0,\n                       group.by = \"carbon_id\",\n                       data=merged,\n                       method = \"anova\",\n                       p.adjust.method = \"BH\") %&gt;% \n  arrange(p.adj) %&gt;% \n  mutate(p.adj.signif = cut(p.adj,breaks = c(0,0.01,0.05,1),labels = c(\"**\",\"*\",\"ns\"))) %&gt;%\n  left_join(carbon_prefer)\n\naov_p\n\n# A tibble: 71 × 9\n   carbon_id .y.          p   p.adj p.format p.signif method p.adj.signif prefer\n       &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;  &lt;fct&gt;        &lt;chr&gt; \n 1        14 ratio1 5.41e-7 3.8 e-5 5.4e-07  ****     Anova  **           None  \n 2        23 ratio1 2.02e-6 7.20e-5 2.0e-06  ****     Anova  **           None  \n 3        24 ratio1 3.46e-6 8.2 e-5 3.5e-06  ****     Anova  **           PP    \n 4        31 ratio1 5.08e-6 9   e-5 5.1e-06  ****     Anova  **           None  \n 5        30 ratio1 4.18e-5 5.9 e-4 4.2e-05  ****     Anova  **           None  \n 6        58 ratio1 7.53e-5 8.9 e-4 7.5e-05  ****     Anova  **           PP    \n 7         8 ratio1 1.45e-4 1   e-3 0.00015  ***      Anova  **           None  \n 8        10 ratio1 2.17e-4 1   e-3 0.00022  ***      Anova  **           None  \n 9        32 ratio1 1.11e-4 1   e-3 0.00011  ***      Anova  **           None  \n10        35 ratio1 2.12e-4 1   e-3 0.00021  ***      Anova  **           None  \n# ℹ 61 more rows\n\n\n\n12.8.1 最终比例 ANOVA 检验p值的密度分布\n为了更好地理解最终比例的差异,我们绘制了所有碳源中三种共培养的最终比例是否不同的调整后 p 值(ANOVA)的密度分布。在Figure 12.2中,垂直线表示 p 值截断值(0.05)的位置。\n\np.cutoff &lt;- 0.05\np1 &lt;- ggplot(aov_p,aes(p.adj)) + \n  # geom_histogram(bins=30) + \n  geom_line(stat = \"density\",lwd=1) +\n  geom_density(lwd=0,color=NA,fill=\"lightblue\") +\n  geom_vline(xintercept = p.cutoff,lwd=1,lty=\"dashed\",color=\"firebrick\") +\n  labs(x=\"P.adj\",y=\"Density\")+\n  geom_text(x=0.06,y=0,label=p.cutoff,\n            vjust=\"top\",\n            hjust=\"left\",\n            color=\"firebrick\")\np1\n\n\n\n\n\n\n\nFigure 12.2: 所有碳源中检验最终比例三种共培养是否不同的调整后p值(ANOVA)的密度分布。\n\n\n\n\n\nFigure 12.3显示了调整后 p 值显著性的频数，并按碳源偏好性着色。\n\np2 &lt;- ggplot(aov_p, aes(p.adj.signif,fill=prefer)) + geom_bar() +\n  labs(x=\"Significance of adjusted p-value\",y=\"Frequency\") + \n  # geom_text(aes(label=Freq),vjust=0,nudge_y = 1) +\n  scale_fill_discrete(breaks=c(\"None\",\"EC\",\"PP\"),labels=c(\"None\",\"E. coli\",\"P. putida\"),name=\"Preference\") +\n  theme(legend.text = element_text(face=\"italic\"),\n        legend.position = c(0.65,0.7))\np2\n\n\n\n\n\n\n\nFigure 12.3\n\n\n\n\n\n\n\n12.8.2 最终比例\n为了进一步佐证最终比例之间存在的差异，我们从显著/非显著结果分别挑选了 5 个实例。并使用箱线图展示了这些实例中最终比例的分布Figure 12.4。\n\nmerged$ratio0 &lt;- relevel(merged$ratio0, \"less\")\n\ncarbon_name_labeller &lt;- function(x){\n  name_of &lt;- carbon_name$carbon_source\n  names(name_of) &lt;- carbon_name$carbon_id\n  return(as.character(name_of[x]))\n}\nselected_significant_carbon_id &lt;- c(29,32,36,39,46)\nselected_nonsignificant_carbon_id &lt;- c(3,4,12,15,53)\np1 &lt;- ggplot(\n  data=filter(merged,carbon_id %in% selected_significant_carbon_id) %&gt;%\n    left_join(aov_p), \n  mapping = aes(ratio0,ratio1,color=prefer)) \np2 &lt;- ggplot(\n  data=filter(merged,carbon_id %in% selected_nonsignificant_carbon_id) %&gt;%\n    left_join(aov_p),\n  mapping = aes(ratio0,ratio1,color=prefer)) \n\nplots &lt;- lapply(list(p1,p2),function(x){\n  x + geom_boxplot() + geom_jitter() +\n    geom_text(aes(x=\"equal\", y=0.15,label= paste(\"p.adj=\",p.adj,sep = \"\")),check_overlap = T,size=3,show.legend = FALSE) +\n    geom_text(aes(x=\"less\",y=.65,label=carbon_id),color=\"grey\",size=3) +\n    facet_wrap(~carbon_id,\n               ncol=5,\n               labeller = labeller(carbon_id=carbon_name_labeller)) + \n    # stat_compare_means(method=\"aov\") + \n    scale_x_discrete(breaks=c(\"less\",\"equal\",\"more\"),labels=c(\"1:1000\",\"1:1\",\"1000:1\")) +\n    theme(axis.text.x = element_text(angle = 60, hjust = 1,vjust = 1)) +\n    scale_color_discrete(breaks=c(\"None\",\"EC\",\"PP\"),labels=c(\"None\",\"E. coli\",\"P. putida\"),name=\"Preference\")+\n    theme(legend.text = element_text(face=\"italic\")) +\n    labs(x=\"\",y=\"Final ratio (EC/PP)\")\n})\n\nplot_grid(plotlist = plots,ncol=1,labels=c(\"B\",\"C\"))\n\n\n\n\n\n\n\nFigure 12.4: （原文图 2B,C）共培养的最终比例。最终比例显著结果(B)或非显著结果(C)的示例。每个子图上方的标签表示碳源。\n\n\n\n\n\n这些结果表明，初始比例对最终比例有显著影响，但最终比例并不完全由初始比例决定。",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>分组数据分析实战</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis-in-action.html#碳源偏好性对最终比例的影响",
    "href": "grouped-data-analysis-in-action.html#碳源偏好性对最终比例的影响",
    "title": "12  分组数据分析实战",
    "section": "12.9 碳源偏好性对最终比例的影响",
    "text": "12.9 碳源偏好性对最终比例的影响\n为了分析碳源偏好性如何影响最终比例，我们将最终比例分为三组，然后分别对比碳源偏好性和初始比例。\n\n12.9.1 绘制偏好利用的碳源\n这里，分别绘制了大肠杆菌和恶臭假单胞菌偏好的碳源。在Figure 12.5中，左侧是大肠杆菌偏好的碳源，右侧是恶臭假单胞菌偏好的碳源。\n\nplots &lt;- lapply(c(\"EC\",\"PP\"),function(x){\n  d &lt;- biolog_mono_24h %&gt;% \n    left_join(carbon_name) %&gt;%\n    filter(prefer == x)\n  ggplot(d,aes(carbon_source,A590,fill=species)) + \n    geom_boxplot() +\n    labs(y=\"CUE\",x=\"\") +\n    # coord_flip() +\n    theme(legend.position = c(0.5,0.9),\n          legend.direction = \"horizontal\",\n          legend.text = element_text(face = \"italic\"),\n          legend.title = element_blank(),\n          axis.text.x = element_text(angle = 60, hjust = 1,vjust = 1))\n})\n\nplot_grid(plotlist = plots, ncol=2,labels=c(\"A\",\"B\"))\n\n\n\n\n\n\n\nFigure 12.5\n\n\n\n\n\n首先，按初始比例分为 3 组，分别分析碳源利用偏好性对最终比例的影响。Figure 12.6显示了不同初始比例的前提下，不同碳源偏好性下的最终比例。\n\nratio0_labeller &lt;- function(x){\n  name_of_ratio0 &lt;- c(\"P. putida\",\"1:1000\",\"1:1\",\"1000:1\",\"E. coli\") \n  names(name_of_ratio0) &lt;- c(\"none\",\"less\",\"equal\",\"more\",\"all\")\n  return(as.character(name_of_ratio0[x]))\n}\n\np1 &lt;- ggplot(merged,aes(x=prefer,y=ratio1)) + \n  geom_boxplot() + \n  facet_wrap(~ratio0,\n             labeller = labeller(ratio0 = ratio0_labeller)) + \n  scale_x_discrete(breaks=c(\"None\",\"EC\",\"PP\"),labels=c(\"None\",\"E. coli\",\"P. putida\")) +\n  theme(axis.text.x = element_text(face = \"italic\",\n                                   angle = 60,\n                                   hjust = 1,\n                                   vjust = 1)) +\n  stat_compare_means(method=\"wilcox.test\",comparisons = list(c(\"EC\",\"None\"),c(\"None\",\"PP\"),c(\"EC\",\"PP\")),size=3) +\n  labs(x=\"Carbon Source Preference\", y=\"Final Ratio (EC/PP)\") +\n  ylim(c(NA,1.5))\n\np1\n\n\n\n\n\n\n\nFigure 12.6: 碳源利用偏好性对最终比例的影响。箱线图显示了不同初始比例的前提下，不同碳源偏好性下的最终比例。\n\n\n\n\n\n其次，我们将碳源偏好性分为 3 组，分别分析初始比例对最终比例的影响。Figure 12.7显示了不同碳源偏好性下，不同初始比例的最终比例。\n\nprefer_labeller &lt;- function(x){\n  name_of_prefer &lt;- c(\"None\",\"E. coli\",\"P. putida\")\n  names(name_of_prefer) &lt;- c(\"None\",\"EC\",\"PP\")\n  return(as.character(name_of_prefer[x]))\n}\n\np2 &lt;- ggplot(merged,aes(x=ratio0,y=ratio1)) + \n  geom_boxplot() + \n  facet_wrap(~prefer, labeller = labeller(prefer = prefer_labeller)) + \n  theme(strip.text = element_text(face = \"italic\")) +\n  stat_compare_means(method=\"wilcox.test\",comparisons = list(c(\"less\",\"equal\"),c(\"equal\",\"more\"),c(\"less\",\"more\")),size=3) +\n  scale_x_discrete(breaks=c(\"less\",\"equal\",\"more\"),labels=c(\"1:1000\",\"1:1\",\"1000:1\")) +\n    theme(axis.text.x = element_text(angle = 60, hjust = 1,vjust = 1)) +\n  labs(x=\"Initial Ratio (EC/PP)\", y=\"Final Ratio (EC/PP)\") +\n  ylim(c(NA,1.5))\n\np2\n\n\n\n\n\n\n\nFigure 12.7: 初始比例对最终比例的影响。箱线图显示了不同碳源偏好类型中，不同初始比例下的最终比例。",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>分组数据分析实战</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis-in-action.html#初始比例调控碳源利用效率",
    "href": "grouped-data-analysis-in-action.html#初始比例调控碳源利用效率",
    "title": "12  分组数据分析实战",
    "section": "12.10 初始比例调控碳源利用效率",
    "text": "12.10 初始比例调控碳源利用效率\n\n12.10.1 单培养和共培养的比较\n首先，我们比较了单培养和共培养的碳源利用效率(CUE)。Figure 12.8显示了单培养和共培养的 CUE。为了便于比较，我们在图上做了一个水平参考线，表示所有样本的中位数。此外，还在图上标记了每个初始比例下的中位数。\n\np_cue &lt;- ggplot(biolog_24h,aes(ratio0,A590)) + \n  geom_boxplot() +\n  geom_hline(aes(yintercept = median(A590)),lty=2,color=\"firebrick\") +\n  scale_x_discrete(breaks=c(\"none\",\"less\",\"equal\",\"more\",\"all\"),\n                  labels=c(\"P. putida\",\"1:1000\",\"1:1\",\"1000:1\",\"E. coli\")) +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1,vjust = 1)) +\n  geom_text(aes(ratio0,y,label=y),\n            inherit.aes = F, \n            data = biolog_24h %&gt;% group_by(ratio0) %&gt;% \n              summarise(y=median(A590)),\n            vjust=-0.3) +\n  labs(x=\"Initial Ratio (EC/PP)\",y=\"CUE\")\n\np_cue\n\n\n\n\n\n\n\nFigure 12.8: 单培养和共培养的比较。用箱线图显示了单培养和共培养的碳源利用效率(CUE)。为了便于比较，在图上做了一个水平参考线，表示所有样本的中位数。此外，还在图上标记了每个初始比例下的中位数。\n\n\n\n\n\n接下来，我们使用 vegan::rda() 对不同初始比例下的碳源利用情况进行 PCA 分析。\n为了更好地理解不同初始比例菌群碳源利用谱的差异，我们将 PCA 分析结果绘制成图。Figure 12.9显示了碳源利用谱的 PCA 分析。椭圆代表聚类的 95% 置信区间。由图可知，这 5 组样品大体上被分为 3 类。第一类是大肠杆菌单独培养，第二类是恶臭假单胞菌和1:1000 共培养，第三类是大肠杆菌与恶臭假单胞菌 1:1 和 1000:1 的共培养。\n\nlibrary(vegan)\n\n# PCA\npca &lt;- rda(t(M_A590_24h))\n\n# 计算贡献率\npercent_var &lt;- pca$CA$eig/pca$tot.chi\ndf &lt;- scores(pca)$sites  %&gt;%\n  as.data.frame() %&gt;%\n  tibble::rownames_to_column(var=\"sample\") %&gt;%\n  separate(sample,c(\"ratio0\",\"rep\"),sep=\"-\",remove = F)\ndf$ratio0 &lt;- factor(df$ratio0, \n  levels = c(\"none\",\"less\",\"equal\",\"more\",\"all\"),\n  labels = c(\"P. putida\",\"1:1000\",\"1:1\",\"1000:1\",\"E. coli\"))\n\n# 聚类\ngroup &lt;- cutree(hclust(dist(t(M_A590_24h))),k=3)\nclustered_group &lt;- as.data.frame(group) %&gt;% \n  tibble::rownames_to_column(var = \"sample\")\ndf = df |&gt;  left_join(clustered_group) \n\n# PCA plot\np &lt;- ggplot(df, aes(PC1,PC2,label=ratio0, color=ratio0))+\n  geom_point(size=3) +\n  scale_color_manual(values = brewer.pal(9,\"YlOrRd\")[5:9],name=\"initial ratio\")+\n  xlab(paste0(\"PC1: \", round(percent_var[1] * 100), \"% Variance\")) +\n  ylab(paste0(\"PC2: \", round(percent_var[2] * 100), \"% Variance\"))\n\np\n\n\n\n\n\n\n\nFigure 12.9: 碳源利用谱的PCA分析。椭圆代表聚类的95%置信区间。\n\n\n\n\n\n\n\n12.10.2 初始比例调控共培养的碳源利用能力\n特定比例的大肠杆菌和恶臭假单胞菌共培养获得了对 14 种 U2 碳源更高的代谢能力Figure 12.10。\n\nanno_carbon_group &lt;- carbon_group %&gt;% \n  left_join(carbon_prefer) %&gt;%\n  column_to_rownames(var=\"carbon_id\") %&gt;%\n  rename(Usage = usage, Preference = prefer)\ncolnames(M_A590_24h) &lt;- rep(c(\"E. coli\",\"1:1\",\"1:1000\",\"1000:1\",\"P. putida\"),each=3)\npheatmap(t(M_A590_24h),\n         annotation_col = anno_carbon_group[c(2,1)],\n         cutree_cols = 3,\n         # cutree_rows = 3,\n         fontsize_col = 6,\n         silent = T) |&gt; print()\n\n\n\n\n\n\n\nFigure 12.10: 初始比例调控共培养的碳源利用谱。按使用组对碳源进行聚类。在热图中,顶部的条带表示碳源类型,底部的数字表示碳源ID,右侧给出了实验重复。图例条表示CUE值的范围。\n\n\n\n\n\n我们接下来分析，这14 种 U2 碳源下不同体系的CUEFigure 12.11。\n\n# 仅分析U2碳源\nbiolog_24h_U2 &lt;- left_join(biolog_24h,carbon_group) %&gt;% \n  filter(usage==\"U2\") \n\n# 多重比较\nhsd_group &lt;- lapply(unique(biolog_24h_U2$carbon_id), function(x){\n  m &lt;- aov(A590~ratio0,data=filter(biolog_24h_U2,carbon_id==x))\n  library(agricolae)\n  g &lt;- HSD.test(m,\"ratio0\",group=TRUE)$groups\n  d &lt;- rownames_to_column(g,var=\"ratio0\")\n  d$carbon_id &lt;- x\n  return(d[-2])\n})\n\n# 合并多重比较结果\nhsd_group &lt;- do.call(\"rbind\",hsd_group)\nhsd_group$ratio0 &lt;- factor(hsd_group$ratio0, \n                           levels = c(\"none\",\"less\",\"equal\",\"more\",\"all\"))\n# 在箱线图顶部添加组标签\nhsd_group &lt;- biolog_24h_U2 %&gt;% \n  group_by(ratio0,carbon_id) %&gt;%\n  summarize(q3=quantile(A590)[3]) %&gt;% \n  left_join(hsd_group)\n\nFigure 12.11 显示了 14 种 U2 碳源的单培养和共培养的 CUE。x 轴表示培养条件，y 轴表示 CUE。进行了 ANOVA 和 Tukey 多重比较。箱线图上的文本表示不同培养物之间是否观察到显著差异。明显可以得出，特定初始比例的大肠杆菌和恶臭假单胞菌共培养获得了对 14 种 U2 碳源更高的代谢能力。说明合成菌群的功能受到初始比例的调控。\n\nu2_p1 &lt;- ggplot(biolog_24h_U2, aes(ratio0,A590)) + \n  geom_boxplot() + \n  geom_text(aes(x=\"none\",y=max(A590)*1.1,label=carbon_id),color=\"grey\",vjust=1,size=3,show.legend = F) +\n  geom_text(aes(x=ratio0,y=q3,label=groups),show.legend = F,\n            data = hsd_group,inherit.aes = F,\n            vjust=0,nudge_y = .2,hjust=0) +\n  facet_wrap(~carbon_id,ncol=5,\n             labeller = labeller(carbon_id=carbon_name_labeller)) +\n  scale_x_discrete(breaks=c(\"none\",\"less\",\"equal\",\"more\",\"all\"),labels=c(\"P. putida\",\"1:1000\",\"1:1\",\"1000:1\",\"E. coli\")) +\n  scale_y_continuous(breaks = c(0,1,2)) +\n  labs(x=\"\",y=\"CUE\") + \n  # ggpubr::stat_compare_means(method=\"aov\",label=\"p.format\") +\n  theme(axis.text.x = element_text(angle = 45,hjust = 1,vjust = 1),\n        legend.position = \"top\",\n        legend.direction = \"horizontal\"\n  )\nu2_p1\n\n\n\n\n\n\n\nFigure 12.11: 14种U2碳源的单培养和共培养的CUE(从左到右,从上到下)。x轴表示培养条件,y轴表示CUE。进行了ANOVA和Tukey多重比较。箱线图上的文本表示不同培养物之间是否观察到显著差异。",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>分组数据分析实战</span>"
    ]
  },
  {
    "objectID": "grouped-data-analysis-in-action.html#小结",
    "href": "grouped-data-analysis-in-action.html#小结",
    "title": "12  分组数据分析实战",
    "section": "12.11 小结",
    "text": "12.11 小结\n本研究通过对碳源利用谱的分析，揭示了初始比例对合成菌群的功能有重要影响。我们发现，初始比例对最终比例有显著影响，但最终比例并不完全由初始比例决定。碳源偏好性对最终比例有显著影响，但不能完全解释这种现象。进一步的分析发现，特定比例的大肠杆菌和恶臭假单胞菌共培养获得了对 14 种 U2 碳源更高的代谢能力。这些结果表明，合成菌群的功能受到初始比例的调控。\n\n\n\n\nGao, Chun-Hui, Hui Cao, Peng Cai, and Søren J. Sørensen. 2021. “The Initial Inoculation Ratio Regulates Bacterial Coculture Interactions and Metabolic Capacity.” ISME Journal 15 (1): 29–40. https://doi.org/10.1038/s41396-020-00751-7.",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>分组数据分析实战</span>"
    ]
  },
  {
    "objectID": "score-analysis.html",
    "href": "score-analysis.html",
    "title": "13  测试成绩分析",
    "section": "",
    "text": "13.1 数据读取\n读取全部成绩数据。这里去掉了成绩为 0 的数据，因为这些数据可能是未参加考试的学生。\n# 读取数据\nlibrary(tidyverse)\n# 这里使用的是不含有学生真实姓名的匿名数据\nscore = read_csv(\"./data/score/score-anonymized.csv\") |&gt; \n    filter(grade &gt; 0) # 去掉成绩为 0 的数据\n\n# 查看数据集的前几行\nscore\n\n# A tibble: 298 × 4\n   group role   name                             grade\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;                            &lt;dbl&gt;\n 1     1 leader 6615cc6c8c7912dbedc4a7a34a1ef30c    79\n 2     1 member 019ba56b8d3607d60f2874496061643a    95\n 3     1 member 4c18826f2d9fe9109c55a8c98fcd8b15    81\n 4     1 member ba282a65bf882e5230af6c5d371ea2f0    79\n 5     1 member c4bef7c9914dd2f984389922335b4fdc    86\n 6     1 member d09c322f37e06e18ba37f385d64e6c3b    81\n 7     1 member 3f7c71dfbc00799d461285593e02023a    85\n 8     1 member 1b9de69574bc33486eb6bf921a93a99e    91\n 9     2 leader 735ff5379fdedfc92a86ab6db937a6e5    76\n10     2 member cfe087466143f41c0e13d034f7e21844    77\n# ℹ 288 more rows\n接下来，我们对成绩数据进行描述性统计分析。使用 summary 函数可以得到数据的基本统计信息。使用 ggplot2 包中的 geom_histogram 函数可以绘制直方图。\n# 描述性统计分析\nsummary(score$grade)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  46.00   78.00   82.00   81.44   86.00  100.00\n# 检验成绩分布情况可视化\nggplot(score, aes(grade)) +\n    geom_histogram(bins = 30)",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>测试成绩分析</span>"
    ]
  },
  {
    "objectID": "score-analysis.html#正态性检验",
    "href": "score-analysis.html#正态性检验",
    "title": "13  测试成绩分析",
    "section": "13.2 正态性检验",
    "text": "13.2 正态性检验\n检验是否符合正态分布。\n\nhist(score$grade)\n\n\n\n\n\n\n\n\n\n# 可视化检验正态性\nqqnorm(score$grade)\nqqline(score$grade)\n\n\n\n\n\n\n\n\n\n# 正态性检验\nshapiro.test(score$grade)\n\n\n    Shapiro-Wilk normality test\n\ndata:  score$grade\nW = 0.96996, p-value = 7.047e-06\n\n\n上面通过了 3 种方式检验了成绩的正态性，他们分别是直方图、QQ 图和 Shapiro-Wilk 正态性检验。从直方图和 QQ 图可以看出，成绩数据大致符合正态分布。而 Shapiro-Wilk 正态性检验的 p 值为却远小于 0.05，说明成绩数据不符合正态分布。\n在这种情况下，我们可以使用非参数检验方法，如秩和检验、秩和检验等。",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>测试成绩分析</span>"
    ]
  },
  {
    "objectID": "score-analysis.html#按分组统计",
    "href": "score-analysis.html#按分组统计",
    "title": "13  测试成绩分析",
    "section": "13.3 按分组统计",
    "text": "13.3 按分组统计\n我们看一下各个分组的成绩情况，比较一个各个分组的成绩是否存在显著差异。\n因为成绩数据不符合正态分布，所以我们首先使用非参数性的多分组比较方法，如 Kruskal-Wallis 秩和检验。\n\n# 检验多个分组间的成绩是否存在显著差异\nkruskal.test(grade ~ group, data = score)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  grade by group\nKruskal-Wallis chi-squared = 39.677, df = 34, p-value = 0.2317\n\n\n这个检验的原假设是各个分组的成绩没有显著差异，备择假设是各个分组的成绩有显著差异。\n根据检验的结果，p 值大于 0.05，我们不能拒绝原假设，即不同分组的成绩没有显著差异。\n各个分组得分的数据可视化结果如下：\n\nggplot(score, aes(factor(group), grade)) +\n    geom_boxplot(outliers = FALSE) +\n    geom_jitter(width = 0.2) +\n    labs(title = \"Score by Group\",\n         x = 'Group', \n         y = \"Score\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n因为我们使用的 geom_gitter() 来添加数据点，所以在 geom_boxplot() 中设置 outliers = FALSE 来去除离群值在 boxplot 中的显示，避免离散的数值显示 2 次。使用 labs() 函数可以方便的设置图形的标题和坐标轴标签。使用 theme_bw() 函数可以设置图形的主题为白色背景，更符合学术研究的规范。",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>测试成绩分析</span>"
    ]
  },
  {
    "objectID": "score-analysis.html#图片美化",
    "href": "score-analysis.html#图片美化",
    "title": "13  测试成绩分析",
    "section": "13.4 图片美化",
    "text": "13.4 图片美化\n可能你会觉得上面的图形有些单调，我们可以使用 ggthemes 包中的 theme_economist() 函数来美化图形。\n\n# 图片美化\nlibrary(ggthemes)\nscore |&gt;\n    mutate(group = factor(group)) |&gt;\n    ggplot(aes(group, grade, color = group)) +\n    geom_boxplot(outliers = FALSE) +\n    geom_jitter(width = 0.2) +\n    labs(title = \"Score by Group\",\n         x = 'Group', \n         y = \"Score\") +\n    theme_economist() +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 13.1: 使用 ggthemes 包中的 theme_economist() 函数美化图形\n\n\n\n\n\n这里我们通过将 group 映射为 color 来为不同的分组设置不同的颜色。并使用 theme_economist() 函数可以设置图形的主题为经济学家杂志（The Economist）的风格。因为分组数据已使用 x 轴显示，所以颜色注释是多余的。因此，这里使用 theme(legend.position = \"none\") 函数去掉了图例。\n另外，还想介绍一个适用于论文发表的图形主题包，它的名字叫 ggsci。这个包提供了一些适合于科学研究的调色板，如 npg、lancet、jco 等。这里我们使用 npg 主题的调色板来美化图形。\n\n# 图片美化\nlibrary(ggsci)\nscore |&gt;\n    filter(group %in% 1:10) |&gt;\n    mutate(group = factor(group)) |&gt;\n    ggplot(aes(group, grade, color = group)) +\n    geom_boxplot(outliers = FALSE) +\n    geom_jitter(width = 0.2) +\n    labs(title = \"Score by Group\",\n         x = 'Group', \n         y = \"Score\") +\n    scale_color_npg() +\n    theme_bw() +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 13.2: 使用 ggsci 包中的 scale_color_npg() 函数美化图形\n\n\n\n\n\n需要注意的是，因为自带的 scale_color_npg() 函数仅包含了 10 种颜色，而我们的分组有 35 个，所以这里使用了前 10 组的数据来演示。",
    "crumbs": [
      "分组数据分析",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>测试成绩分析</span>"
    ]
  },
  {
    "objectID": "transcriptomics-data-analysis-and-visualization.html",
    "href": "transcriptomics-data-analysis-and-visualization.html",
    "title": "14  转录组学数据分析",
    "section": "",
    "text": "14.1 项目简介",
    "crumbs": [
      "转录组数据分析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>转录组学数据分析</span>"
    ]
  },
  {
    "objectID": "transcriptomics-data-analysis-and-visualization.html#项目简介",
    "href": "transcriptomics-data-analysis-and-visualization.html#项目简介",
    "title": "14  转录组学数据分析",
    "section": "",
    "text": "利用 RNA-seq 技术定量细菌基因表达差异\n从样品制备到数据分析的完整流程\n强调实验设计、数据预处理、比对、计数、差异分析和功能富集",
    "crumbs": [
      "转录组数据分析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>转录组学数据分析</span>"
    ]
  },
  {
    "objectID": "transcriptomics-data-analysis-and-visualization.html#实验设计",
    "href": "transcriptomics-data-analysis-and-visualization.html#实验设计",
    "title": "14  转录组学数据分析",
    "section": "14.2 实验设计",
    "text": "14.2 实验设计\n\n组别设置： 对照组与实验组，建议每组至少 3 个生物学重复\n关键点：\n\nRNA 提取及文库构建（推荐使用链特异性 RNA-seq）\n样品处理需注意 RNA 的稳定性及操作规范",
    "crumbs": [
      "转录组数据分析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>转录组学数据分析</span>"
    ]
  },
  {
    "objectID": "transcriptomics-data-analysis-and-visualization.html#rna-seq-实验流程概览",
    "href": "transcriptomics-data-analysis-and-visualization.html#rna-seq-实验流程概览",
    "title": "14  转录组学数据分析",
    "section": "14.3 RNA-seq 实验流程概览",
    "text": "14.3 RNA-seq 实验流程概览\n\n样品准备\n\nRNA 提取\n文库构建（推荐链特异性 RNA-seq）\n\n测序\n\n生成双端 paired-end reads (*.fastq.gz)\n\n数据预处理\n\n质量控制（例如 FastQC、Trimmomatic）",
    "crumbs": [
      "转录组数据分析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>转录组学数据分析</span>"
    ]
  },
  {
    "objectID": "transcriptomics-data-analysis-and-visualization.html#数据分析流程",
    "href": "transcriptomics-data-analysis-and-visualization.html#数据分析流程",
    "title": "14  转录组学数据分析",
    "section": "14.4 数据分析流程",
    "text": "14.4 数据分析流程\n\n短序列比对\n\n将 reads 比对到参考基因组（工具：hisat2、bowtie2）\n\n计数\n\n统计每个基因上的 reads 数（工具：featureCounts 或 htseq-count）\n\n差异表达分析\n\n使用 DESeq2 进行统计检验\n\n功能富集分析\n\n利用 ClusterProfiler 进行通路及 GO 富集分析",
    "crumbs": [
      "转录组数据分析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>转录组学数据分析</span>"
    ]
  },
  {
    "objectID": "transcriptomics-data-analysis-and-visualization.html#短序列比对",
    "href": "transcriptomics-data-analysis-and-visualization.html#短序列比对",
    "title": "14  转录组学数据分析",
    "section": "14.5 短序列比对",
    "text": "14.5 短序列比对\n使用 hisat2 建立索引并进行比对：\n# 建立基因组索引\nhisat2-build genome.fasta genome_index\n\n# 比对 paired-end reads\nhisat2 -1 sample_R1.fastq.gz -2 sample_R2.fastq.gz -x genome_index -S sample.sam",
    "crumbs": [
      "转录组数据分析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>转录组学数据分析</span>"
    ]
  },
  {
    "objectID": "transcriptomics-data-analysis-and-visualization.html#计数步骤",
    "href": "transcriptomics-data-analysis-and-visualization.html#计数步骤",
    "title": "14  转录组学数据分析",
    "section": "14.6 计数步骤",
    "text": "14.6 计数步骤\n转换比对结果并统计每个基因的 reads 数：\n# SAM 转 BAM 并排序\nsamtools view -bS sample.sam -o sample.bam\nsamtools sort sample.bam -o sample.sorted.bam\n\n# 计数（使用 featureCounts）\nfeatureCounts -T 4 -t CDS -g ID -a gene.gff -o counts.txt sample.sorted.bam",
    "crumbs": [
      "转录组数据分析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>转录组学数据分析</span>"
    ]
  },
  {
    "objectID": "transcriptomics-data-analysis-and-visualization.html#差异表达分析-deseq2",
    "href": "transcriptomics-data-analysis-and-visualization.html#差异表达分析-deseq2",
    "title": "14  转录组学数据分析",
    "section": "14.7 差异表达分析 (DESeq2)",
    "text": "14.7 差异表达分析 (DESeq2)\n在 R 中使用 DESeq2 进行差异表达基因分析：\nlibrary(DESeq2)\n\n# 读取计数数据\ncountData &lt;- read.table(\"counts.txt\", header = TRUE, row.names = 1)\n# 定义实验条件 (示例)\ncolData &lt;- data.frame(condition = factor(c(\"Control\", \"Control\", \"Control\", \"Treated\", \"Treated\", \"Treated\")))\nrownames(colData) &lt;- colnames(countData)\n\n# 构建 DESeq2 数据集并运行分析\ndds &lt;- DESeqDataSetFromMatrix(countData, colData, design = ~ condition)\ndds &lt;- DESeq(dds)\nres &lt;- results(dds)\n\n# 查看显著差异表达基因\nhead(res[order(res$pvalue), ])",
    "crumbs": [
      "转录组数据分析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>转录组学数据分析</span>"
    ]
  },
  {
    "objectID": "transcriptomics-data-analysis-and-visualization.html#功能富集分析-clusterprofiler",
    "href": "transcriptomics-data-analysis-and-visualization.html#功能富集分析-clusterprofiler",
    "title": "14  转录组学数据分析",
    "section": "14.8 功能富集分析 (ClusterProfiler)",
    "text": "14.8 功能富集分析 (ClusterProfiler)\n利用 ClusterProfiler 对差异表达基因进行 KEGG 或 GO 富集分析：\n\n14.8.1 方法一：富集分析\nlibrary(clusterProfiler)\n\n# 假设 gene_list 为筛选后的基因 ID 向量\nkk &lt;- enrichKEGG(gene         = gene_list,\n                 organism     = 'ko',\n                 pAdjustMethod = \"BH\",\n                 qvalueCutoff  = 0.05)\n\n# 可视化富集结果\ndotplot(kk)\n\n\n14.8.2 方法二：GSEA 分析\nlibrary(clusterProfiler)\n\n# 假设 gene_list 为筛选后的基因 ID 向量\ngse &lt;- gseKEGG(geneList=gene_list,\n               organism=\"ko\",\n               nPerm=1000,\n               minGSSize=10,\n               maxGSSize=500,\n               pvalueCutoff=0.05,\n               pAdjustMethod=\"BH\")\n\n# 可视化 GSEA 结果\ndotplot(gse)\n\n\n\n\nGao, Chun-Hui, Hui Cao, Feng Ju, Ke-Qing Xiao, Peng Cai, Yichao Wu, and Qiaoyun Huang. 2021. “Emergent Transcriptional Adaption Facilitates Convergent Succession Within a Synthetic Community.” ISME Communications 1 (1): 46. https://doi.org/10.1038/s43705-021-00049-5.",
    "crumbs": [
      "转录组数据分析",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>转录组学数据分析</span>"
    ]
  },
  {
    "objectID": "transcriptomics-data-analysis-in-action.html",
    "href": "transcriptomics-data-analysis-in-action.html",
    "title": "15  转录组数据分析实战",
    "section": "",
    "text": "15.1 识别差异表达基因 (Differentially Expressed Genes, DEGs)",
    "crumbs": [
      "转录组数据分析",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>转录组数据分析实战</span>"
    ]
  },
  {
    "objectID": "transcriptomics-data-analysis-in-action.html#识别差异表达基因-differentially-expressed-genes-degs",
    "href": "transcriptomics-data-analysis-in-action.html#识别差异表达基因-differentially-expressed-genes-degs",
    "title": "15  转录组数据分析实战",
    "section": "",
    "text": "15.1.1 读取 HT-seq 计数数据\n我们从 HT-seq 的计数数据开始，这些数据包含每个样本中基因的表达量。\n\n# 读取 HT-seq 计数数据\nht_counts &lt;- readRDS(\"data/deg/ht_counts.rds\")\n\n# 查看数据结构\nht_counts\n\n      gene count sample_id organism time ratio0   group\n1  PP_0001     1       A11       PP    0   more more_0h\n2  PP_0002     2       A11       PP    0   more more_0h\n3  PP_0003     4       A11       PP    0   more more_0h\n4  PP_0004    12       A11       PP    0   more more_0h\n5  PP_0005     1       A11       PP    0   more more_0h\n6  PP_0006    17       A11       PP    0   more more_0h\n7  PP_0007     0       A11       PP    0   more more_0h\n8  PP_0008     0       A11       PP    0   more more_0h\n9  PP_0009     0       A11       PP    0   more more_0h\n10 PP_0010     7       A11       PP    0   more more_0h\n11 PP_0011     3       A11       PP    0   more more_0h\n12 PP_0012     3       A11       PP    0   more more_0h\n13 PP_0013    22       A11       PP    0   more more_0h\n14 PP_0014     0       A11       PP    0   more more_0h\n [ reached 'max' / getOption(\"max.print\") -- omitted 608866 rows ]\n\n\n这个数据有很多行，每一行包括一个基因在某一实验条件下的 Read counts。对其所有的列的解释如下：\n\ngene：基因名。\ncount：基因的计数。基因在对应样本中的 reads count。\nsample_id：样本 ID。样本的唯一标识。\norganism：物种名称。EC 指这个基因是 E. coli 中的基因；PP 指这个基因是 P. putida 中的基因。\ntime：采样时间。间隔 4 h 取样，一共取样 48 h。\nratio0：初始比例。这里，none 表示为 P. putida 纯培养，less 表示 1:1000，equal 表示 1:1，more 表示 1000:1，all 表示为 E. coli 纯培养。\ngroup：实验组别。按照初始比例和时间分组后，得到的实验组别。每个实验组别包括 3 个实验重复。\n\n\n\n15.1.2 数据预处理\n我们需要将原始计数数据转换为适合 DESeq2 分析的格式。以下函数 myDESeqMatrix 用于按物种筛选数据并生成计数矩阵和列数据。\n\nlibrary(DESeq2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(tibble)\n\n# 定义一个函数来处理 HT-seq 数据\nmyDESeqMatrix &lt;- function(ht_counts, org = NULL) {\n  # 如果指定了物种，则筛选对应物种的数据\n  if (!is.null(org)) {\n    ht_counts &lt;- ht_counts %&gt;%\n      filter(organism == org) %&gt;%  # 筛选指定物种\n      filter(ratio0 != ifelse(org == \"EC\", \"none\", \"all\"))  # 去除不需要的组\n  }\n  \n  # 构建计数矩阵\n  count_data &lt;- ht_counts %&gt;%\n    dplyr::select(gene, count, sample_id) %&gt;%  # 选择基因、计数和样本ID\n    group_by(sample_id) %&gt;%\n    spread(sample_id, count) %&gt;%  # 将样本ID转为列\n    as.data.frame() %&gt;%\n    column_to_rownames(var = \"gene\")  # 设置基因为行名\n  \n  # 构建列数据（样本信息）\n  col_data &lt;- ht_counts %&gt;%\n    dplyr::select(sample_id, ratio0, time, group) %&gt;%\n    arrange(sample_id) %&gt;%  # 按样本ID排序\n    distinct()  # 去重\n  \n  return(list(\"count_data\" = count_data, \"col_data\" = col_data))\n}\n\n# 分别处理 E. coli 和 P. putida 的数据\nmat.EC &lt;- myDESeqMatrix(ht_counts, org = \"EC\")  # 处理 E. coli 数据\nmat.PP &lt;- myDESeqMatrix(ht_counts, org = \"PP\")  # 处理 P. putida 数据\n\n\n\n15.1.3 构建 DESeq 数据集\n使用 DESeqDataSetFromMatrix 函数构建 DESeq 数据集。\n\n# 构建 E. coli 的 DESeq 数据集\ndds.EC &lt;- DESeqDataSetFromMatrix(\n  countData = mat.EC$count_data,  # 计数矩阵\n  colData   = mat.EC$col_data,    # 样本信息\n  design    = ~ group             # 设计公式：按组别分析\n)\n\n# 构建 P. putida 的 DESeq 数据集\ndds.PP &lt;- DESeqDataSetFromMatrix(\n  countData = mat.PP$count_data,\n  colData   = mat.PP$col_data,\n  design    = ~ group\n)\n\n\n15.1.3.1 差异表达分析\n运行 DESeq 函数进行差异表达分析。这一步会进行标准化和统计测试。\n\n# 对 E. coli 和 P. putida 数据进行差异表达分析\ndds.EC &lt;- DESeq(dds.EC)  # 这一步可能耗时较长\ndds.PP &lt;- DESeq(dds.PP)\n\n\n\n15.1.3.2 提取差异表达基因结果\n定义比较组并提取差异表达基因结果。\n\n#' 依据 comparison 的分组信息从 dds 中获取结果,并进行富集分析,返回dotplot\n#'\n#' @param dds \n#' @param comparison \n#' @param lfcThreshold \n#' @param p.adjusted \n#'\n#' @return A list of DEG\n#' @export\n#'\n#' @examples\nmyDEG_Results &lt;- function(dds = NULL, comparison = NULL, \n                          lfcThreshold = 1,p.adjusted=0.05,\n                          filtered = TRUE) {\n  require(tibble,quietly = T)\n  require(dplyr,quietly = T)\n  results &lt;- lapply(comparison, function(x){\n    results(dds, contrast = c(\"group\",x),\n            lfcThreshold = lfcThreshold,\n            alpha = p.adjusted)\n  })\n  names(results) &lt;- sapply(comparison,function(x)paste(x,collapse = \"_vs_\"))\n  for (i in seq_along(results)){\n    results[[i]] %&lt;&gt;%\n      as.data.frame() %&gt;%\n      rownames_to_column(var=\"gene\") %&gt;%\n      as_tibble() %&gt;%\n      mutate(comparison = names(results[i])) \n  }\n  if (!filtered) return(results)\n  for (i in seq_along(results)){\n    results[[i]] %&lt;&gt;%\n      filter(padj &lt; p.adjusted) %&gt;%\n      dplyr::select(gene,log2FoldChange,padj,comparison) %&gt;%\n      mutate(expression = ifelse(log2FoldChange&gt;0,\"up\",\"dn\")) \n  }\n  return(results)\n}\n\n\n# 定义 E. coli 的比较组\ncomparisons.EC &lt;- list(\n  c(\"less_0h\", \"all_0h\"), c(\"equal_0h\", \"all_0h\"), c(\"more_0h\", \"all_0h\"),\n  c(\"less_4h\", \"all_4h\"), c(\"equal_4h\", \"all_4h\"), c(\"more_4h\", \"all_4h\"),\n  c(\"less_8h\", \"all_8h\"), c(\"equal_8h\", \"all_8h\"), c(\"more_8h\", \"all_8h\"),\n  c(\"less_24h\", \"all_24h\"), c(\"equal_24h\", \"all_24h\"), c(\"more_24h\", \"all_24h\")\n)\n\n# 定义 P. putida 的比较组\ncomparisons.PP &lt;- list(\n  c(\"less_0h\", \"none_0h\"), c(\"equal_0h\", \"none_0h\"), c(\"more_0h\", \"none_0h\"),\n  c(\"less_4h\", \"none_4h\"), c(\"equal_4h\", \"none_4h\"), c(\"more_4h\", \"none_4h\"),\n  c(\"less_8h\", \"none_8h\"), c(\"equal_8h\", \"none_8h\"), c(\"more_8h\", \"none_8h\"),\n  c(\"less_24h\", \"none_24h\"), c(\"equal_24h\", \"none_24h\"), c(\"more_24h\", \"none_24h\")\n)\n\n# 提取差异表达基因结果\nDEG_results.EC &lt;- myDEG_Results(dds = dds.EC, comparison = comparisons.EC)\nDEG_results.PP &lt;- myDEG_Results(dds = dds.PP, comparison = comparisons.PP)\n\n\n\n15.1.3.3 获取基因表达谱\n提取所有基因的表达谱，用于后续的富集分析。\n\ngene_expression.EC &lt;- myDEG_Results(dds.EC, comparison = comparisons.EC, filtered = FALSE)\ngene_expression.PP &lt;- myDEG_Results(dds.PP, comparison = comparisons.PP, filtered = FALSE)",
    "crumbs": [
      "转录组数据分析",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>转录组数据分析实战</span>"
    ]
  },
  {
    "objectID": "transcriptomics-data-analysis-in-action.html#数据可视化",
    "href": "transcriptomics-data-analysis-in-action.html#数据可视化",
    "title": "15  转录组数据分析实战",
    "section": "15.2 数据可视化",
    "text": "15.2 数据可视化\n继续添加数据可视化部分的内容。我们将重点放在 RNA-seq 数据分析中的关键可视化步骤，包括 PCA 分析、差异表达基因数量统计、Venn 图展示特异性差异基因、KEGG 富集分析和 GSEA 结果的可视化。\n\n15.2.1 PCA 分析 (主成分分析)\nPCA 是一种常用的数据降维方法，用于探索样本间的整体表达模式差异。我们通过 PCA 展示 E. coli 和 P. putida 在不同时间点和条件下的基因表达变化。\n\n# 加载必要的包\nlibrary(cowplot)\nlibrary(ggplot2)\nlibrary(RColorBrewer)\n\n# 定义 PCA 绘图函数\nmyPlotPCA &lt;- function(object, intgroup = c(\"time\", \"ratio0\"), show.label = FALSE) {\n  require(dplyr)\n  require(DESeq2)\n  require(vegan)\n  \n  # 执行 PCA 分析\n  pca &lt;- rda(t(assay(object)))  # 使用 vegan 包的 rda 函数进行 PCA\n  percent_var &lt;- pca$CA$eig / pca$tot.chi  # 计算主成分解释的方差比例\n  \n  # 提取样本分组信息\n  intgroup_df &lt;- as.data.frame(colData(object)[, intgroup, drop = FALSE]) %&gt;%\n    tibble::rownames_to_column(var = \"sample_id\")\n  \n  # 提取 PCA 坐标并合并分组信息\n  df &lt;- scores(pca)$sites %&gt;%\n    as.data.frame() %&gt;%\n    tibble::rownames_to_column(var = \"sample_id\") %&gt;%\n    left_join(intgroup_df, by = \"sample_id\") %&gt;%\n    mutate(time = factor(time, levels = sort(unique(as.numeric(time)))))\n  \n  # 绘制 PCA 图\n  p &lt;- ggplot(df, aes(PC1, PC2, color = ratio0)) +\n    geom_point(size = 2) +\n    xlab(paste0(\"PC1: \", round(percent_var[1] * 100), \"% variance\")) +\n    ylab(paste0(\"PC2: \", round(percent_var[2] * 100), \"% variance\")) +\n    facet_wrap(~ time, ncol = 4) +  # 按时间分面\n    scale_color_manual(values = brewer.pal(5, \"Dark2\")) +\n    theme(legend.position = \"none\")\n  \n  return(p)\n}\n\nvsd.EC &lt;- vst(dds.EC)\nvsd.PP &lt;- vst(dds.PP)\n\n# 提取 PCA 结果\nlist_of_vsd &lt;- list(vsd.EC, vsd.PP)\n\n# 绘制 *E. coli* 和 *P. putida* 的 PCA 图\nlist_of_PCA_plot &lt;- lapply(list_of_vsd, myPlotPCA)\nplot_grid(plotlist = list_of_PCA_plot, labels = \"auto\", ncol = 1)\n\n\n\n\n\n\n\nFigure 15.1: PCA 分析结果\n\n\n\n\n\n结果解释：\n\n图 A 展示了 E. coli 的 PCA 结果，图 B 展示了 P. putida 的结果。\n不同颜色表示不同的实验条件（如单培养和共培养）。\n随着时间推移，样本之间的表达模式逐渐趋于一致。\n\n\n\n15.2.2 差异表达基因数量统计\n我们绘制了每个时间点和条件下差异表达基因的数量，并用颜色区分上调和下调基因。\n\nlibrary(stringr)\n\n# 统计差异表达基因数量\ndeg_count &lt;- function(data) {\n  do.call(\"rbind\", lapply(data, function(x) table(x$expression))) %&gt;%\n    as.data.frame() %&gt;%\n    rownames_to_column(var = \"name\") %&gt;%\n    separate(name, into = c(\"ratio\", \"time\"), sep = \"_\", extra = \"drop\") %&gt;%\n    mutate(time = as.numeric(str_extract(time, \"[0-9]+\"))) %&gt;%\n    pivot_longer(cols = c(\"dn\", \"up\"), names_to = \"type\", values_to = \"count\") %&gt;%\n    mutate(count = ifelse(type == \"dn\", -count, count)) %&gt;%\n    complete(ratio, time, type, fill = list(count = 0)) %&gt;%\n    mutate(\n      ratio = factor(ratio, levels = c(\"less\", \"equal\", \"more\"), labels = c(\"1:1000\", \"1:1\", \"1000:1\")),\n      type = factor(type, levels = c(\"up\", \"dn\"), labels = c(\"Up\", \"Down\"))\n    )\n}\n\ndeg_count_EC &lt;- deg_count(DEG_results.EC)\ndeg_count_PP &lt;- deg_count(DEG_results.PP)\n\n# 绘制差异表达基因数量图\ndeg_count_plots &lt;- lapply(seq_along(list(deg_count_EC, deg_count_PP)), function(i) {\n  x &lt;- list(deg_count_EC, deg_count_PP)[[i]]\n  ggplot(x, aes(x = time, y = count, color = type)) +\n    geom_point() +\n    geom_line(linewidth = 1) +\n    scale_y_continuous(labels = function(x) abs(x)) +\n    facet_wrap(~ ratio) +\n    labs(x = \"Time (h)\", y = \"Number of DEGs\", color = \"Gene expression:\") +\n    theme(legend.position = \"right\")\n})\n\nplot_grid(plotlist = deg_count_plots, labels = \"auto\", ncol = 1)\n\n\n\n\n\n\n\nFigure 15.2: 差异表达基因数量统计\n\n\n\n\n\n结果解释：\n\n图 A 和图 B 分别展示了 E. coli 和 P. putida 的差异表达基因数量。\n上调基因用红色表示，下调基因用青色表示。\n\n\n\n15.2.3 Venn 图展示特异性差异基因\n为了展示不同时间点之间差异基因的重叠情况，我们使用 Venn 图进行可视化。\n\n# 加载必要的包\nlibrary(ggVennDiagram)\nlibrary(ggtext)\n\n# 绘制 Venn 图\ndeg_Venn_plot_EC &lt;- lapply(seq_along(c(\"1:1000\", \"1:1\", \"1000:1\")), function(i) {\n  gene_list &lt;- lapply(DEG_results.EC[(i * 4 - 3):(i * 4)], function(x) x$gene)\n  ggVennDiagram(gene_list, label = \"count\", category.names = c(\"0h\", \"4h\", \"8h\", \"24h\")) +\n    scale_fill_gradient(low = \"white\", high = \"red\", limits = c(0, 310)) +\n    labs(title = paste0(c(\"1:1000\", \"1:1\", \"1000:1\")[[i]], \" - *E. coli*\")) +\n    theme(legend.position = \"none\", \n    plot.title = element_markdown())\n})\n\ndeg_Venn_plot_PP &lt;- lapply(seq_along(c(\"1:1000\", \"1:1\", \"1000:1\")), function(i) {\n  gene_list &lt;- lapply(DEG_results.PP[(i * 4 - 3):(i * 4)], function(x) x$gene)\n  ggVennDiagram(gene_list, label = \"count\", category.names = c(\"0h\", \"4h\", \"8h\", \"24h\")) +\n    scale_fill_gradient(low = \"white\", high = \"red\", limits = c(0, 310)) +\n    labs(title = paste0(c(\"1:1000\", \"1:1\", \"1000:1\")[[i]], \" - *P. putida*\")) +\n    theme(legend.position = \"none\", \n    plot.title = element_markdown())\n})\n\nplot_grid(plotlist = c(deg_Venn_plot_EC, deg_Venn_plot_PP), labels = \"auto\")\n\n\n\n\n\n\n\nFigure 15.3: Venn 图展示特异性差异基因\n\n\n\n\n\n结果解释：\n\n图 A-C 展示了 E. coli 的特异性差异基因，图 D-F 展示了 P. putida 的结果。\n大多数差异基因在不同时间点之间是特异性的。\n\n\n\n15.2.4 KEGG 富集分析结果可视化\nck1 和 ck2 分别对应 E. coli 和 P. putida 的 KEGG 富集分析结果。这些结果是通过 clusterProfiler 包中的 compareCluster 函数生成的。compareCluster 用于对不同条件（如时间点和比例）下的基因列表进行功能富集分析。\n\n15.2.4.1 准备差异表达基因数据\n首先，从 DEG_results.EC 和 DEG_results.PP 中提取差异表达基因的数据，并将其整理为适合 compareCluster 函数输入的格式。\n\n# 整理 *E. coli* 的差异表达基因数据\ndeg1 &lt;- do.call(\"rbind\", DEG_results.EC) %&gt;% \n  separate(comparison, into = c(\"ratio\", \"time\"), extra = \"drop\")\n\n# 整理 *P. putida* 的差异表达基因数据\ndeg2 &lt;- do.call(\"rbind\", DEG_results.PP) %&gt;% \n  separate(comparison, into = c(\"ratio\", \"time\"), extra = \"drop\")\n\n\nDEG_results.EC 和 DEG_results.PP 是预先计算好的差异表达基因结果。\n使用 separate 函数将 comparison 列拆分为 ratio（比例条件）和 time（时间点）两列，方便后续分组分析。\n\n\n\n15.2.4.2 执行 KEGG 富集分析\n使用 compareCluster 函数对每个物种的差异表达基因进行 KEGG 富集分析。\n\nlibrary(clusterProfiler)\n\n# 对 *E. coli* 进行 KEGG 富集分析\nck1 &lt;- compareCluster(\n  gene ~ ratio + time,          # 公式：基因 ~ 条件变量\n  data = deg1,                  # 数据源\n  fun = \"enrichKEGG\",           # 使用 enrichKEGG 方法\n  organism = \"eco\",             # 物种：E. coli\n  use_internal_data = FALSE      # TRUE 时使用内置的 KEGG 数据\n)\n\n# 对 *P. putida* 进行 KEGG 富集分析\nck2 &lt;- compareCluster(\n  gene ~ ratio + time,\n  data = deg2,\n  fun = \"enrichKEGG\",\n  organism = \"ppu\",             # 物种：P. putida\n  use_internal_data = FALSE # TRUE 时使用内置的 KEGG 数据\n)\n\n\ngene ~ ratio + time 指定了分析的公式，其中 gene 是基因列表，ratio 和 time 是分组变量。\nfun = \"enrichKEGG\" 表示使用 KEGG 富集分析方法。\norganism 参数指定了目标物种的缩写（eco 表示 E. coli，ppu 表示 P. putida）。\nuse_internal_data = TRUE 表示使用 clusterProfiler 内置的 KEGG 数据库。\n\n\n\n15.2.4.3 可视化富集分析结果\n\n# 定义 KEGG 富集分析绘图函数\nck_plot &lt;- function(ck) {\n  df &lt;- data.frame(ck) %&gt;%\n    mutate(\n      ratio = factor(ratio, levels = c(\"less\", \"equal\", \"more\"), labels = c(\"1:1000\", \"1:1\", \"1000:1\")),\n      time = factor(time, levels = c(\"0h\", \"4h\", \"8h\", \"24h\"))\n    )\n  \n  ggplot(df, aes(time, Description, size = Count, color = p.adjust)) +\n    geom_point() +\n    facet_grid(~ ratio, scales = \"free_y\") +\n    labs(y = \"KEGG pathway\")\n}\n\n# 绘制 KEGG 富集分析结果\np1 &lt;- ck_plot(ck1)  # *E. coli*\np2 &lt;- ck_plot(ck2)  # *P. putida*\n\nplot_grid(p1, p2, rel_heights = c(1, 0.3), ncol = 1, labels = \"auto\", align = \"v\")\n\n\n\n\n\n\n\nFigure 15.4: KEGG 富集分析结果可视化\n\n\n\n\n\n结果解释：\n\n点图展示了不同时间点和条件下的显著富集通路。\n点的大小表示基因比例，颜色表示调整后的 p 值。\n\n\n\n\n15.2.5 GSEA 结果可视化\nGSEA（Gene Set Enrichment Analysis，基因集富集分析）结果。\n\n15.2.5.1 加载预计算的基因表达数据\n首先，加载预先计算好的基因表达数据，这些数据包含了不同条件下的基因表达信息。\n\n# 加载预计算的基因表达数据\ngene_expression.EC[[1]] # *E. coli* 数据\n\n# A tibble: 4,419 × 8\n   gene  baseMean log2FoldChange lfcSE   stat pvalue  padj comparison       \n   &lt;chr&gt;    &lt;dbl&gt;          &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;            \n 1 b0001     25.3        -1.27   1.25  -1.01  0.450  1     less_0h_vs_all_0h\n 2 b0002    447.          0.609  0.365  1.67  0.858  1     less_0h_vs_all_0h\n 3 b0003    239.          1.52   0.320  4.76  0.0511 0.534 less_0h_vs_all_0h\n 4 b0004    291.          0.956  0.300  3.19  0.558  1     less_0h_vs_all_0h\n 5 b0005     18.1        -0.381  1.22  -0.313 0.823  1     less_0h_vs_all_0h\n 6 b0006    156.          0.0746 0.436  0.171 0.990  1     less_0h_vs_all_0h\n 7 b0007    114.          0.932  0.390  2.39  0.569  1     less_0h_vs_all_0h\n 8 b0008   3706.         -0.929  0.241 -3.86  0.615  1     less_0h_vs_all_0h\n 9 b0009    219.          0.0859 0.310  0.277 0.999  1     less_0h_vs_all_0h\n10 b0010    100.          0.809  0.606  1.33  0.625  1     less_0h_vs_all_0h\n# ℹ 4,409 more rows\n\ngene_expression.PP[[1]] # *P. putida* 数据\n\n# A tibble: 5,729 × 8\n   gene    baseMean log2FoldChange lfcSE   stat pvalue  padj comparison        \n   &lt;chr&gt;      &lt;dbl&gt;          &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;             \n 1 PP_0001    471.          0.135  0.241  0.561  1.00      1 less_0h_vs_none_0h\n 2 PP_0002    376.          0.190  0.288  0.661  0.998     1 less_0h_vs_none_0h\n 3 PP_0003    283.          0.0392 0.230  0.170  1.00      1 less_0h_vs_none_0h\n 4 PP_0004   1565.          0.0787 0.232  0.338  1.00      1 less_0h_vs_none_0h\n 5 PP_0005    410.         -0.328  0.234 -1.40   0.998     1 less_0h_vs_none_0h\n 6 PP_0006   1183.          0.524  0.477  1.10   0.841     1 less_0h_vs_none_0h\n 7 PP_0007     11.8         0.506  0.459  1.10   0.860     1 less_0h_vs_none_0h\n 8 PP_0008    302.          0.502  0.388  1.29   0.901     1 less_0h_vs_none_0h\n 9 PP_0009     30.8         1.14   0.439  2.59   0.378     1 less_0h_vs_none_0h\n10 PP_0010    830.          0.0860 0.228  0.377  1.00      1 less_0h_vs_none_0h\n# ℹ 5,719 more rows\n\n\n\ngene_expression.EC 和 gene_expression.PP 是两个列表，分别包含 E. coli 和 P. putida 在不同条件下的基因表达数据。\n\n\n\n15.2.5.2 定义提取基因列表的函数\n为了准备 GSEA 分析所需的输入数据，定义一个函数 get_genelist，用于提取每个比较组的基因列表。\n\n# 定义提取基因列表的函数\nget_genelist &lt;- function(x) {\n  if (nrow(x) &lt; 1) return(NULL)  # 如果数据为空，则返回 NULL\n  geneList &lt;- x$log2FoldChange   # 提取 log2FoldChange 列\n  names(geneList) &lt;- x$gene      # 设置基因为名称\n  geneList &lt;- sort(geneList, decreasing = TRUE)  # 按降序排序\n  return(geneList)\n}\nset.seed(1234)  # 设置随机种子以确保结果可重复\n\n\ngeneList 是一个向量，其中每个元素是基因的 log2FoldChange 值，基因名作为向量的名称。\n\n\n\n15.2.5.3 运行 GSEA 分析\n使用 gseKEGG 函数对每个基因列表执行 GSEA 分析。\n\n# 对 *E. coli* 运行 GSEA 分析\ngseKEGG_results.EC &lt;- lapply(gene_expression.EC, function(x) {\n  geneList &lt;- get_genelist(x)  # 提取基因列表\n  tryCatch(\n    gseKEGG(\n      geneList,\n      organism = \"eco\",         # 物种：E. coli\n      eps = 1e-20,              # 防止数值溢出的小值\n      pvalueCutoff = 1,         # 返回所有结果（不限制 p 值）\n      use_internal_data = FALSE  # 使用内置 KEGG 数据\n    ),\n    error = function(e) NULL    # 捕获错误并返回 NULL\n  )\n})\n\n# 对 *P. putida* 运行 GSEA 分析\ngseKEGG_results.PP &lt;- lapply(gene_expression.PP, function(x) {\n  geneList &lt;- get_genelist(x)\n  tryCatch(\n    gseKEGG(\n      geneList,\n      organism = \"ppu\",         # 物种：P. putida\n      eps = 1e-20,\n      pvalueCutoff = 1,\n      use_internal_data = FALSE\n    ),\n    error = function(e) NULL\n  )\n})\n\n\ngseKEGG_results.EC 和 gseKEGG_results.PP 是两个列表，分别包含 E. coli 和 P. putida 的 GSEA 分析结果。\n\n\n\n15.2.5.4 整理 GSEA 结果\n定义一个函数 gse_result，将 GSEA 分析结果整理为一个数据框。\n\n# 整理 GSEA 结果的函数\ngse_result &lt;- function(result) {\n  name &lt;- names(result)  # 获取比较组名称\n  l &lt;- lapply(seq_along(result), function(i) {\n    data.frame(result[[i]]) %&gt;%  # 将每个 GSEA 结果转换为数据框\n      mutate(comparison = name[[i]])  # 添加比较组列\n  })\n  do.call(\"rbind\", l) %&gt;%  # 合并所有结果\n    separate(comparison, into = c(\"ratio\", \"time\"), extra = \"drop\") %&gt;%  # 拆分比较组列为比例和时间\n    mutate(\n      ratio = factor(ratio, levels = c(\"less\", \"equal\", \"more\"), labels = c(\"1:1000\", \"1:1\", \"1000:1\")),\n      time = factor(time, levels = c(\"0h\", \"4h\", \"8h\", \"24h\")),\n      type = ifelse(p.adjust &gt; 0.05, \"unchanged\",  # 根据调整后的 p 值判断基因集是否显著\n                    ifelse(enrichmentScore &gt; 0, \"activated\", \"suppressed\")),\n      enrichScore = abs(enrichmentScore)  # 取绝对值作为富集得分\n    )\n}\n\n\ngse_result 函数将 GSEA 结果整理为一个数据框，包含以下列：\n\nratio：比例条件（如 “1:1000”, “1:1”, “1000:1”）。\ntime：时间点（如 “0h”, “4h”, “8h”, “24h”）。\ntype：基因集的状态（激活、抑制或无变化）。\nenrichScore：富集得分的绝对值。\n\n\n调用 gse_result 函数，分别生成 E. coli 和 P. putida 的 GSEA 数据框。\n\n# 生成 *E. coli* 的 GSEA 数据框\ndf1 &lt;- gse_result(gseKEGG_results.EC) %&gt;%\n  filter(type %in% c(\"activated\", \"suppressed\"))  # 仅保留激活或抑制的基因集\n\n# 生成 *P. putida* 的 GSEA 数据框\ndf2 &lt;- gse_result(gseKEGG_results.PP) %&gt;%\n  filter(type %in% c(\"activated\", \"suppressed\"))\n\n\ndf1 和 df2 是两个数据框，分别包含 E. coli 和 P. putida 的 GSEA 分析结果。\nfilter 函数用于筛选出显著激活或抑制的基因集（排除无变化的基因集）。\n\n最后，我们使用点图展示 GSEA 分析结果，突出显示激活或抑制的通路。\n\n# 定义 GSEA 点图绘图函数\ngse_dotplot &lt;- function(df) {\n  ggplot(df, aes(time, Description, size = enrichScore, color = type)) +\n    geom_point() +\n    facet_grid(~ ratio, scales = \"free_y\") +\n    labs(y = \"KEGG pathway\") +\n    scale_size(limits = c(0.2, 1.0))\n}\n\n# 绘制 GSEA 点图\ngsea_plot_EC &lt;- gse_dotplot(df1)  # *E. coli*\ngsea_plot_PP &lt;- gse_dotplot(df2)  # *P. putida*\n\nplot_grid(gsea_plot_EC, gsea_plot_PP, align = \"v\", ncol = 1, labels = \"auto\", rel_heights = c(1.5, 1))\n\n\n\n\n\n\n\nFigure 15.5: GSEA 结果可视化\n\n\n\n\n\n结果解释：\n\n点图展示了 GSEA 分析中显著激活或抑制的通路。\n点的大小表示富集得分，颜色表示通路状态（激活或抑制）。\n\n\n\n\n\nGao, Chun-Hui, Hui Cao, Feng Ju, Ke-Qing Xiao, Peng Cai, Yichao Wu, and Qiaoyun Huang. 2021. “Emergent Transcriptional Adaption Facilitates Convergent Succession Within a Synthetic Community.” ISME Communications 1 (1): 46. https://doi.org/10.1038/s43705-021-00049-5.",
    "crumbs": [
      "转录组数据分析",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>转录组数据分析实战</span>"
    ]
  },
  {
    "objectID": "microbiome-data-analysis-and-visualization.html",
    "href": "microbiome-data-analysis-and-visualization.html",
    "title": "16  微生物组数据分析及可视化",
    "section": "",
    "text": "16.1 数据准备与预处理",
    "crumbs": [
      "微生物组数据分析",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>微生物组数据分析及可视化</span>"
    ]
  },
  {
    "objectID": "microbiome-data-analysis-and-visualization.html#数据准备与预处理",
    "href": "microbiome-data-analysis-and-visualization.html#数据准备与预处理",
    "title": "16  微生物组数据分析及可视化",
    "section": "",
    "text": "16.1.1 样品信息整理\n\n# 加载必要的包\nlibrary(tidyverse)\n\n# 示例元数据文件（CSV格式）\nmetadata &lt;- read_tsv(\"data/MiSeq_SOP/mouse.time.design\")  # 假设包含采样时间、地点、重复等信息\nhead(metadata)\n\n# A tibble: 6 × 2\n  group  time \n  &lt;chr&gt;  &lt;chr&gt;\n1 F3D0   Early\n2 F3D1   Early\n3 F3D141 Late \n4 F3D142 Late \n5 F3D143 Late \n6 F3D144 Late \n\n\n\n\n16.1.2 原始数据导入与质控\n\n# 安装并加载FastQC和Cutadapt\n# system(\"fastqc raw_data/*.fastq.gz -o fastqc_results/\")  # 运行FastQC生成质量报告\n# system(\"cutadapt -a ADAPTER_SEQUENCE -o trimmed_data/sample.fastq raw_data/sample.fastq\")  # 使用Cutadapt去除接头\n\n# R中读取FASTQ文件（可选）\nlibrary(ShortRead)\nfastq_data &lt;- readFastq(\"data/MiSeq_SOP/F3D0_S188_L001_R1_001.fastq.gz\")\nsummary(fastq_data)\n\n    Length      Class       Mode \n      7793 ShortReadQ         S4 \n\n\n\n\n16.1.3 ASV构建\n\n16.1.3.1 序列去噪与错误校正\n在开始ASV构建之前，我们首先需要设置输入文件路径并读取数据。这里我们使用DADA2包来处理双端测序数据。\n\n# 使用DADA2进行ASV构建\nlibrary(dada2)\n\n# 设置路径\npath &lt;- \"data/MiSeq_SOP\"\nfnFs &lt;- sort(list.files(path, pattern = \"_R1_001.fastq.gz\"))\nfnRs &lt;- sort(list.files(path, pattern = \"_R2_001.fastq.gz\"))\nsample.names &lt;- sapply(strsplit(fnFs, \"_\"), `[`, 1)\n\n接下来我们需要评估测序数据的质量。通过查看正向读段的质量分布图，我们可以确定合适的质量过滤参数。\n\n# 查看正向读段的质量分布\nplotQualityProfile(file.path(path, fnFs[1:2]))  # 可视化前两个样本的正向读段质量分布\n\n\n\n\n\n\n\n\n从质量分布图中，我们可以看到碱基质量随着测序位置的变化趋势。通常在读段末端质量会下降，这有助于我们确定截断位置。\n同样，我们也需要查看反向读段的质量分布。\n\n# 查看反向读段的质量分布\nplotQualityProfile(file.path(path, fnRs[1:2]))  # 可视化前两个样本的反向读段质量分布\n\n\n\n\n\n\n\n\n根据质量分布图，我们可以看到反向读段的质量通常低于正向读段，这是正常现象。这些信息将帮助我们在下一步设置适当的质量过滤参数。\n\n# 质量过滤\nfilt_path &lt;- file.path(path, \"filtered\")\nif (!file.exists(filt_path)) dir.create(filt_path)\nfnFs.filt &lt;- file.path(filt_path, paste0(sample.names, \"_F_filt.fastq.gz\"))\nfnRs.filt &lt;- file.path(filt_path, paste0(sample.names, \"_R_filt.fastq.gz\"))\n\n# 设置文件名\nnames(fnFs.filt) &lt;- sample.names\nnames(fnRs.filt) &lt;- sample.names\n\n\n# 过滤和修剪\nout &lt;- filterAndTrim(file.path(path, fnFs), fnFs.filt,\n                     file.path(path, fnRs), fnRs.filt,\n                     truncLen = c(240, 160), maxN = 0, maxEE = c(2, 2), \n                     truncQ = 2, rm.phix = TRUE)\nhead(out)\n\n                                 reads.in reads.out\nF3D0_S188_L001_R1_001.fastq.gz       7793      7113\nF3D1_S189_L001_R1_001.fastq.gz       5869      5299\nF3D141_S207_L001_R1_001.fastq.gz     5958      5463\nF3D142_S208_L001_R1_001.fastq.gz     3183      2914\nF3D143_S209_L001_R1_001.fastq.gz     3178      2941\nF3D144_S210_L001_R1_001.fastq.gz     4827      4312\n\n\n\n# 学习错误模型\nerrF &lt;- learnErrors(fnFs.filt, multithread = TRUE)\n\n33514080 total bases in 139642 reads from 20 samples will be used for learning the error rates.\n\nerrR &lt;- learnErrors(fnRs.filt, multithread = TRUE)\n\n22342720 total bases in 139642 reads from 20 samples will be used for learning the error rates.\n\n# 可视化错误模型\nplotErrors(errF, nominalQ = TRUE)\n\n\n\n\n\n\n\nplotErrors(errR, nominalQ = TRUE)\n\n\n\n\n\n\n\n\n\n# 使用DADA2进行序列去噪\ndadaFs &lt;- dada(fnFs.filt, err = errF, multithread = TRUE)\n\nSample 1 - 7113 reads in 1979 unique sequences.\nSample 2 - 5299 reads in 1639 unique sequences.\nSample 3 - 5463 reads in 1477 unique sequences.\nSample 4 - 2914 reads in 904 unique sequences.\nSample 5 - 2941 reads in 939 unique sequences.\nSample 6 - 4312 reads in 1267 unique sequences.\nSample 7 - 6741 reads in 1756 unique sequences.\nSample 8 - 4560 reads in 1438 unique sequences.\nSample 9 - 15637 reads in 3590 unique sequences.\nSample 10 - 11413 reads in 2762 unique sequences.\nSample 11 - 12017 reads in 3021 unique sequences.\nSample 12 - 5032 reads in 1566 unique sequences.\nSample 13 - 18075 reads in 3707 unique sequences.\nSample 14 - 6250 reads in 1479 unique sequences.\nSample 15 - 4052 reads in 1195 unique sequences.\nSample 16 - 7369 reads in 1832 unique sequences.\nSample 17 - 4765 reads in 1183 unique sequences.\nSample 18 - 4871 reads in 1382 unique sequences.\nSample 19 - 6504 reads in 1709 unique sequences.\nSample 20 - 4314 reads in 897 unique sequences.\n\ndadaRs &lt;- dada(fnRs.filt, err = errR, multithread = TRUE)\n\nSample 1 - 7113 reads in 1660 unique sequences.\nSample 2 - 5299 reads in 1349 unique sequences.\nSample 3 - 5463 reads in 1335 unique sequences.\nSample 4 - 2914 reads in 853 unique sequences.\nSample 5 - 2941 reads in 880 unique sequences.\nSample 6 - 4312 reads in 1286 unique sequences.\nSample 7 - 6741 reads in 1803 unique sequences.\nSample 8 - 4560 reads in 1265 unique sequences.\nSample 9 - 15637 reads in 3414 unique sequences.\nSample 10 - 11413 reads in 2522 unique sequences.\nSample 11 - 12017 reads in 2771 unique sequences.\nSample 12 - 5032 reads in 1415 unique sequences.\nSample 13 - 18075 reads in 3290 unique sequences.\nSample 14 - 6250 reads in 1390 unique sequences.\nSample 15 - 4052 reads in 1134 unique sequences.\nSample 16 - 7369 reads in 1635 unique sequences.\nSample 17 - 4765 reads in 1084 unique sequences.\nSample 18 - 4871 reads in 1161 unique sequences.\nSample 19 - 6504 reads in 1502 unique sequences.\nSample 20 - 4314 reads in 732 unique sequences.\n\n# 查看去噪结果\n#head(dadaFs[[1]])\n#head(dadaRs[[1]])\n\n\n# 合并配对末端\nmergers &lt;- mergePairs(dadaFs, fnFs.filt, dadaRs, fnRs.filt)\n# 查看合并结果\nhead(mergers[[1]])\n\n                                                                                                                                                                                                                                                      sequence\n1 TACGGAGGATGCGAGCGTTATCCGGATTTATTGGGTTTAAAGGGTGCGCAGGCGGAAGATCAAGTCAGCGGTAAAATTGAGAGGCTCAACCTCTTCGAGCCGTTGAAACTGGTTTTCTTGAGTGAGCGAGAAGTATGCGGAATGCGTGGTGTAGCGGTGAAATGCATAGATATCACGCAGAACTCCGATTGCGAAGGCAGCATACCGGCGCTCAACTGACGCTCATGCACGAAAGTGTGGGTATCGAACAGG\n2 TACGGAGGATGCGAGCGTTATCCGGATTTATTGGGTTTAAAGGGTGCGTAGGCGGCCTGCCAAGTCAGCGGTAAAATTGCGGGGCTCAACCCCGTACAGCCGTTGAAACTGCCGGGCTCGAGTGGGCGAGAAGTATGCGGAATGCGTGGTGTAGCGGTGAAATGCATAGATATCACGCAGAACCCCGATTGCGAAGGCAGCATACCGGCGCCCTACTGACGCTGAGGCACGAAAGTGCGGGGATCAAACAGG\n3 TACGGAGGATGCGAGCGTTATCCGGATTTATTGGGTTTAAAGGGTGCGTAGGCGGGCTGTTAAGTCAGCGGTCAAATGTCGGGGCTCAACCCCGGCCTGCCGTTGAAACTGGCGGCCTCGAGTGGGCGAGAAGTATGCGGAATGCGTGGTGTAGCGGTGAAATGCATAGATATCACGCAGAACTCCGATTGCGAAGGCAGCATACCGGCGCCCGACTGACGCTGAGGCACGAAAGCGTGGGTATCGAACAGG\n4 TACGGAGGATGCGAGCGTTATCCGGATTTATTGGGTTTAAAGGGTGCGTAGGCGGGCTTTTAAGTCAGCGGTAAAAATTCGGGGCTCAACCCCGTCCGGCCGTTGAAACTGGGGGCCTTGAGTGGGCGAGAAGAAGGCGGAATGCGTGGTGTAGCGGTGAAATGCATAGATATCACGCAGAACCCCGATTGCGAAGGCAGCCTTCCGGCGCCCTACTGACGCTGAGGCACGAAAGTGCGGGGATCGAACAGG\n5 TACGGAGGATGCGAGCGTTATCCGGATTTATTGGGTTTAAAGGGTGCGCAGGCGGACTCTCAAGTCAGCGGTCAAATCGCGGGGCTCAACCCCGTTCCGCCGTTGAAACTGGGAGCCTTGAGTGCGCGAGAAGTAGGCGGAATGCGTGGTGTAGCGGTGAAATGCATAGATATCACGCAGAACTCCGATTGCGAAGGCAGCCTACCGGCGCGCAACTGACGCTCATGCACGAAAGCGTGGGTATCGAACAGG\n6 TACGGAGGATGCGAGCGTTATCCGGATTTATTGGGTTTAAAGGGTGCGTAGGCGGGATGCCAAGTCAGCGGTAAAAAAGCGGTGCTCAACGCCGTCGAGCCGTTGAAACTGGCGTTCTTGAGTGGGCGAGAAGTATGCGGAATGCGTGGTGTAGCGGTGAAATGCATAGATATCACGCAGAACTCCGATTGCGAAGGCAGCATACCGGCGCCCTACTGACGCTGAGGCACGAAAGCGTGGGTATCGAACAGG\n  abundance forward reverse nmatch nmismatch nindel prefer accept\n1       579       1       1    148         0      0      1   TRUE\n2       470       2       2    148         0      0      2   TRUE\n3       449       3       4    148         0      0      1   TRUE\n4       430       4       3    148         0      0      2   TRUE\n5       345       5       6    148         0      0      1   TRUE\n6       282       6       5    148         0      0      2   TRUE\n\n\n\n# 构建ASV表\nseqtab &lt;- makeSequenceTable(mergers)\ndim(seqtab)\n\n[1]  20 293\n\n\n\n# 查看序列长度分布\ntable(nchar(getSequences(seqtab)))  |&gt; barplot()\n\n\n\n\n\n\n\n\n\n# 去除嵌合体\nseqtab.nochim &lt;- removeBimeraDenovo(seqtab, method = \"consensus\", multithread = TRUE)\n\n# 计算嵌合体去除效率\nsum(seqtab.nochim)/sum(seqtab)\n\n[1] 0.9640374\n\n\n\n\n16.1.3.2 序列数量统计\n\n# 计算每个步骤的序列数量\ngetN &lt;- function(x) sum(getUniques(x))\ntrack &lt;- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))\n# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)\ncolnames(track) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\", \"merged\", \"nonchim\")\nrownames(track) &lt;- sample.names\nhead(track)\n\n       input filtered denoisedF denoisedR merged nonchim\nF3D0    7793     7113      6976      6979   6540    6528\nF3D1    5869     5299      5227      5239   5028    5017\nF3D141  5958     5463      5331      5357   4986    4863\nF3D142  3183     2914      2799      2830   2595    2521\nF3D143  3178     2941      2822      2868   2553    2519\nF3D144  4827     4312      4151      4228   3646    3507\n\n\n\n\n\n16.1.4 分类学注释\n\n16.1.4.1 使用SILVA数据库进行分类学注释\n在进行物种分类注释之前，我们需要准备SILVA参考数据库。这个数据库包含了已知细菌和古菌的16S rRNA基因序列及其分类信息。\n\n# 下载SILVA数据库（如果尚未下载）\n# system(\"wget https://zenodo.org/records/14169026/files/silva_nr99_v138.2_toSpecies_trainset.fa.gz\")\n\nsilva_db = \"./data/silva_v138.2/silva_nr99_v138.2_wSpecies_train_set.fa.gz\"\n\n# 分类学注释\ntaxa &lt;- assignTaxonomy(seqtab.nochim, silva_db, multithread = TRUE)\n\n注释完成后，我们可以查看注释结果。结果中包含了每个ASV的完整分类学信息，从门到种水平。\n\n# 查看分类学注释结果\ntaxa.print &lt;- taxa # Removing sequence rownames for display only\nrownames(taxa.print) &lt;- NULL\nhead(taxa.print)\n\n     Kingdom    Phylum         Class         Order           Family          \n[1,] \"Bacteria\" \"Bacteroidota\" \"Bacteroidia\" \"Bacteroidales\" \"Muribaculaceae\"\n[2,] \"Bacteria\" \"Bacteroidota\" \"Bacteroidia\" \"Bacteroidales\" \"Muribaculaceae\"\n[3,] \"Bacteria\" \"Bacteroidota\" \"Bacteroidia\" \"Bacteroidales\" \"Muribaculaceae\"\n[4,] \"Bacteria\" \"Bacteroidota\" \"Bacteroidia\" \"Bacteroidales\" \"Muribaculaceae\"\n[5,] \"Bacteria\" \"Bacteroidota\" \"Bacteroidia\" \"Bacteroidales\" \"Bacteroidaceae\"\n[6,] \"Bacteria\" \"Bacteroidota\" \"Bacteroidia\" \"Bacteroidales\" \"Muribaculaceae\"\n     Genus         Species     \n[1,] NA            NA          \n[2,] NA            NA          \n[3,] NA            NA          \n[4,] NA            NA          \n[5,] \"Bacteroides\" \"caecimuris\"\n[6,] NA            NA          \n\n\n从输出结果可以看到，每个ASV都被赋予了一个完整的分类系统，包括门(Phylum)、纲(Class)、目(Order)、科(Family)、属(Genus)和种(Species)水平的分类信息。如果某一分类级别无法确定，则会显示为NA。\n\n\n\n16.1.5 创建phyloseq对象\n\nlibrary(phyloseq)\n\nsamples.out &lt;- rownames(seqtab.nochim)\nsubject &lt;- sapply(strsplit(samples.out, \"D\"), `[`, 1)\ngender &lt;- substr(subject,1,1)\nsubject &lt;- substr(subject,2,999)\nday &lt;- as.integer(sapply(strsplit(samples.out, \"D\"), `[`, 2))\nsamdf &lt;- data.frame(Subject=subject, Gender=gender, Day=day)\nsamdf$When &lt;- \"Early\"\nsamdf$When[samdf$Day&gt;100] &lt;- \"Late\"\nrownames(samdf) &lt;- samples.out\n\n\nps &lt;- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), \n               sample_data(samdf), \n               tax_table(taxa))\nps &lt;- prune_samples(sample_names(ps) != \"Mock\", ps) # Remove mock sample\n\n\n# 添加序列名称\ndna &lt;- Biostrings::DNAStringSet(taxa_names(ps))\nnames(dna) &lt;- taxa_names(ps)\nps &lt;- merge_phyloseq(ps, dna)\ntaxa_names(ps) &lt;- paste0(\"ASV\", seq(ntaxa(ps)))\nps\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 232 taxa and 19 samples ]\nsample_data() Sample Data:       [ 19 samples by 4 sample variables ]\ntax_table()   Taxonomy Table:    [ 232 taxa by 7 taxonomic ranks ]\nrefseq()      DNAStringSet:      [ 232 reference sequences ]",
    "crumbs": [
      "微生物组数据分析",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>微生物组数据分析及可视化</span>"
    ]
  },
  {
    "objectID": "microbiome-data-analysis-and-visualization.html#多样性分析",
    "href": "microbiome-data-analysis-and-visualization.html#多样性分析",
    "title": "16  微生物组数据分析及可视化",
    "section": "16.2 多样性分析",
    "text": "16.2 多样性分析\n\n16.2.1 Alpha多样性分析\n\nplot_richness(ps, x=\"Day\", measures=c(\"Shannon\", \"Simpson\"), color=\"When\")\n\n\n\n\n\n\n\n\n\n\n16.2.2 Beta多样性分析\n\n# 计算Bray-Curtis距离\nps.prop &lt;- transform_sample_counts(ps, function(otu) otu/sum(otu))\nord.nmds.bray &lt;- ordinate(ps.prop, method=\"NMDS\", distance=\"bray\")\n\nRun 0 stress 0.08043117 \nRun 1 stress 0.08076338 \n... Procrustes: rmse 0.01052743  max resid 0.03240295 \nRun 2 stress 0.08076337 \n... Procrustes: rmse 0.01048205  max resid 0.03225559 \nRun 3 stress 0.08076338 \n... Procrustes: rmse 0.01053409  max resid 0.03242571 \nRun 4 stress 0.1326152 \nRun 5 stress 0.08076338 \n... Procrustes: rmse 0.01054091  max resid 0.03244821 \nRun 6 stress 0.08076337 \n... Procrustes: rmse 0.01049199  max resid 0.03228863 \nRun 7 stress 0.08616061 \nRun 8 stress 0.08043116 \n... New best solution\n... Procrustes: rmse 1.226092e-06  max resid 2.909654e-06 \n... Similar to previous best\nRun 9 stress 0.08616061 \nRun 10 stress 0.08076336 \n... Procrustes: rmse 0.01047795  max resid 0.03224207 \nRun 11 stress 0.08076337 \n... Procrustes: rmse 0.01052241  max resid 0.03238733 \nRun 12 stress 0.09477093 \nRun 13 stress 0.101063 \nRun 14 stress 0.08616061 \nRun 15 stress 0.08043116 \n... Procrustes: rmse 4.902388e-07  max resid 9.732552e-07 \n... Similar to previous best\nRun 16 stress 0.08616061 \nRun 17 stress 0.08076341 \n... Procrustes: rmse 0.01057329  max resid 0.03255292 \nRun 18 stress 0.08076343 \n... Procrustes: rmse 0.01059541  max resid 0.03262505 \nRun 19 stress 0.1358145 \nRun 20 stress 0.1274324 \n*** Best solution repeated 2 times\n\n# 绘制NMDS图\nplot_ordination(ps.prop, ord.nmds.bray, color = \"When\", shape = \"Gender\") + \n  geom_point(size = 3)\n\n\n\n\n\n\n\n\n\n\n16.2.3 差异分析\n\n16.2.3.1 差异丰度分析\n\nlibrary(DESeq2)\n\n# 准备DESeq2输入数据\ndds &lt;- phyloseq_to_deseq2(ps, ~ When)\ndds &lt;- DESeq(dds)\n\n# 获取差异显著的ASV\nres &lt;- results(dds, contrast = c(\"When\", \"Early\", \"Late\"))\nsig_asvs &lt;- res[which(res$padj &lt; 0.05), ]\nhead(sig_asvs)\n\nlog2 fold change (MLE): When Early vs Late \nWald test p-value: When Early vs Late \nDataFrame with 6 rows and 6 columns\n       baseMean log2FoldChange     lfcSE      stat      pvalue        padj\n      &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;   &lt;numeric&gt;   &lt;numeric&gt;\nASV4    368.082      -1.127928  0.266729  -4.22875 2.34994e-05 0.000143040\nASV6    240.045      -3.137408  0.730308  -4.29601 1.73902e-05 0.000110665\nASV7    250.823      -0.854399  0.198571  -4.30274 1.68698e-05 0.000110665\nASV10   181.772       0.464222  0.190101   2.44198 1.46071e-02 0.038584923\nASV11   136.812      -1.952974  0.490931  -3.97811 6.94666e-05 0.000374051\nASV12   115.139      -3.210652  1.098399  -2.92303 3.46643e-03 0.011836602\n\n\n\n# 绘制火山图\nggplot(res, aes(x = log2FoldChange, y = -log10(padj))) +\n  geom_point(aes(color = padj &lt; 0.05)) +\n  scale_color_manual(values = c(\"red\", \"grey\"))\n\n\n\n\n\n\n\n\n\n\n\n16.2.4 代谢功能预测\n\n16.2.4.1 使用PICRUSt2进行功能预测\n# 在命令行运行PICRUSt2\npicrust2_pipeline.py -s seqtab_nochim.fasta -i seqtab_nochim.biom -o picrust2_output --threads 4\n\n\n16.2.4.2 功能差异分析\n# 加载KEGG通路预测结果\nkegg_pathways &lt;- read.table(\"picrust2_output/path_abun.tsv\", header = TRUE, row.names = 1)\nkegg_diff &lt;- DESeq2::DESeqDataSetFromMatrix(countData = kegg_pathways, colData = metadata, design = ~ TimePoint)\n\n\n\n16.2.5 共现网络分析\n在R中分析微生物数据时，构建共现网络（Co-occurrence Network）是一种常见的方法，用于探索不同微生物群落之间的相互关系。以下是详细的步骤和代码示例，帮助您从头到尾完成这一任务。\n\n16.2.5.1 数据准备\n\n# 去除低丰度OTU（例如总丰度小于10的OTU）\nps &lt;- prune_taxa(taxa_sums(ps) &gt; 30, ps)\notu_table &lt;- otu_table(ps) |&gt; as.data.frame()\n\n# 检查是否有缺失值\nif (any(is.na(otu_table))) {\n  stop(\"OTU表中存在缺失值，请先处理缺失值！\")\n}\n\n\n\n16.2.5.2 计算相关性矩阵\n共现网络通常基于微生物之间的相关性（如Spearman、Pearson或Kendall相关系数）。您可以使用 cor() 函数计算相关性矩阵。\n\nlibrary(Hmisc)\n# 计算Spearman相关性矩阵\ncor_matrix &lt;- rcorr(as.matrix(otu_table), type = \"spearman\")\nstr(cor_matrix)\n\nList of 4\n $ r   : num [1:153, 1:153] 1 0.909 0.881 0.575 0.489 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:153] \"ASV1\" \"ASV2\" \"ASV3\" \"ASV4\" ...\n  .. ..$ : chr [1:153] \"ASV1\" \"ASV2\" \"ASV3\" \"ASV4\" ...\n $ n   : int [1:153, 1:153] 19 19 19 19 19 19 19 19 19 19 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:153] \"ASV1\" \"ASV2\" \"ASV3\" \"ASV4\" ...\n  .. ..$ : chr [1:153] \"ASV1\" \"ASV2\" \"ASV3\" \"ASV4\" ...\n $ P   : num [1:153, 1:153] NA 7.32e-08 6.49e-07 9.94e-03 3.34e-02 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:153] \"ASV1\" \"ASV2\" \"ASV3\" \"ASV4\" ...\n  .. ..$ : chr [1:153] \"ASV1\" \"ASV2\" \"ASV3\" \"ASV4\" ...\n $ type: chr \"spearman\"\n - attr(*, \"class\")= chr \"rcorr\"\n\n\n\n\n16.2.5.3 筛选显著的相关性\n为了减少噪声，通常只保留显著的相关性（例如p值小于某个阈值）。可以使用 Hmisc 包中的 rcorr() 函数来计算相关性和p值。\n\n# 计算相关性和p值\ncor_test &lt;- rcorr(as.matrix(otu_table), type = \"spearman\")\ncor_values &lt;- cor_test$r  # 相关性矩阵\np_values &lt;- cor_test$P    # p值矩阵\n\n# 设置显著性阈值\np_threshold &lt;- 0.05\nsignificant_cor &lt;- ifelse(p_values &lt; p_threshold, cor_values, 0)\nstr(significant_cor)\n\n num [1:153, 1:153] NA 0.909 0.881 0.575 0.489 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:153] \"ASV1\" \"ASV2\" \"ASV3\" \"ASV4\" ...\n  ..$ : chr [1:153] \"ASV1\" \"ASV2\" \"ASV3\" \"ASV4\" ...\n\n\n\n\n16.2.5.4 构建邻接矩阵\n将显著的相关性转换为邻接矩阵（Adjacency Matrix），以便后续用于网络分析。可以设置一个相关性阈值（例如绝对值大于0.6）来进一步筛选强相关性。\n\n# 设置相关性阈值\ncor_threshold &lt;- 0.8\nadj_matrix &lt;- ifelse(abs(significant_cor) &gt; cor_threshold, significant_cor, 0)\ndiag(adj_matrix) &lt;- 0  # 对角线设为0，避免自相关\nstr(adj_matrix)\n\n num [1:153, 1:153] 0 0.909 0.881 0 0 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:153] \"ASV1\" \"ASV2\" \"ASV3\" \"ASV4\" ...\n  ..$ : chr [1:153] \"ASV1\" \"ASV2\" \"ASV3\" \"ASV4\" ...\n\n\n\n\n16.2.5.5 可视化共现网络\n使用 igraph 或 networkD3 等包可以可视化共现网络。\n\n\n16.2.5.6 使用 igraph 包\n\n# 加载igraph包\nlibrary(igraph)\n\n# 将邻接矩阵转换为图对象\ngraph &lt;- graph_from_adjacency_matrix(adj_matrix, mode = \"undirected\", weighted = TRUE)\n\n# 绘制网络图\nplot(graph,\n     vertex.size = 10,\n     vertex.color = \"lightblue\",\n     vertex.label.color = \"black\",\n     edge.width = E(graph)$weight * 5,  # 边权重影响宽度\n     main = \"Microbiome Co-occurrence Network\")\n\n\n\n\n\n\n\n\n\n\n\n16.2.6 进一步分析\n\n模块检测：可以使用 igraph 中的社区检测算法（如Louvain方法）来识别网络中的模块。\n网络属性：计算网络的拓扑属性（如节点度、聚类系数等）以深入了解微生物群落的结构。\n\n# 检测社区\ncommunities &lt;- cluster_louvain(graph)\nmembership &lt;- membership(communities)\nprint(membership)\n\n# 计算节点度\ndegrees &lt;- degree(graph)\nprint(degrees)\n以上代码提供了一个完整的微生物组数据分析流程框架，具体细节可以根据实际数据和研究需求调整。",
    "crumbs": [
      "微生物组数据分析",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>微生物组数据分析及可视化</span>"
    ]
  },
  {
    "objectID": "how-to-get-api-key.html",
    "href": "how-to-get-api-key.html",
    "title": "17  获取 API KEY",
    "section": "",
    "text": "17.1 获取阿里云百炼的 API KEY\n要获取阿里云百炼的 API Key，请按照以下步骤操作（详细图文步骤请参见官方文档）：",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>获取 API KEY</span>"
    ]
  },
  {
    "objectID": "how-to-get-api-key.html#获取阿里云百炼的-api-key",
    "href": "how-to-get-api-key.html#获取阿里云百炼的-api-key",
    "title": "17  获取 API KEY",
    "section": "",
    "text": "登录阿里云百炼大模型服务平台：使用您的阿里云账号登录 阿里云百炼大模型服务平台。\n开通百炼模型服务：如果登录后在页面顶部看到提示，要求开通百炼模型服务，请按照提示操作，以获得免费额度。开通百炼本身不产生费用，只有在调用、部署或调优模型时，超出免费额度部分才会产生相应费用。\n进入 API Key 管理界面：将鼠标悬停在页面右上角的头像图标上，在下拉菜单中点击“API-KEY”。在左侧导航栏中，选择“全部API-KEY”或“我的API-KEY”。主账号可以查看和管理所有子账号的 API Key，子账号仅能查看和管理自己的 API Key。\n创建或查看 API Key：在对应的页面中，点击“创建”按钮以生成新的 API Key，或在已有的 API Key 操作列中点击“查看”以获取现有的 API Key。请注意，删除 API Key 后，使用该 Key 的服务将无法继续访问百炼大模型提供的各项服务。",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>获取 API KEY</span>"
    ]
  },
  {
    "objectID": "how-to-get-api-key.html#获取-deepseek-的-api-key",
    "href": "how-to-get-api-key.html#获取-deepseek-的-api-key",
    "title": "17  获取 API KEY",
    "section": "17.2 获取 DeepSeek 的 API KEY",
    "text": "17.2 获取 DeepSeek 的 API KEY\n要获取 DeepSeek 的 API Key，请按照以下步骤操作：\n\n注册或登录 DeepSeek 平台：\n\n访问 DeepSeek 官方网站。\n如果您已有账号，请直接登录；如果没有，请按照指引注册新账号。\n\n创建 API Key：\n\n登录后，点击左侧导航栏中的“API Keys”选项。\n在“API Keys”页面，点击“创建 API Key”按钮。\n为您的 API Key 输入一个描述性的名称，以便日后管理。\n创建后，系统会生成一个唯一的 API Key。请务必立即复制并妥善保存，因为出于安全原因，您将无法在平台上再次查看该密钥。",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>获取 API KEY</span>"
    ]
  },
  {
    "objectID": "how-to-get-api-key.html#获取-chatgpt-的-api-key",
    "href": "how-to-get-api-key.html#获取-chatgpt-的-api-key",
    "title": "17  获取 API KEY",
    "section": "17.3 获取 ChatGPT 的 API KEY",
    "text": "17.3 获取 ChatGPT 的 API KEY\nChatGPT 官方服务中国地区不可用，这里介绍 ChatAnyWhere 接口 API KEY 的办法。要获取 ChatAnyWhere 的 API Key，请按照以下步骤操作：\n\n🚀申请领取内测免费API Key\n免费版支持gpt-3.5-turbo, embedding, gpt-4o-mini, gpt-4。其中gpt-4由于价格过高，每天限制3次调用（0点刷新）。需要更稳定快速的gpt-4请使用付费版。\n免费版gpt-4由gpt-4o提供服务，但免费版暂不支持识图。\n转发Host1: https://api.chatanywhere.tech (国内中转，延时更低)\n转发Host2: https://api.chatanywhere.org (国外使用)\n\n付费版API\n\n纯公益提供免费Key显然不是能持久运营下去的方案，所以我们引入付费API Key维持项目的日常开销，以促进项目的良性循环，还望大家理解。\n购买付费Key\n付费版价格表\n\n\n支持更稳定更快速的GPT4 API，GPT4体验更好，无限使用，价格低于官方，充值更便捷。\n同官网计费策略，流式问答使用tiktoken库准确计算Tokens，非流式问答直接使用官方返回Tokens用量计费。\n余额不会过期，永久有效。根据用户反馈30块钱个人中度使用gpt-4o-mini估计能用半年。\n所有的接口（包括免费版本）都保证转发自OpenAI或Azure官方接口，非peo、plus等不稳定方案或逆向方案，无水分，不掺假，保证稳定性。\n\n付费版支持更多主流AI模，型包括 GPT-4、GPT-4o、o1-preview 及其变种，以及 Claude、Gemini 等第三方模型，均支持自然语言处理任务。\nGPT-4o-mini系列价格较低，适用于一般任务，而 GPT-4o 系列在性能、速度和多模态能力上更优，支持图片输入并提供高性价比。GPT-4-Turbo 提供 128K 上下文窗口，适用于长文本处理。Claude 3.5、Gemini 1.5 等模型由第三方提供，适用于复杂推理任务，但可能存在稳定性问题。\n此外，DALL-E 3 支持高质量图像生成，TTS-1 用于文本转语音，Whisper 则用于语音识别。OpenAI 的 embedding 模型如 text-embedding-3 适用于文本向量化任务。",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>获取 API KEY</span>"
    ]
  },
  {
    "objectID": "how-to-get-api-key.html#获取-huggingface-的-api-key",
    "href": "how-to-get-api-key.html#获取-huggingface-的-api-key",
    "title": "17  获取 API KEY",
    "section": "17.4 获取 Huggingface 的 API KEY",
    "text": "17.4 获取 Huggingface 的 API KEY\n获取 Hugging Face 的 API Key 步骤如下：\n\n登录 Hugging Face\n访问 Hugging Face 官网 并登录你的账户。如果没有账户，需要先注册。\n进入 API Key 页面\n点击右上角的个人头像，选择 “Settings”（设置），然后在左侧菜单中找到 “Access Tokens”。\n生成 API Key\n在 Access Tokens 页面，点击 “New token”（新建 Token），输入 Token 名称，并选择权限（一般选择 “Write” 或 “Read”）。\n复制 API Key\n生成后，复制 API Key 并妥善保存，因为它只会显示一次。",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>获取 API KEY</span>"
    ]
  },
  {
    "objectID": "how-to-get-api-key.html#使用注意事项",
    "href": "how-to-get-api-key.html#使用注意事项",
    "title": "17  获取 API KEY",
    "section": "17.5 使用注意事项",
    "text": "17.5 使用注意事项\n\n保密：请勿以任何方式公开您的 API Key，以避免未经授权的使用带来安全风险或资金损失。API Key 是重要的鉴权凭证，请妥善保管（特别有些 API KEY 只在生成时显示一次）。\n获取 API Key 后，建议将其配置到环境变量中，以便在调用模型或应用时使用，从而降低 API Key 泄漏的风险。详细操作请参考 配置 API Key 到环境变量。\n\n\n17.5.1 在 Linux/MacOS 系统中配置环境变量\n\n17.5.1.1 临时设置（当前终端会话有效）\n在终端中运行以下命令（以 Hugging Face 为例）：\nexport HF_API_KEY=\"your_huggingface_api_key\"\n这只会在当前终端会话中生效，关闭终端后失效。\n\n\n17.5.1.2 永久设置（适用于所有终端）\n编辑 ~/.bashrc（适用于 Bash）或 ~/.zshrc（适用于 Zsh）：\necho 'export HF_API_KEY=\"your_huggingface_api_key\"' &gt;&gt; ~/.zshrc\nsource ~/.zshrc  # 使更改生效\n其他 API（如阿里云百炼、ChatAnywhere、DeepSeek）类似：\necho 'export BAILIAN_API_KEY=\"your_bailian_api_key\"' &gt;&gt; ~/.zshrc\necho 'export CHATANYWHERE_API_KEY=\"your_chatanywhere_api_key\"' &gt;&gt; ~/.zshrc\necho 'export DEEPSEEK_API_KEY=\"your_deepseek_api_key\"' &gt;&gt; ~/.zshrc\nsource ~/.zshrc\n\n\n\n17.5.2 在 Windows 系统中配置环境变量\n\n17.5.2.1 临时设置（当前终端会话有效）\n在命令提示符（CMD）中运行：\nset HF_API_KEY=your_huggingface_api_key\n在 PowerShell 中运行：\n$env:HF_API_KEY=\"your_huggingface_api_key\"\n\n\n17.5.2.2 永久设置（适用于所有终端）\n\n打开 “环境变量”（Win + R，输入 sysdm.cpl，然后选择 “高级” -&gt; “环境变量”）。\n\n在 “系统变量” 或 “用户变量” 中点击 “新建”，输入变量名 HF_API_KEY，变量值 your_huggingface_api_key。\n\n点击 “确定”，然后重新启动终端。\n\n其他 API 配置方式相同，替换 HF_API_KEY 为相应的 API 变量名。\n\n\n\n17.5.3 在 R 中配置环境变量\n\n17.5.3.1 临时设置（当前 R 会话有效）\nSys.setenv(HF_API_KEY = \"your_huggingface_api_key\")\n\n\n17.5.3.2 永久设置（适用于所有 R 会话）\n编辑 ~/.Renviron 文件（Linux/MacOS）或 C:\\Users\\你的用户名\\Documents\\.Renviron（Windows），添加：\nHF_API_KEY=your_huggingface_api_key\n保存后，重新启动 R。\n\n\n\n17.5.4 在 Python 中配置环境变量\n\n17.5.4.1 临时设置（当前 Python 进程有效）\nimport os\nos.environ[\"HF_API_KEY\"] = \"your_huggingface_api_key\"\n\n\n17.5.4.2 永久设置（适用于所有 Python 进程）\n\n方法 1：写入 shell 配置文件（Linux/MacOS）\necho 'export HF_API_KEY=\"your_huggingface_api_key\"' &gt;&gt; ~/.zshrc\nsource ~/.zshrc\n方法 2：写入 .env 文件（适用于 python-dotenv 库）\n创建 .env 文件，添加：\nHF_API_KEY=your_huggingface_api_key\n然后在 Python 代码中加载：\nfrom dotenv import load_dotenv\nload_dotenv()  # 自动加载 .env 文件\nimport os\nprint(os.getenv(\"HF_API_KEY\"))\n\n其他 API 的配置方式相同，替换 HF_API_KEY 为 BAILIAN_API_KEY、CHATANYWHERE_API_KEY、DEEPSEEK_API_KEY 等。",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>获取 API KEY</span>"
    ]
  },
  {
    "objectID": "reading-paper.html",
    "href": "reading-paper.html",
    "title": "18  AI 批量读文献",
    "section": "",
    "text": "18.1 批处理程序",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>AI 批量读文献</span>"
    ]
  },
  {
    "objectID": "reading-paper.html#批处理程序",
    "href": "reading-paper.html#批处理程序",
    "title": "18  AI 批量读文献",
    "section": "",
    "text": "18.1.1 配置客户端和缓存\n\nimport os\nimport json\nfrom pathlib import Path\nfrom datetime import datetime\nfrom openai import OpenAI\n\n# 初始化百炼客户端\nclient = OpenAI(\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n)\n\n# 配置路径\nPDF_BASE_DIR = Path(\"paper\")\nCACHE_DIR = Path(\"output/data-processed/paper-cache/\")\nFILEID_CACHE_DIR = Path(\"output/data-processed/fileid-cache/\")  # 新增fileid缓存目录\nCACHE_DIR.mkdir(parents=True, exist_ok=True)\nFILEID_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n\n\n\n18.1.2 设定提示词\n\n# 提示词模板\nPROMPT_TEMPLATE = \"\"\"请按照以下步骤对英文文献进行深入解读和分析，确保结果逻辑清晰、内容全面：\n\n### 基本信息提取\n\n提取文章标题、作者、通讯作者的单位（附中文翻译）、发表年份、期刊名称等关键信息。\n\n### 研究背景\n\n- 总结文献的研究背景，说明研究所解决的问题或提出的假设。\n- 明确指出作者的研究目的和研究动机。\n\n### 研究结论\n\n- 概述文章的核心发现和关键数据。\n- 对图表、统计数据和实验结果进行总结和分析。\n- 强调研究结果对原始问题的解答和新发现。\n\n### 核心创新点\n\n- 指出文献在理论、方法或实践方面的创新与独特贡献。\n- 讨论该研究如何推动领域的发展，及其实际应用意义。\n\n### 实验设计\n\n- 指出研究使用的样品来源，材料出处等。\n- 描述作者采用的研究方法（如实验、调查、建模、定量/定性分析等）。\n- 解释数据来源、采集方式以及实验设计或分析框架。\n\n### 讨论\n\n分析作者如何讨论结果及其对研究领域的影响，并指出研究局限性、未解决的问题或作者提出的未来研究方向。\n\n### 产业转化可行性\n\n评估研究在产业转化上的前景并给出理由。\n\n### 结论\n\n最后，用一句话讲清楚研究的重要发现及意义。\n\n\n请确保在解读过程中：\n\n- 语言表达准确、逻辑清晰；\n- 分析内容既关注整体框架也注意细节；\n- 引用和解释关键概念和数据时要做到充分且有条理。\n\n注意：在输出列表的时候，需要再列表头与列表项之间加入两个空行（换行符），否则Quarto渲染时候会出错。\n\"\"\"\n\n\n\n18.1.3 编写自定义函数\n\ndef get_pdf_files():\n    \"\"\"获取paper目录下的所有pdf文件\"\"\"\n    pdf_files = []\n    for file in PDF_BASE_DIR.glob(\"*.pdf\"):\n        pdf_files.append({\n            \"ID\": file.stem,\n            \"pdf_path\": str(file)\n        })\n    return pdf_files\n\ndef process_paper(entry):\n    \"\"\"处理单个文献条目\"\"\"\n    cache_file = CACHE_DIR / f\"{entry['ID']}.json\"\n    fileid_cache_file = FILEID_CACHE_DIR / f\"{entry['ID']}.txt\"\n    \n    # 检查处理结果缓存\n    if cache_file.exists():\n        with open(cache_file, 'r', encoding='utf-8') as f:\n            return json.load(f)\n    \n    # 检查PDF文件存在性\n    pdf_path = Path(entry['pdf_path'])\n    print(f\"正在检查PDF文件路径: {pdf_path}\")  # 调试日志\n    if not pdf_path.exists():\n        print(f\"警告：跳过未找到的PDF文件: {pdf_path}\")\n        return None\n    \n    try:\n        # 检查文件ID缓存\n        if fileid_cache_file.exists():\n            with open(fileid_cache_file, 'r', encoding='utf-8') as f:\n                file_id = f.read().strip()\n        else:\n            # 上传文件并缓存ID\n            file_object = client.files.create(\n                file=Path(entry['pdf_path']),\n                purpose=\"file-extract\"\n            )\n            file_id = file_object.id\n            with open(fileid_cache_file, 'w', encoding='utf-8') as f:\n                f.write(file_id)\n        \n        completion = client.chat.completions.create(\n            model=\"qwen-long\",\n            messages=[\n                {'role': 'system', 'content': f'fileid://{file_id}'},\n                {'role': 'user', 'content': PROMPT_TEMPLATE}\n            ],\n            temperature=0.2\n        )\n        \n        # 解析结果并缓存\n        result = {\n            \"id\": entry['ID'],\n            \"title\": entry['pdf_path'],\n            \"content\": completion.choices[0].message.content,\n            \"processed_at\": datetime.now().isoformat()\n        }\n        \n        with open(cache_file, 'w', encoding='utf-8') as f:\n            json.dump(result, f, ensure_ascii=False, indent=2)\n            \n        return result\n    \n    except Exception as e:\n        print(f\"处理文献 {entry['ID']} 时出错: {str(e)}\")\n        # 清理可能不完整的缓存\n        if 'file_id' in locals() and not fileid_cache_file.exists():\n            fileid_cache_file.unlink(missing_ok=True)\n        return None\n\ndef generate_markdown(result):\n    \"\"\"生成Markdown格式的报告\"\"\"\n    if not result:\n        return \"\"\n    \n    md_content = f\"\"\"\n## {result['title']}\n\n{result['content']}\n\n---\n\"\"\"\n    return md_content\n\n\n# 主处理流程\nentries = get_pdf_files()\n\nlen(entries)\n\n3\n\n\n\nall_results = []\nfor entry in entries:\n    result = process_paper(entry)\n    if result:\n        all_results.append(result)\n\n\n# 生成最终报告\noutput = \"\\n\".join([generate_markdown(r) for r in all_results])",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>AI 批量读文献</span>"
    ]
  },
  {
    "objectID": "reading-paper.html#输出报告",
    "href": "reading-paper.html#输出报告",
    "title": "18  AI 批量读文献",
    "section": "18.2 输出报告",
    "text": "18.2 输出报告",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>AI 批量读文献</span>"
    ]
  },
  {
    "objectID": "reading-paper.html#papergao-et-al_2021_emergent-transcriptional-adaption-facilitates-convergent-succession-within-a.pdf",
    "href": "reading-paper.html#papergao-et-al_2021_emergent-transcriptional-adaption-facilitates-convergent-succession-within-a.pdf",
    "title": "18  AI 批量读文献",
    "section": "18.3 paper/Gao et al_2021_Emergent transcriptional adaption facilitates convergent succession within a.pdf",
    "text": "18.3 paper/Gao et al_2021_Emergent transcriptional adaption facilitates convergent succession within a.pdf\n\n18.3.1 基本信息提取\n文章标题：\nEmergent transcriptional adaption facilitates convergent succession within a synthetic community\n作者：\nChun-Hui Gao, Hui Cao, Feng Ju, Ke-Qing Xiao, Peng Cai, Yichao Wu, Qiaoyun Huang\n通讯作者单位：\nCollege of Resources and Environment, Huazhong Agricultural University, 武汉, 中国 (华中农业大学资源与环境学院)\n发表年份：\n2021\n期刊名称：\nISME Communications\n\n\n\n18.3.2 研究背景\n文献的研究背景主要集中在微生物群落的趋同演化及其分子机制。尽管趋同现象在自然界的细菌群落中普遍存在，但其背后的分子机制尚不明确。为此，作者通过构建一个由两种模式微生物（大肠杆菌K-12和铜绿假单胞菌KT2440）组成的合成群落，在封闭的培养系统中进行了时间序列转录组分析。研究旨在探讨物种间的相互作用如何影响基因表达，并揭示这些变化对群落结构和功能的影响。\n\n\n\n18.3.3 研究结论\n文章的核心发现包括：\n\n“0 h效应”：基因表达的变化在培养初期就已经开始，表明物种间的相互作用不可避免地影响了基因表达。\n“群体效应”：多数物种对少数物种的基因表达有更大的影响。\n严格的时间和初始结构调控：基因表达受到时间和初始结构的严格调控，特别是在共培养条件下，许多代谢途径被抑制，而少数途径被激活或条件性表达。\n\n通过对基因表达的分析，作者发现，在24小时后，E. coli 和 P. putida 的基因表达趋于一致，这表明基因表达的变化是群落趋同的基础。\n\n\n\n18.3.4 核心创新点\n该研究在以下几个方面具有创新性和独特贡献：\n\n首次揭示了基因表达变化在群落趋同中的作用：通过时间序列转录组分析，作者证明了基因表达的变化是群落结构趋同的关键驱动因素。\n提出了“0 h效应”和“群体效应”：这两个概念为理解物种间相互作用提供了新的视角，尤其是在早期阶段的相互作用对基因表达的影响。\n揭示了代谢途径的调控机制：研究表明，大多数代谢途径在共培养条件下被抑制，而少数途径被激活或条件性表达，这为理解微生物群落的功能调节提供了新的见解。\n\n\n\n\n18.3.5 实验设计\n样品来源和材料：\n研究使用了两种模式微生物——大肠杆菌K-12（Escherichia coli K-12）和铜绿假单胞菌KT2440（Pseudomonas putida KT2440）。实验中设置了三种不同的初始比例（1:1000、1:1、1000:1），并在封闭的培养系统中进行共培养。\n研究方法：\n1. 时间序列转录组分析：通过高通量mRNA测序（RNA-seq）分析不同时间点的基因表达变化。 2. 定量PCR监测：使用物种特异性定量PCR监测细菌生长情况。 3. 数据分析：使用DESeq2进行差异表达基因（DEGs）的鉴定，并通过基因集富集分析（GSEA）揭示代谢途径的变化。\n数据来源和采集方式：\n所有样本均在0、0.5、1、2、4、8和24小时采集，每个时间点至少重复三次。RNA-seq数据已存入Sequence Read Archive（SRA）数据库，qPCR原始数据及相关代码已存入GitHub。\n\n\n\n18.3.6 讨论\n作者通过实验结果讨论了基因表达变化对群落趋同的影响，并指出了以下几点：\n\n基因表达变化是群落趋同的基础：研究结果表明，基因表达的变化发生在群落结构变化之前，因此可以推断基因表达的变化是群落趋同的诱导因素。\n物种间相互作用的复杂性：即使在细胞数量相近的情况下，物种间的相互作用仍然显著影响基因表达，这表明微生物之间的相互作用是不可避免的。\n代谢途径的调控机制：大多数代谢途径在共培养条件下被抑制，这可能反映了微生物在资源有限的情况下优化能量利用的策略。\n\n然而，研究也存在一些局限性，例如实验是在封闭的培养系统中进行的，可能无法完全模拟自然环境中的复杂条件。此外，研究仅涉及两种微生物，未来的研究可以扩展到更多种类的微生物群落。\n\n\n\n18.3.7 产业转化可行性\n该研究在产业转化上具有一定的前景。首先，研究揭示了微生物群落趋同的分子机制，这对于开发新型生物技术（如合成生物学、生物修复等）具有重要意义。其次，通过理解基因表达的变化，可以更好地设计和优化微生物群落的功能，从而应用于工业发酵、环境保护等领域。此外，研究结果还可以为微生物生态学的基础研究提供新的思路，推动相关领域的进一步发展。\n\n\n\n18.3.8 结论\n该研究通过时间序列转录组分析揭示了基因表达变化在微生物群落趋同中的重要作用，提出了“0 h效应”和“群体效应”等新概念，为理解微生物群落的生态和进化机制提供了新的视角。",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>AI 批量读文献</span>"
    ]
  },
  {
    "objectID": "reading-paper.html#papergao-et-al_2021_the-initial-inoculation-ratio-regulates-bacterial-coculture-interactions-and.pdf",
    "href": "reading-paper.html#papergao-et-al_2021_the-initial-inoculation-ratio-regulates-bacterial-coculture-interactions-and.pdf",
    "title": "18  AI 批量读文献",
    "section": "18.4 paper/Gao et al_2021_The initial inoculation ratio regulates bacterial coculture interactions and.pdf",
    "text": "18.4 paper/Gao et al_2021_The initial inoculation ratio regulates bacterial coculture interactions and.pdf\n\n18.4.1 基本信息提取\n文章标题：The initial inoculation ratio regulates bacterial coculture interactions and metabolic capacity\n作者：Chun-Hui Gao, Hui Cao, Peng Cai, Søren J. Sørensen\n通讯作者单位：\n- 华中农业大学农业微生物学国家重点实验室，资源与环境学院，中国武汉（华中农业大学）\n- 哥本哈根大学生物系微生物学部，丹麦哥本哈根（哥本哈根大学）\n发表年份：2020\n期刊名称：The ISME Journal\n\n\n18.4.2 研究背景\n\n18.4.2.1 研究背景概述\n微生物共培养系统是微生物生态学研究中的重要模型系统。初始接种比例作为关键实验参数，对共培养系统的结构和功能有着至关重要的影响。然而，这种影响从未在多种生态位条件下进行过系统研究。本文旨在探讨不同初始接种比例对细菌共培养系统在不同碳源条件下的群落结构、功能和细菌相互作用的影响。\n\n\n18.4.2.2 研究目的和动机\n作者希望通过这项研究揭示初始接种比例如何调节细菌共培养系统的代谢能力和相互作用模式，从而为微生物生态学研究提供新的视角，并提高共培养实验的可重复性和预测性。\n\n\n\n18.4.3 研究结论\n\n18.4.3.1 核心发现和关键数据\n\n最终比例依赖于初始接种比例：在大约五分之六的碳源中，不同初始接种比例的共培养系统的最终比例存在显著差异，表明最终比例高度依赖于初始接种比例。\n初始比例调节代谢能力：只有初始比例为1:1和1000:1的共培养系统在14种特定碳源上表现出高代谢能力，这可能是由于初始比例改变了物种间的相互作用模式。\n碳源偏好无法预测最终比例：细菌对碳源的偏好并不能预测共培养系统的最终比例。\n\n\n\n18.4.3.2 图表和实验结果分析\n\n图2展示了不同碳源下三种共培养系统的最终比例分布，表明初始比例对最终比例有显著影响。\n图3显示了不同碳源偏好下共培养系统的最终比例变化，强调了碳源偏好对相对丰度的影响。\n图4展示了不同初始比例的共培养系统在71种碳源上的代谢能力（CUE）差异，突出了1:1和1000:1比例在某些碳源上的高效利用。\n\n\n\n\n18.4.4 核心创新点\n\n18.4.4.1 理论和方法创新\n\n首次系统研究：这是首次系统地研究初始接种比例在多种培养条件下对共培养系统的影响。\n引入代谢耦合概念：通过实验验证了初始比例如何影响代谢耦合的建立，特别是在14种特定碳源上的协同作用。\n多因素综合分析：结合碳源偏好、初始比例和代谢能力，提出了一个综合模型来解释共培养系统的动态变化。\n\n\n\n18.4.4.2 推动领域发展\n该研究不仅揭示了初始接种比例对共培养系统结构和功能的调控机制，还为微生物生态学研究提供了新的理论框架和实验方法，有助于更好地理解和预测复杂微生物群落的行为。\n\n\n\n18.4.5 实验设计\n\n18.4.5.1 样品来源和材料出处\n\n使用了两种常见的模式菌株：大肠杆菌K-12（EC）和假单胞菌KT2440（PP），这些菌株广泛存在于土壤、水体和宿主相关环境中。\n实验使用了Biolog GEN III微孔板，包含71种不同的碳源。\n\n\n\n18.4.5.2 研究方法\n\n实验设计：建立了两物种共培养系统，初始比例分别为1:1000、1:1和1000:1，在71种不同碳源中进行培养。\n数据分析：通过Biolog MicroStation机器测量碳氧化效率（CUE），并使用qPCR定量分析共培养系统中各物种的相对丰度。\n统计分析：使用R软件进行多元线性回归分析，揭示关键参数之间的关联。\n\n\n\n\n18.4.6 讨论\n\n18.4.6.1 结果讨论及其影响\n作者讨论了初始接种比例对细菌相互作用和代谢能力的调控机制，强调了碳源偏好和初始比例的协同作用。研究结果表明，初始比例不仅影响共培养实验的可重复性，还对理解通用微生物生态学具有重要意义。\n\n\n18.4.6.2 研究局限性和未来方向\n\n局限性：研究主要集中在两种模式菌株和71种碳源，未来可以扩展到更多种类的菌株和更复杂的生态位条件。\n未来方向：进一步探索初始比例与其他环境因素（如温度、pH值）的交互作用，以更全面地理解微生物群落的动态变化。\n\n\n\n\n18.4.7 产业转化可行性\n该研究在工业微生物发酵、生物修复和合成生物学等领域具有潜在的应用价值。通过优化初始接种比例，可以提高共培养系统的代谢效率和稳定性，从而提升生产效率和产品质量。\n\n\n18.4.8 结论\n该研究揭示了初始接种比例对细菌共培养系统结构和功能的调控机制，特别是其对代谢能力和物种相互作用的影响，为微生物生态学研究提供了新的理论框架和实验方法。",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>AI 批量读文献</span>"
    ]
  },
  {
    "objectID": "reading-paper.html#paperho-et-al.---2019---rapid-identification-of-pathogenic-bacteria-using-raman-spectroscopy-and-deep-learning.pdf",
    "href": "reading-paper.html#paperho-et-al.---2019---rapid-identification-of-pathogenic-bacteria-using-raman-spectroscopy-and-deep-learning.pdf",
    "title": "18  AI 批量读文献",
    "section": "18.5 paper/Ho et al. - 2019 - Rapid identification of pathogenic bacteria using Raman spectroscopy and deep learning.pdf",
    "text": "18.5 paper/Ho et al. - 2019 - Rapid identification of pathogenic bacteria using Raman spectroscopy and deep learning.pdf\n\n18.5.1 基本信息提取\n文章标题：\nRapid identification of pathogenic bacteria using Raman spectroscopy and deep learning\n作者：\nChi-Sing Ho, Neal Jean, Catherine A. Hogan, Lena Blackmon, Stefanie S. Jeffrey, Mark Holodniy, Niaz Banaei, Amr A.E. Saleh, Stefano Ermon, Jennifer Dionne\n通讯作者单位：\n- 斯坦福大学应用物理系（Stanford University, Dept. of Applied Physics） - 斯坦福大学材料科学与工程系（Stanford University, Dept. of Materials Science and Engineering） - 斯坦福大学计算机科学系（Stanford University, Dept. of Computer Science） - 斯坦福大学电气工程系（Stanford University, Dept. of Electrical Engineering） - 斯坦福大学医学院病理学系（Stanford University School of Medicine, Dept. of Pathology） - 斯坦福健康护理临床微生物实验室（Stanford Health Care, Clinical Microbiology Laboratory） - 斯坦福大学医学院外科系（Stanford University School of Medicine, Dept. of Surgery） - 斯坦福大学医学院医学系（Stanford University School of Medicine, Dept. of Medicine） - 退伍军人事务部帕洛阿尔托医疗保健系统（VA Palo Alto Health Care System） - 斯坦福大学医学院传染病与地理医学科（Stanford University School of Medicine, Division of Infectious Diseases and Geographic Medicine） - 开罗大学工程学院工程数学与物理系（Cairo University, Faculty of Engineering, Dept. of Engineering Mathematics and Physics）\n发表年份：\n2019\n期刊名称：\nNature Communications\n\n\n18.5.2 研究背景\n该研究旨在解决细菌感染诊断中的速度和准确性问题。当前的诊断方法依赖于样本培养，这不仅耗时，而且可能导致不必要的广谱抗生素使用。研究提出了一种结合拉曼光谱和深度学习的新方法，以实现快速、无培养的病原体识别和抗生素敏感性测试。这种方法可以显著缩短诊断时间，提高治疗效果，并有助于减少抗生素滥用。\n\n\n18.5.3 研究结论\n文章的核心发现包括：\n\n使用深度学习模型（卷积神经网络，CNN）对30种常见病原菌进行了分类，平均分离水平准确率达到82.2±0.3%。\n在低信噪比（SNR=4.1）的情况下，仍能保持较高的分类精度。\n对经验性治疗的识别准确率高达97.0±0.3%，显著优于传统的逻辑回归和支持向量机（SVM）方法。\n成功区分了耐甲氧西林金黄色葡萄球菌（MRSA）和敏感株（MSSA），准确率为89.1±0.1%。\n在临床样本中，仅使用每个患者隔离物的10个光谱，即可达到99.7%的治疗识别准确率。\n\n这些结果表明，该方法在快速、准确地识别病原体和指导抗生素选择方面具有巨大潜力。\n\n\n18.5.4 核心创新点\n该研究在以下几个方面具有创新性和独特贡献：\n\n理论创新：首次将深度学习应用于低信噪比的拉曼光谱数据处理，解决了传统方法难以处理的高噪声问题。\n方法创新：开发了一种基于残差连接的1D卷积神经网络架构，能够有效保留光谱峰值位置，提高了分类性能。\n实践意义：该方法无需样本培养，可在数小时内完成病原体识别和抗生素敏感性测试，为临床快速诊断提供了新的工具。\n\n这项研究推动了拉曼光谱技术在微生物诊断中的应用，有望改善感染性疾病患者的治疗效果。\n\n\n18.5.5 实验设计\n样品来源和材料：\n- 研究使用了来自斯坦福医院的30种细菌和酵母分离物，涵盖了超过94%的常见感染病原体。 - 还包括从临床患者样本中获得的额外12,000个光谱，用于验证模型的泛化能力。\n研究方法：\n- 光谱采集：通过短时间测量（1秒）获取干燥单层样品的拉曼光谱，确保大多数光谱来自单个细胞。 - 数据预处理：对光谱进行背景校正，使用多项式拟合去除背景噪声。 - 模型训练：使用参考数据集训练CNN模型，然后在临床数据集上进行微调，以适应不同样本条件的变化。 - 性能评估：通过混淆矩阵、ROC曲线等指标评估模型的分类性能，并与传统方法进行比较。\n\n\n18.5.6 讨论\n作者讨论了研究结果对微生物诊断领域的潜在影响：\n\n优势：该方法能够在短时间内提供高精度的病原体识别和抗生素敏感性测试，有助于早期针对性治疗，减少抗生素滥用。\n局限性：尽管取得了显著进展，但该方法仍需进一步优化以应对更多种类的病原体和复杂的临床环境。\n未来研究方向：建议扩大数据集，涵盖更多耐药性和敏感性的临床分离物，以提高模型的鲁棒性和泛化能力。此外，还需探索将该方法应用于其他生物流体（如全血、痰液、尿液）的可行性。\n\n\n\n18.5.7 产业转化可行性\n该研究在产业转化方面具有广阔前景：\n\n快速诊断：该方法可以在数小时内完成病原体识别和抗生素敏感性测试，显著缩短诊断时间，提高临床效率。\n自动化潜力：结合高度自动化的样本制备和数据分析系统，该平台可以实现大规模临床应用，降低医疗成本。\n广泛适用性：由于不需要特殊标签，该方法易于推广到新的病原体种类，具有广泛的临床应用价值。\n\n\n\n18.5.8 结论\n该研究展示了结合拉曼光谱和深度学习在快速、无培养的病原体识别和抗生素敏感性测试中的巨大潜力，为临床快速诊断提供了新的高效工具。",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>AI 批量读文献</span>"
    ]
  },
  {
    "objectID": "bailian.html",
    "href": "bailian.html",
    "title": "19  阿里云百炼",
    "section": "",
    "text": "19.1 百炼模型\n百炼提供了丰富多样的模型选择，它集成了通义系列大模型和第三方大模型，涵盖文本、图像、音视频等不同模态。\n参见：https://help.aliyun.com/zh/model-studio/getting-started/models?",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>阿里云百炼</span>"
    ]
  },
  {
    "objectID": "bailian.html#调用百炼模型",
    "href": "bailian.html#调用百炼模型",
    "title": "19  阿里云百炼",
    "section": "19.2 调用百炼模型",
    "text": "19.2 调用百炼模型\n您可以使用 OpenAI Python SDK、DashScope SDK 或 HTTP 接口调用通义千问模型。\n\n19.2.1 使用 OpenAI SDK\n下面使用 OpenAI SDK 调用百炼平台的通义千问 Max 模型。通义千问 Max 模型是通义千问系列模型中性能最强的模型，支持中文、英文等不同语言输入。2025年2月，三方基准测试平台LMArena1的大语言模型盲测榜单（“ChatBot Arena LLM”）最新排名显示，“Qwen2.5-Max”以1332分排总榜第7名，超过了深度求索的“DeepSeek-V3”以及OpenAI的“o1-mini”。而在数学和编程方面，“Qwen2.5-Max”则排名第1，在Hard prompts方面排名第2。\n\nfrom openai import OpenAI\nimport os\nfrom IPython.display import Markdown\n\n# 创建 client\nclient = OpenAI(\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"), # 如果您没有配置环境变量，请在此处用您的API Key进行替换\n    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",  # 填写DashScope服务的base_url\n)\n\n# 生成对话\ncompletion = client.chat.completions.create(\n    model=\"qwen-max\",\n    messages=[\n        {'role': 'system', 'content': 'You are a helpful assistant.'},\n        {'role': 'user', 'content': '你跟ChatGPT和DeepSeek相比，谁更强？'}],\n    temperature=0.8,\n    top_p=0.8\n    )\n\nMarkdown(completion.choices[0].message.content)\n\n每个AI助手都有其独特的优势和特点，ChatGPT、DeepSeek以及我（Qwen）都是基于不同的技术和训练数据开发出来的。我们各自擅长处理不同类型的任务，在自然语言理解、生成、对话流畅性等方面各有千秋。具体到某个方面谁“更强”，这通常取决于特定应用场景和个人偏好。例如，有些用户可能更喜欢某一模型对于特定话题的理解能力或回复风格。\n作为来自阿里云的大型语言模型，Qwen致力于提供高质量的信息服务与支持，不断学习进步以更好地满足用户需求。如果您有任何问题或需要帮助，请随时告诉我！这样也可以让您直接体验到我的能力和特色。\n\n\n\n\n19.2.2 使用 DashScope SDK\n使用 DashScope SDK 时，会自动读取环境变量中的 API_KEY 并创建对象。\n\n# Refer to the document for workspace information: https://help.aliyun.com/document_detail/2746874.html    \n        \nfrom http import HTTPStatus\nimport dashscope\n\nmessages = [{'role': 'user', 'content': '你跟ChatGPT和DeepSeek相比，谁更强？'}]\nresponse = dashscope.Generation.call(\"qwen-turbo\",\n                            messages=messages,\n                            result_format='message',  # set the result to be \"message\"  format.\n                            stream=False, # set streaming output\n                            )\n\nMarkdown(response.output.choices[0].message.content)\n\n作为阿里云开发的预训练语言模型通义千问，我与ChatGPT、DeepSeek一样，都是旨在理解和生成人类语言的AI助手。我们各自有独特的能力和优势，可以根据不同的场景和需求来提供帮助。与其他AI助手相比，我更了解中文语言习惯和文化背景，也能够更好地为中国用户提供服务。如果您有任何问题或需要帮助，请随时告诉我，我会尽力提供支持。",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>阿里云百炼</span>"
    ]
  },
  {
    "objectID": "bailian.html#视觉推理",
    "href": "bailian.html#视觉推理",
    "title": "19  阿里云百炼",
    "section": "19.3 视觉推理",
    "text": "19.3 视觉推理\nQwen-VL(qwen-vl-plus/qwen-vl-max) 模型现有几大特点：\n\n大幅增强了图片中文字处理能力，能够成为生产力小帮手，提取、整理、总结文字信息不在话下。\n增加可处理分辨率范围，各分辨率和长宽比的图都能处理，大图和长图能看清。\n增强视觉推理和决策能力，适于搭建视觉Agent，让大模型Agent的想象力进一步扩展。\n升级看图做题能力，拍一拍习题图发给Qwen-VL，大模型能帮用户一步步解题。\n\n\n\nfrom http import HTTPStatus\nimport dashscope\nfrom IPython.display import Markdown\n\n# 定义一个简单的多模态对话调用函数\ndef simple_multimodal_conversation_call():\n    \"\"\"Simple single round multimodal conversation call.\n    \"\"\"\n    messages = [\n        {\n            \"role\": \"user\",\n            \n            # 使用一个列表来传递图片地址和提示词\n            \"content\": [\n                {\"image\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg\"},\n                {\"text\": \"这是什么?\"}\n            ]\n        }\n    ]\n    response = dashscope.MultiModalConversation.call(model='qwen-vl-plus',\n                                                     messages=messages)\n    # The response status_code is HTTPStatus.OK indicate success,\n    # otherwise indicate request is failed, you can get error code\n    # and message from code and message.\n    if response.status_code == HTTPStatus.OK:\n        return(response)\n    else:\n        print(response.code)  # The error code.\n        print(response.message)  # The error message.\n\n# 调用视觉推理模型\nresponse = simple_multimodal_conversation_call()\n\n# 获取响应内容\ncontent = response.output.choices[0]['message']['content'][0]['text']\n\n# 使用 IPython.display 模块的 Markdown 类来显示 Markdown 内容。\nMarkdown(content)\n\n这张图片显示了一位女士和一只狗在海滩上。她们似乎正在互动，可能是在玩耍或训练中握手。背景是美丽的日落景色，海浪轻轻拍打着海岸线。\n这位女士穿着格子衬衫，并且戴着一个手镯。她坐在沙滩上与她的宠物进行着愉快的时光。这只狗看起来是一只拉布拉多犬或其他类似的品种，它也戴着手套以保护它的爪子并保持清洁。\n这个场景充满了友谊、爱以及对大自然美景的欣赏。这是一个温馨的画面，展示了人与动物之间深厚的情感纽带。",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>阿里云百炼</span>"
    ]
  },
  {
    "objectID": "bailian.html#文生图",
    "href": "bailian.html#文生图",
    "title": "19  阿里云百炼",
    "section": "19.4 文生图",
    "text": "19.4 文生图\n通义万相-文本生成图像是基于自研的Composer组合生成框架的AI绘画创作大模型，能够根据用户输入的文字内容，生成符合语义描述的多样化风格的图像。通过知识重组与可变维度扩散模型，加速收敛并提升最终生成图片的效果，布局自然、细节丰富、画面细腻、结果逼真。AI深度理解中英文文本语义，让文字秒变精致AI画作。\n当前模型支持的风格包括但不限于：\n\n水彩、油画、中国画、素描、扁平插画、二次元、3D卡通。\n支持中英文双语输入。\n支持客户自定义咒语书/修饰词，可生成不同风格、不同主题、不同派别的图片，满足个性创意的AI图片生成需求。\n支持输入参考图片进行参考内容或者参考风格迁移，支持更丰富的风格、主题和派别，AI作画质量更加高保真。\n\n\nfrom http import HTTPStatus\nfrom urllib.parse import urlparse, unquote\nfrom pathlib import PurePosixPath\nimport requests\nfrom dashscope import ImageSynthesis\n\ndef simple_call(prompt = '一个面容姣好的汉族少女骑着一头白色的老虎在暗夜森林中穿梭，二次元风格', outdir = \"output\", filename = \"girl-riding-tiger.png\"):\n    rsp = ImageSynthesis.call(model=ImageSynthesis.Models.wanx_v1,\n                              prompt=prompt,\n                              n=1,\n                              size='1024*1024')\n    if rsp.status_code == HTTPStatus.OK:\n        print(rsp.output)\n        print(rsp.usage)\n        # save file to current directory\n        files = []\n        for result in rsp.output.results:\n            file = './%s/%s' % (outdir, filename)\n            with open(file, 'wb+') as f:\n                f.write(requests.get(result.url).content)\n            files.append(file)\n        return(files)\n    else:\n        print('Failed, status_code: %s, code: %s, message: %s' %\n              (rsp.status_code, rsp.code, rsp.message))\n\n# 调用文生图模型\nsimple_call()\n\n{\"task_id\": \"729e179b-ffa1-48b3-820f-1f00f2548bd2\", \"task_status\": \"SUCCEEDED\", \"results\": [{\"url\": \"https://dashscope-result-sh.oss-cn-shanghai.aliyuncs.com/1d/3b/20250218/9d34e27a/62c59c45-87af-43f7-ad52-f4be608dbf6f-1.png?Expires=1739960506&OSSAccessKeyId=LTAI5tQZd8AEcZX6KZV4G8qL&Signature=vfhki8kJ3jdVh6y%2BFv0pM7w%2Bc20%3D\"}], \"submit_time\": \"2025-02-18 18:21:20.281\", \"scheduled_time\": \"2025-02-18 18:21:20.336\", \"end_time\": \"2025-02-18 18:21:47.300\", \"task_metrics\": {\"TOTAL\": 1, \"SUCCEEDED\": 1, \"FAILED\": 0}}\n{\"image_count\": 1}\n\n\n['./output/girl-riding-tiger.png']\n\n\n显示模型生成的图片。\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# 读取所有图片文件\nimage_path = \"output/girl-riding-tiger.png\"\nimage = Image.open(image_path)\n\nplt.imshow(image)",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>阿里云百炼</span>"
    ]
  },
  {
    "objectID": "bailian.html#语音识别",
    "href": "bailian.html#语音识别",
    "title": "19  阿里云百炼",
    "section": "19.5 语音识别",
    "text": "19.5 语音识别\nSenseVoice 语音识别大模型专注于高精度多语言语音识别、情感辨识和音频事件检测，支持超过 50 种语言的识别，整体效果优于 Whisper 模型，中文与粤语识别准确率相对提升在 50% 以上。\n\n# For prerequisites running the following sample, visit https://help.aliyun.com/document_detail/611472.html\n\nimport json\nfrom urllib import request\nfrom http import HTTPStatus\n\n# 使用 dashscope 调用模型\nimport dashscope\n\n\ntask_response = dashscope.audio.asr.Transcription.async_call(\n    model='sensevoice-v1',\n    file_urls=[\n        'https://dashscope.oss-cn-beijing.aliyuncs.com/samples/audio/sensevoice/rich_text_example_1.wav'],\n    language_hints=['en'],)\n\ntranscription_response = dashscope.audio.asr.Transcription.wait(\n    task=task_response.output.task_id)\n\nif transcription_response.status_code == HTTPStatus.OK:\n    for transcription in transcription_response.output['results']:\n        url = transcription['transcription_url']\n        result = json.loads(request.urlopen(url).read().decode('utf8'))\n        Markdown(json.dumps(result, indent=4, ensure_ascii=False))\n    print('transcription done!')\nelse:\n    print('Error: ', transcription_response.output.message)\n\ntranscription done!",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>阿里云百炼</span>"
    ]
  },
  {
    "objectID": "bailian.html#语音合成",
    "href": "bailian.html#语音合成",
    "title": "19  阿里云百炼",
    "section": "19.6 语音合成",
    "text": "19.6 语音合成\nSambert 语音合成 API 基于达摩院改良的自回归韵律模型，支持文本至语音的实时流式合成。\n\nimport sys\n\nimport dashscope\nfrom dashscope.audio.tts import SpeechSynthesizer\n\nresult = SpeechSynthesizer.call(model='sambert-zhichu-v1',\n                                text='今天天气怎么样',\n                                sample_rate=48000)\n\ntts_output = \"output/weather.wav\"\nif result.get_audio_data() is not None:\n    with open(tts_output, 'wb') as f:\n        f.write(result.get_audio_data())\n    print('SUCCESS: get audio data: %dbytes in output.wav' %\n          (sys.getsizeof(result.get_audio_data())))\nelse:\n    print('ERROR: response is %s' % (result.get_response()))\n\nSUCCESS: get audio data: 135677bytes in output.wav\n\n\n使用 IPython.display 模块的 Audio 类来显示音频文件。\n\nfrom IPython.display import Audio\nfrom IPython.core.display import HTML\n\ndef html_tag_audio(file, file_type='wav'):\n    file_type = file_type.lower()\n    if file_type not in ['wav', 'mp3', 'ogg']:\n        raise ValueError(\"Invalid audio type. Supported types: 'wav', 'mp3', 'ogg'.\")\n    \n    audio_tag = f'''\n    &lt;audio controls&gt;\n      &lt;source src=\"{file}\" type=\"audio/{file_type}\"&gt;\n      Your browser does not support the audio element.\n    &lt;/audio&gt;\n    '''\n    return HTML(audio_tag)\n\n# Example usage\ntts_output = \"output/weather.wav\"\nhtml_tag_audio(tts_output, file_type=\"wav\")\n\n\n    \n      \n      Your browser does not support the audio element.",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>阿里云百炼</span>"
    ]
  },
  {
    "objectID": "bailian.html#文档解析",
    "href": "bailian.html#文档解析",
    "title": "19  阿里云百炼",
    "section": "19.7 文档解析",
    "text": "19.7 文档解析\nQwen-Long 是在通义千问针对超长上下文处理场景的大语言模型，支持中文、英文等不同语言输入，支持最长 1000 万 tokens(约 1500 万字或 1.5 万页文档)的超长上下文对话。配合同步上线的文档服务，可支持 word、pdf、markdown、epub、mobi 等多种文档格式的解析和对话。\n\n19.7.1 上传文档\n\nfrom pathlib import Path\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"), # 如果您没有配置环境变量，请在此处用您的API Key进行替换\n    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",  # 填写DashScope服务的base_url\n)\n\nfile_object = client.files.create(file=Path(\"example/Kraken2.pdf\"), \n                                  purpose=\"file-extract\")\n\nprint(f\"文件上传成功，文件ID: {file_object.id}\")\n\n文件上传成功，文件ID: file-fe-LySnGx41erbd3pG1rTMiKGVQ\n\n\n\n\n19.7.2 查询文件\n查询、删除文件。\n\n# 查询文件元信息\nclient.files.retrieve(file_object.id)\n\n# 查询文件列表\nclient.files.list()\n\nSyncCursorPage[FileObject](data=[FileObject(id='file-fe-LySnGx41erbd3pG1rTMiKGVQ', bytes=123750, created_at=1739874244, filename='Kraken2.pdf', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-UhX2YTfoGPQr1GRdaDjDCTu9', bytes=1460, created_at=1739873588, filename='syncom-group-library.md', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-fYsRF7lDiTWmp67PK1U6L1El', bytes=1006, created_at=1739873559, filename='gaoch-cv.md', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-OnWGXZae7MUoF7TUuXjDdmLL', bytes=123750, created_at=1739873449, filename='Kraken2.pdf', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-GbY5nvfIzKyPAxP9aAEKQbsn', bytes=1460, created_at=1739414858, filename='syncom-group-library.md', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-d6B6l61mrrs5fc8U1QJ8hi3I', bytes=1006, created_at=1739414852, filename='gaoch-cv.md', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-PVADUTrSKXmwZ1X9x0e9pszT', bytes=123750, created_at=1739414844, filename='Kraken2.pdf', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-eogD3rvgT6uTMJWpJnHNp00U', bytes=1460, created_at=1739414780, filename='syncom-group-library.md', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-cmKyi2v1tC3Mxfu8PWNGQdu9', bytes=1006, created_at=1739414764, filename='gaoch-cv.md', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-cNv1OWmcw1QlT5hs226vdVqo', bytes=123750, created_at=1739414746, filename='Kraken2.pdf', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-deyl3COTUvsxO8JCAfPPzPCU', bytes=1460, created_at=1739413673, filename='syncom-group-library.md', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-YKIpPS6Gn3aSbnbxVK2ITKEV', bytes=1006, created_at=1739413660, filename='gaoch-cv.md', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-Le7L1MdNrQjzH19U4mwoXGhX', bytes=123750, created_at=1739413637, filename='Kraken2.pdf', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-YUueFKyqQgR4KxsD4f0Xo3hy', bytes=1460, created_at=1739413573, filename='syncom-group-library.md', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-Qz4RSuwUCD7GI20h2woiyHBW', bytes=1006, created_at=1739413561, filename='gaoch-cv.md', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-mQUE9MzpzWxLbnsArHXNDL8Z', bytes=123750, created_at=1739413553, filename='Kraken2.pdf', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-uRhvlu2YM7fTtavF8QTQHYf6', bytes=1006, created_at=1739413422, filename='gaoch-cv.md', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-XYmu2ma8WoYeDwapVottjJbP', bytes=123750, created_at=1739413392, filename='Kraken2.pdf', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-PZ8CQnd7f6pQo4wJks6DTdH3', bytes=123750, created_at=1739413294, filename='Kraken2.pdf', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-LBxrE70nGWXxqQoLOtHVn0hK', bytes=123750, created_at=1727754850, filename='Kraken2.pdf', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-99niQQOZZ4IJ4FrkkEcediaY', bytes=1460, created_at=1726562411, filename='syncom-group-library.md', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-YLkL9wbty5jGc1BdGdkO05cN', bytes=1006, created_at=1726562407, filename='gaoch-cv.md', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-cG9T8L8CJhzdb0PTMMoyTizR', bytes=123750, created_at=1726562400, filename='Kraken2.pdf', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-TWXoghFaBtZMkF9jwG505hGl', bytes=1460, created_at=1726562379, filename='syncom-group-library.md', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-84fZye53knxXL0nHaJT3nG5K', bytes=1006, created_at=1726562376, filename='gaoch-cv.md', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-FegrQSGslKgnjoM3hz9BylgC', bytes=123750, created_at=1726562370, filename='Kraken2.pdf', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-d0surOfpU9ck8YP9grzxyeSU', bytes=1460, created_at=1726559802, filename='syncom-group-library.md', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-U8DwGdPB6Txo7cIo90MPAjlU', bytes=1006, created_at=1726559791, filename='gaoch-cv.md', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-iAiDxfZ6l09nmaNKNOjnYdCX', bytes=123750, created_at=1726557247, filename='Kraken2.pdf', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-5q4ob5AoPyEQA9vnQAIBaXIU', bytes=123750, created_at=1726556558, filename='Kraken2.pdf', object='file', purpose='file-extract', status='processed', status_details=None), FileObject(id='file-fe-LZaGRINYblYiXxAhJaHhBMZz', bytes=6416, created_at=1726556278, filename='安装Kraken2.md', object='file', purpose='file-extract', status='processed', status_details=None)], object='list', has_more=False)\n\n\n\n# 删除文件\nclient.files.delete(file_object.id)",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>阿里云百炼</span>"
    ]
  },
  {
    "objectID": "bailian.html#基于文档的对话",
    "href": "bailian.html#基于文档的对话",
    "title": "19  阿里云百炼",
    "section": "19.8 基于文档的对话",
    "text": "19.8 基于文档的对话\nQwen-Long 支持长文本（文档）对话，文档内容需放在 role 为 system 的 message 中，有以下两种方式可将文档信息输入给模型：\n\n在提前上传文档获取文档 ID（fileid）后，可以直接提供 fileid。支持在对话中使用一个或多个 fileid。\n直接输入需要处理的文本格式的文档内容（file content）。\n\n\n# 获取文件内容\n# 新文档上传后需要等待模型解析，首轮响应时间可能较长\ncompletion = client.chat.completions.create(\n    model=\"qwen-long\",\n    messages=[\n        {\n            'role': 'system',\n            'content': 'You are a helpful assistant.'\n        },\n        {\n            'role': 'system',\n            'content': f'fileid://{file_object.id}'\n        },\n        {\n            'role': 'user',\n            'content': '这篇文章讲了什么？'\n        }\n    ],\n    stream=False\n)\n\n# 打印文档解析结果\nMarkdown(completion.choices[0].message.content)\n\n这篇文章介绍了几种常用的基于宏基因组学数据进行物种注释的工具，重点讲述了Kraken2。文章涵盖了以下内容：\n\n常用工具概述：提到了Kraken2、MetaPhlAn（MetaPhlAn3）和Centrifuge三种工具的工作原理、优点及适用场景。\nKraken2的特点：\n\n工作原理：基于k-mer比对技术，将reads比对到参考数据库中的特定k-mer集合，从而实现快速分类注释。\n优势：处理大规模数据时速度非常快。\n数据库选项：支持使用标准数据库（如NCBI RefSeq）或自定义数据库。\n输出结果：生成物种分类信息及相对丰度。\n\nStandard-8数据库：特别介绍了适用于个人电脑的Kraken2 Standard-8数据库，其特点包括数据库大小限制为8GB、针对常见物种优化、快速分析等。该数据库适合在计算资源有限的情况下使用，例如在台式电脑上进行本地分析或在有存储限制的云计算环境中使用。\nKraken2使用流程：包括安装Kraken2、下载数据库、运行Kraken2进行分类注释的具体步骤，并提供了命令行示例。\n结果分析与可视化：说明了如何解读Kraken2的输出文件，并给出了用R语言进行结果可视化的简单示例。\n\n总的来说，文章详细描述了如何利用Kraken2及其优化版本Standard-8数据库来进行宏基因组测序数据的物种注释分析。\n\n\n\n19.8.1 多个文档\n当有多个文档时，可以将多个 fileid 传递给 content。\n# 首次对话会等待文档解析完成，首轮响应时间可能较长\ncompletion = client.chat.completions.create(\n    model=\"qwen-long\",\n    messages=[\n        {\n            'role': 'system',\n            'content': 'You are a helpful assistant.'\n        },\n        {\n            'role': 'system',\n            'content': f\"fileid://{file_1.id},fileid://{file_2.id}\"\n        },\n        {\n            'role': 'user',\n            'content': '这几篇文章讲了什么？'\n        }\n    ],\n    stream=False\n)\n\n\n19.8.2 追加文档\n使用下面的方法，可以在对话过程中追加文档。\n\n# data_1.pdf为原文档，data_2.pdf为追加文档\nfile_1 = client.files.create(file=Path(\"example/gaoch-cv.md\"),\n                             purpose=\"file-extract\")\n\n# 初始化messages列表\nmessages = [\n    {\n        'role': 'system',\n        'content': 'You are a helpful assistant.'\n    },\n    {\n        'role': 'system',\n        'content': f'fileid://{file_1.id}'\n    },\n    {\n        'role': 'user',\n        'content': '这篇文章讲了什么？'\n    },\n]\n# 第一轮响应\ncompletion = client.chat.completions.create(\n    model=\"qwen-long\",\n    messages=messages,\n    stream=False\n)\n\n# 打印出第一轮响应\nMarkdown(f\"第一轮响应：{completion.choices[0].message.content}\")\n\n第一轮响应：文档《gaoch-cv.md》主要是高春辉的个人简介，内容包括他的教育背景、工作经历、研究领域和学术成果等方面。具体如下：\n\n教育及工作背景：高春辉是微生物学博士，现任华中农业大学资源与环境学院副研究员，硕士生导师。他在2012年获得华中农业大学博士学位，并在2013年至2015年间在中国科技大学进行了博士后研究。自2016年起，他在华中农业大学任职。\n研究领域：高春辉拥有分子生物学与生物化学、生物信息学、功能基因组学等多学科背景，专注于微生物关键表型的调控机制研究，特别是合成微生物群落的涌现性功能和调控研究。\n科研项目：他曾主持过多个重要科研项目，如自然科学基金青年项目、面上项目、国家重点研发计划子课题、教育部产学合作项目以及博士后基金等。\n学术成果：高春辉已在ISME J、ISME Communications、Microbial Ecology、Frontiers in Genetics等多个知名学术期刊上发表了45篇研究论文，参与编写了2部书籍，并获得了1项软件著作权。其研究成果在谷歌学术上的引用次数超过1500次，H指数达到24。此外，他还担任iMeta期刊的青年编委。\n\n文档还附带了一个链接，指向高春辉的GitHub头像。\n\n\n将第一轮响应的内容添加到历史记录中。\n\n# 构造assistant_message\nassistant_message = {\n    \"role\": \"assistant\",\n    \"content\": completion.choices[0].message.content}\n\n# 将assistant_message添加到messages中\nmessages.append(assistant_message)\n\n上传一个新文档。\n\n# 获取追加文档的fileid\nfile_2 = client.files.create(file=Path(\"example/syncom-group-library.md\"), \n                             purpose=\"file-extract\")\n\n# 将追加文档的fileid添加到messages中\nsystem_message = {\n    'role': 'system',\n    'content': f'fileid://{file_2.id}'\n}\nmessages.append(system_message)\n\n# 添加用户问题\nmessages.append({\n    'role': 'user',\n    'content': '这两篇文章的内容有什么异同点？'\n})\n\n# 追加文档后的响应\ncompletion = client.chat.completions.create(\n    model=\"qwen-long\",\n    messages=messages,\n    stream=False\n)\n\nMarkdown(f\"追加文档后的响应：{completion.choices[0].message.content}\")\n\n追加文档后的响应：这两篇文章的内容存在明显的不同点和一些潜在的联系，具体如下：\n\n19.8.3 不同点\n\n主题和目的：\n\n《gaoch-cv.md》：主要介绍高春辉个人的学术背景、研究方向和成就，是一份个人简历性质的文档，旨在展示高春辉的专业能力和学术贡献。\n《syncom-group-library.md》：描述的是一个名为“土壤生物膜与环境健康书屋”的机构或设施，介绍了书屋的建立背景、宗旨和服务规则，重点在于书屋的功能和使用约定。\n\n内容结构：\n\n《gaoch-cv.md》：详细列出了个人的教育经历、工作经历、研究领域、科研项目和学术成果，具有较强的个人色彩。\n《syncom-group-library.md》：则侧重于书屋的设立目的、服务对象以及借阅规则等，更偏向于机构管理和运营方面的信息。\n\n受众群体：\n\n《gaoch-cv.md》：面向对高春辉的研究感兴趣的学术界人士、潜在合作者或学生。\n《syncom-group-library.md》：面向书屋的使用者，包括本科生、研究生、博士生和研究人员，告知他们如何利用书屋资源。\n\n\n\n\n19.8.4 相同点或潜在联系\n\n学科关联：\n\n高春辉的研究领域（微生物学、土壤微生物学）与书屋的主题（土壤生物膜与环境健康）有密切的关系，都涉及微生物学及其应用。\n\n教育和人才培养：\n\n两篇文章都提到了对学生和研究人员的支持。高春辉作为导师指导学生，而书屋则是为不同层次的学生和研究人员提供学习和研究资源，服务于“本-硕-博-研”全流程人才培养。\n\n资源共享：\n\n高春辉的研究成果可以通过书屋中的相关书籍和文献得到传播和应用，书屋的建立有助于促进微生物学领域的知识交流和共享。\n\n\n总结来说，虽然两篇文章的具体内容和侧重点不同，但它们都在微生物学及其相关领域内发挥作用，共同推动该领域的发展和人才培养。",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>阿里云百炼</span>"
    ]
  },
  {
    "objectID": "bailian.html#总结",
    "href": "bailian.html#总结",
    "title": "19  阿里云百炼",
    "section": "19.9 总结",
    "text": "19.9 总结\n通过上面的介绍，我们可以看到，百炼平台提供了丰富的模型和功能，可以满足不同的需求。在国内，类似的平台还有火山引擎、Kimi、ChatGLM 等，他们都有提供自身自主开发的大模型、国际上的开源模型以及国内第三方发表的模型等。通常都支持 OpenAI 的 API 接口，方便开发者使用。",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>阿里云百炼</span>"
    ]
  },
  {
    "objectID": "bailian.html#footnotes",
    "href": "bailian.html#footnotes",
    "title": "19  阿里云百炼",
    "section": "",
    "text": "“ChatBot Arena LLM”榜单由美国加州大学伯利克分校天空计算实验室与LMArena联合开发，通过用户盲测的方式，覆盖了对话、代码、图文生成、网页开发等多维度能力评估，最终基于260万票结果反映出197个模型在真实体验下的排名情况，也是业内公认的权威榜单。↩︎",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>阿里云百炼</span>"
    ]
  },
  {
    "objectID": "huggingface.html",
    "href": "huggingface.html",
    "title": "20  Hugging Face",
    "section": "",
    "text": "20.1 安装",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hugging Face</span>"
    ]
  },
  {
    "objectID": "huggingface.html#安装",
    "href": "huggingface.html#安装",
    "title": "20  Hugging Face",
    "section": "",
    "text": "安装机器学习基础库（pytorch 或者 TensorFlow）。 首先创建一个 Conda 环境，然后安装 Pytorch。conda install pytorch::pytorch torchvision torchaudio -c pytorch\n安装 transformers： pip install transformers datasets evaluate accelerate。",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hugging Face</span>"
    ]
  },
  {
    "objectID": "huggingface.html#配置-gpu-加速",
    "href": "huggingface.html#配置-gpu-加速",
    "title": "20  Hugging Face",
    "section": "20.2 配置 GPU 加速",
    "text": "20.2 配置 GPU 加速\nPytorch 支持 CUDA（NVIDIA）和 MPS（Mac）平台的 GPU 加速，所以这里检测一下硬件环境。\n\nimport torch\n\n# select the device for computation\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"using device: {device}\")\n\nusing device: mps",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hugging Face</span>"
    ]
  },
  {
    "objectID": "huggingface.html#pipeline",
    "href": "huggingface.html#pipeline",
    "title": "20  Hugging Face",
    "section": "20.3 Pipeline",
    "text": "20.3 Pipeline\npipeline 是 transformers 库中的一个高层接口，旨在简化模型的使用。它封装了模型的加载、输入处理、预测和输出处理的细节，使得用户可以以更简单的方式执行常见任务。\n在 Hugging Face Transformers 库中，pipelines 可以被分为不同的类别，以适应音频、计算机视觉、自然语言处理（NLP）和多模态任务。以下是这些类别中常见的 pipelines：\n\n音频（Audio）:\n\n语音识别（Speech Recognition）: 将音频转换为文本。\n语音合成（Text-to-Speech）: 将文本转换为语音。\n语音分类（Speech Classification）: 对音频进行分类，如情绪识别。\n\n计算机视觉（Computer Vision）:\n\n图像分类（Image Classification）: 识别图像中的主要对象或场景。\n对象检测（Object Detection）: 识别并定位图像中的对象。\n图像分割（Image Segmentation）: 将图像分割成多个部分或对象。\n图像生成（Image Generation）: 根据文本描述生成图像。\n\n自然语言处理（NLP）:\n\n文本分类（Text Classification）: 对文本进行分类，如情感分析。\n命名实体识别（Named Entity Recognition, NER）: 识别文本中的实体。\n问答（Question Answering）: 从文本中找到问题的答案。\n文本生成（Text Generation）: 生成新的文本内容。\n摘要（Summarization）: 生成文本的摘要。\n翻译（Translation）: 将文本从一种语言翻译成另一种语言。\n\n多模态（Multimodal）:\n\n视觉问答（Visual Question Answering, VQA）: 结合图像和文本问题，提供答案。\n图像字幕生成（Image Captioning）: 为图像生成描述性文本。\n视频问答（Video Question Answering）: 根据视频内容回答问题。\n多模态情感分析（Multimodal Sentiment Analysis）: 结合文本、音频和视觉信息进行情感分析。\n\n\n这些 pipelines 利用了预训练的模型，可以处理各种任务，从单一模态的音频或图像处理到结合多种模态信息的复杂任务。用户可以根据自己的需求选择合适的模型和pipeline来实现特定的任务。\n\nimport inspect\nfrom transformers import pipelines\n\n# 获取 transformers.pipeline 模块中的所有成员\npipeline_members = inspect.getmembers(pipelines)\n\n# 过滤出类，并且名称以 'Pipeline' 结尾\npipeline_classes = [name for name, obj in pipeline_members if inspect.isclass(obj) and name.endswith('Pipeline')]\n\n# 打印符合条件的类名称\nprint(\"Classes ending with 'Pipeline':\")\nfor class_name in pipeline_classes:\n    print(class_name)\n\nClasses ending with 'Pipeline':\nAudioClassificationPipeline\nAutomaticSpeechRecognitionPipeline\nDepthEstimationPipeline\nDocumentQuestionAnsweringPipeline\nFeatureExtractionPipeline\nFillMaskPipeline\nImageClassificationPipeline\nImageFeatureExtractionPipeline\nImageSegmentationPipeline\nImageTextToTextPipeline\nImageToImagePipeline\nImageToTextPipeline\nMaskGenerationPipeline\nNerPipeline\nObjectDetectionPipeline\nPipeline\nQuestionAnsweringPipeline\nSummarizationPipeline\nTableQuestionAnsweringPipeline\nText2TextGenerationPipeline\nTextClassificationPipeline\nTextGenerationPipeline\nTextToAudioPipeline\nTokenClassificationPipeline\nTranslationPipeline\nVideoClassificationPipeline\nVisualQuestionAnsweringPipeline\nZeroShotAudioClassificationPipeline\nZeroShotClassificationPipeline\nZeroShotImageClassificationPipeline\nZeroShotObjectDetectionPipeline\n\n\n以下是一些常见的 pipeline 类型和它们的应用场景：\n\n20.3.1 文本分类 (text-classification)\n用于对输入文本进行分类，例如情感分析。\n\nfrom transformers import pipeline\nfrom pprint import pprint  # pretty print\n\nclassifier = pipeline('text-classification')\nresult = classifier(\"I love using transformers!\")\npprint(result)\n\n[{'label': 'POSITIVE', 'score': 0.9994327425956726}]\n\n\n\n\n20.3.2 命名实体识别 (ner)\n用于识别文本中的命名实体（如人名、地点、组织等）。\n\nfrom transformers import pipeline\n\nner = pipeline('ner', device = device)\nresult = ner(\"Hugging Face is based in New York City.\")\npprint(result)\n\n[{'end': 2,\n  'entity': 'I-ORG',\n  'index': 1,\n  'score': np.float32(0.9736425),\n  'start': 0,\n  'word': 'Hu'},\n {'end': 7,\n  'entity': 'I-ORG',\n  'index': 2,\n  'score': np.float32(0.7939444),\n  'start': 2,\n  'word': '##gging'},\n {'end': 12,\n  'entity': 'I-ORG',\n  'index': 3,\n  'score': np.float32(0.9046833),\n  'start': 8,\n  'word': 'Face'},\n {'end': 28,\n  'entity': 'I-LOC',\n  'index': 7,\n  'score': np.float32(0.9990658),\n  'start': 25,\n  'word': 'New'},\n {'end': 33,\n  'entity': 'I-LOC',\n  'index': 8,\n  'score': np.float32(0.99908113),\n  'start': 29,\n  'word': 'York'},\n {'end': 38,\n  'entity': 'I-LOC',\n  'index': 9,\n  'score': np.float32(0.99939454),\n  'start': 34,\n  'word': 'City'}]\n\n\n\n\n20.3.3 问答 (question-answering)\n用于从给定上下文中回答问题。\n\nfrom transformers import pipeline\n\nquestion_answerer = pipeline('question-answering', device = device)\nresult = question_answerer(question=\"What is the capital of France?\", context=\"The capital of France is Paris.\")\npprint(result)\n\n{'answer': 'Paris', 'end': 30, 'score': 0.9863281846046448, 'start': 25}\n\n\n\n\n20.3.4 文本生成 (text-generation)\n用于生成文本，例如生成续写或对话。\n\nfrom transformers import pipeline\n\ngenerator = pipeline('text-generation', device = device)\nresult = generator(\"Once upon a time\", max_length=50)\npprint(result)\n\n[{'generated_text': 'Once upon a time her vision suddenly became clear and so '\n                    'did her ears and she could clearly hear it on the bright '\n                    'plains of the Himalayas. This image was known '\n                    \"as'shambhala' – 'Shambhala is the ocean\"}]\n\n\n\n\n20.3.5 翻译 (translation)\n用于将文本从一种语言翻译成另一种语言。\n\nfrom transformers import pipeline\n\ntranslator = pipeline('translation_en_to_fr', device = device)\nresult = translator(\"Hello, how are you?\")\npprint(result)\n\n[{'translation_text': 'Bonjour, comment êtes-vous?'}]\n\n\n\n\n20.3.6 文本摘要 (summarization)\n用于对长文本进行摘要，提取主要内容。\n\nfrom transformers import pipeline\n\nsummarizer = pipeline('summarization', device = device)\nresult = summarizer(\"Hugging Face is creating a tool that democratizes AI. The library will support various tasks and models.\")\npprint(result)\n\n[{'summary_text': ' Hugging Face is creating a tool that democratizes AI . The '\n                  'library will support various tasks and models . Hugging '\n                  'face is creating an AI tool that can be used to help people '\n                  'learn more about AI . It will be available in the U.S. for '\n                  'free .'}]",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hugging Face</span>"
    ]
  },
  {
    "objectID": "huggingface.html#运行机制",
    "href": "huggingface.html#运行机制",
    "title": "20  Hugging Face",
    "section": "20.4 运行机制",
    "text": "20.4 运行机制\n使用 Hugging Face 的 pipeline 运行任务时，任务默认是在本地执行的。\n当你使用 pipeline 函数时，它会加载一个预训练的模型（可以是 Hugging Face Hub 上的模型，也可以是你本地的模型），然后在你的本地机器上执行推理任务。这意味着所有计算都是在你的本地计算机上进行的，而不是在 Hugging Face 的服务器上进行的。\n不过，pipeline 也可以访问在线的模型存储库。如果你指定了一个在线模型（例如 Hugging Face Hub 上的某个模型），那么 pipeline 会先从在线存储库下载模型到本地，然后在本地运行推理任务。因此，即使你访问的是在线模型，执行过程仍然是在本地完成的。\n本地运行推理，可以很方便的执行批处理任务。\n\nfrom transformers import pipeline\n\nclassifier = pipeline(\"sentiment-analysis\", device=device)\n\nresults = classifier([\"We are very happy to show you the 🤗 Transformers library.\", \"We hope you don't hate it.\"])\nfor result in results:\n    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\n\nlabel: POSITIVE, with score: 0.9998\nlabel: NEGATIVE, with score: 0.5309\n\n\n\n20.4.1 配置模型和参数\npipeline 会自动下载和使用默认的预训练模型。但是有些任务可能没有指定模型，这时候就需要配置模型参数。此外，如果需要使用特定的模型，也可以在 pipeline 构造函数中指定模型名称。\npipeline 会根据任务类型自动处理输入和输出。例如，文本分类任务的输出通常是每个类别的概率，而翻译任务的输出是翻译后的文本。如果提供的任务名称不正确，pipeline 可能会抛出错误。如果模型不适合特定任务，也可能会得到不准确的结果或遇到运行时错误。\n下面的示例中，我们针对 ZeroShotClassificationPipeline 任务指定使用了 Facebook 的 bart-large-mnli 模型。\n\noracle = pipeline(\"zero-shot-classification\", \n                  model=\"facebook/bart-large-mnli\",\n                  device=device)\noracle(\n    \"I have a problem with my iphone that needs to be resolved asap!!\",\n    candidate_labels=[\"urgent\", \"not urgent\", \"phone\", \"tablet\", \"computer\"],\n)\n\n{'sequence': 'I have a problem with my iphone that needs to be resolved asap!!',\n 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'],\n 'scores': [0.503635823726654,\n  0.47879907488822937,\n  0.012600619345903397,\n  0.002655782038345933,\n  0.002308761700987816]}\n\n\n\noracle(\n    \"I have a problem with my iphone that needs to be resolved asap!!\",\n    candidate_labels=[\"english\", \"german\"],\n)\n\n{'sequence': 'I have a problem with my iphone that needs to be resolved asap!!',\n 'labels': ['english', 'german'],\n 'scores': [0.8135169744491577, 0.18648304045200348]}\n\n\n整个流程以及本地模型的安装和运行情况：\n\n模型下载：\n\n当你指定 model=\"facebook/bart-large-mnli\" 时，Hugging Face 的 transformers 库会从 Hugging Face Hub 下载这个预训练模型（facebook/bart-large-mnli）。\n模型下载后，会被存储在你的本地文件系统中，默认位置通常是 ~/.cache/huggingface/transformers/ 目录下。如果你有自定义的缓存目录设置，模型将下载到指定位置。\n\n模型加载：\n\n下载完成后，pipeline 会将该模型加载到内存中。这包括模型的权重、配置文件以及与之相关的词汇表（tokenizer）。\n\n任务执行：\n\n当你使用 oracle 这个 pipeline 对象来进行零样本分类任务时，所有的计算和推理（inference）都是在你的本地机器上执行的。这包括文本的预处理、模型的前向传播计算、以及后处理和输出结果。\n\n\n\n\n20.4.2 本地模型的存储和管理\n\n存储位置：模型的权重文件、配置文件和词汇表会存储在 ~/.cache/huggingface/hub/ 下的一个以模型名称命名的目录中。例如：~/.cache/huggingface/hub/models--facebook--bart-large-mnli/。\n缓存机制：如果你再次使用同一个模型（如 facebook/bart-large-mnli），pipeline 会直接从本地缓存中加载模型，而不会再次从 Hugging Face Hub 下载，除非你手动清除缓存或指定下载新的模型版本。\n\n\n\n20.4.3 配置一个翻译器\n上面我们调用一个翻译器，将英文翻译为法文。\n\nen_fr_translator = pipeline(\"translation_en_to_fr\", device=device)\nen_fr_translator(\"How old are you?\")\n\n[{'translation_text': ' quel âge êtes-vous?'}]\n\n\n不过，调用 pipeline(\"tranlation_en_to_zh\") 却会出错。这是因为：虽然 “translation_en_to_zh” 是一个有效的任务标识符，但它并不是直接指定模型的名称。这时，需要显式指定模型名称来避免这种情况：\n\n\n\n\n\n\nNote\n\n\n\n这里还需要安装一个缺失的模块：SentencePiece。\nSentencePiece 是一个用于文本分词和词汇生成的工具，它在自然语言处理（NLP）任务中非常有用，尤其是在训练和使用基于子词单元的模型时。SentencePiece 由 Google 开发，作为一种无语言依赖的方法，它可以处理几乎任何语言的文本数据。\nSentencePiece 的主要功能\n\n子词单元（Subword Units）生成:\n\nSentencePiece 不依赖于语言的特定词汇表，而是通过数据驱动的方法生成子词单元。它通过分析训练数据中的常见字符序列，生成适合该数据集的子词单元，这些子词单元可以是完整的词、词的一部分（如词缀、词根）、甚至是单个字符。\n这在处理低资源语言或多语言任务时特别有用，因为它可以生成跨语言的统一词汇表，减少OOV（Out of Vocabulary，词汇表外的词）问题。\n\nBPE（Byte-Pair Encoding）和 Unigram 模型:\n\nSentencePiece 支持多种子词分割方法，包括 BPE（Byte-Pair Encoding）和 Unigram 模型。BPE 是一种常用的子词分割算法，通过频繁地合并字符对来生成子词单元。Unigram 模型则是一种基于概率的模型，它根据子词单元的概率来分割文本。\n\n语言无关性:\n\n与传统的分词器不同，SentencePiece 不需要依赖于空格或其他特定的标记来分割词语。这使得它在处理没有明确单词边界的语言（如中文、日文、泰语等）时非常有效。\n\n处理未归一化的文本:\n\nSentencePiece 可以直接处理未经归一化的原始文本（如带有标点符号的文本），这在实际应用中非常有用，避免了对数据进行预处理的需求。\n\n\n使用 SentencePiece 的场景\n\n机器翻译: 在训练机器翻译模型时，使用 SentencePiece 可以将输入和输出文本分割成子词单元，减少词汇表大小，并提高模型的泛化能力。\n预训练语言模型: 诸如 BERT、GPT、T5 等模型在预训练时，通常使用 SentencePiece 生成子词单元词汇表，这些词汇表有助于处理多语言数据和稀有词汇。\n文本生成: 在生成任务中，子词单元可以更好地表示稀有或长尾词汇，减少生成过程中出现的OOV问题。\n\nSentencePiece 是一个强大的分词工具，它通过生成数据驱动的子词单元词汇表，在 NLP 任务中广泛使用，特别是在处理多语言文本和训练深度学习模型时。\n\n\n\ntranslator = pipeline(\"translation_en_to_zh\",\n                      model=\"Helsinki-NLP/opus-mt-en-zh\", device=device)\ntranslator(\"This is a introduction to Huggingface.\")\n\n[{'translation_text': '这是哈ggingface的介绍。'}]\n\n\n结果很一般。这是因为翻译任务不仅需要模型支持，还需要有一个中文的分词器。而默认的分词器不适合进行中文分词的任务。下面，我们优化一下分词器的设置。\n\nfrom transformers import AutoModelWithLMHead,AutoTokenizer,pipeline\nmode_name = 'liam168/trans-opus-mt-en-zh'\nmodel = AutoModelWithLMHead.from_pretrained(mode_name)\ntokenizer = AutoTokenizer.from_pretrained(mode_name)\ntranslation = pipeline(\"translation_en_to_zh\", \n                      model=model, \n                      tokenizer=tokenizer, device=device)\ntranslation('This is a introduction to Huggingface.')\n\n[{'translation_text': '这是\"抱起脸\"的引言'}]\n\n\n让我们逐行解释代码的含义：\nfrom transformers import AutoModelWithLMHead, AutoTokenizer, pipeline\n\nAutoModelWithLMHead: 这是一个自动加载语言模型（Language Model）的类，通常用于加载带有语言建模头的模型。LMHead 代表语言模型的输出层。\nAutoTokenizer: 这是一个自动加载适当的分词器（tokenizer）的类。分词器用于将输入文本转换为模型可以处理的令牌（tokens）序列。\npipeline: 这是 Hugging Face 的高层 API，它提供了各种 NLP 任务的预定义管道（pipeline），如文本分类、翻译、文本生成等。\n\nmode_name = 'liam168/trans-opus-mt-en-zh'\n\nmode_name: 这是一个字符串变量，存储了模型的名称或路径。这里的 liam168/trans-opus-mt-en-zh 是在 Hugging Face 模型库中的一个模型名称，表示一个预训练的从英语到中文翻译的模型。\n\nmodel = AutoModelWithLMHead.from_pretrained(mode_name)\n\nAutoModelWithLMHead.from_pretrained(mode_name): 这行代码加载了 liam168/trans-opus-mt-en-zh 模型的预训练权重和配置。from_pretrained 方法从 Hugging Face 的模型库下载（如果尚未下载）并加载该模型到内存中。\n\ntokenizer = AutoTokenizer.from_pretrained(mode_name)\n\nAutoTokenizer.from_pretrained(mode_name): 这行代码加载了与模型配套的分词器。分词器将输入的英文句子转换为模型所需的令牌（tokens），并且会执行必要的文本预处理。\n\ntranslation = pipeline(\"translation_en_to_zh\", model=model, tokenizer=tokenizer)\n\npipeline(\"translation_en_to_zh\", model=model, tokenizer=tokenizer): 这里使用了 pipeline 函数来创建一个翻译管道，指定了任务类型为 \"translation_en_to_zh\"（从英语到中文的翻译），并传入了之前加载的模型和分词器。这个管道封装了翻译任务的所有步骤，使得翻译文本变得简单且易于使用。\n\ntranslation('This is a introduction to Huggingface.')\n\ntranslation('This is a introduction to Huggingface.'): 这行代码调用了翻译管道，将输入的英文句子 \"This is a introduction to Huggingface.\" 翻译为中文。管道会自动执行以下步骤：\n\n使用 tokenizer 将输入的英文句子分词为令牌。\n将令牌输入到 model 中进行翻译。\n生成的中文令牌序列被解码成自然语言文本。\n\n\n最终输出会是 \"This is a introduction to Huggingface.\" 的中文翻译版本，例如 \"这是对 Huggingface 的介绍。\"（具体翻译结果可能有所不同，取决于模型的性能）。\n这段代码通过加载 Hugging Face 提供的预训练模型和分词器，实现了从英语到中文的自动翻译任务，并且通过使用高层的 pipeline API，简化了翻译任务的执行。",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hugging Face</span>"
    ]
  },
  {
    "objectID": "huggingface.html#两种调用模型的方式",
    "href": "huggingface.html#两种调用模型的方式",
    "title": "20  Hugging Face",
    "section": "20.5 两种调用模型的方式",
    "text": "20.5 两种调用模型的方式\n这里以一个图片对象识别任务为例，展示使用图片数据调用模型执行对象检测任务的两种方式。两种方式都使用 DETR 模型。\nDEtection TRansformer（DETR）模型，通过端到端训练在 COCO 2017 对象检测数据集上进行训练（包含 118K 张标注图像）。\nDETR 模型是一种具有卷积骨干的编码器-解码器变换器。在解码器输出之上添加了两个头部，以执行对象检测：一个线性层用于类别标签，一个多层感知器（MLP）用于边界框。该模型使用所谓的对象查询来检测图像中的对象。每个对象查询都在寻找图像中的特定对象。对于 COCO，设置的对象查询数量为 100。\n模型使用“二部匹配损失”进行训练：将预测的 N=100 个对象查询中的每个类别的预测框与 ground truth 注释进行比较，填充到相同的长度 N（如果一张图片只包含 4 个对象，那么 96 个注释将只是“无对象”作为类别，“无框”作为框）。匈牙利匹配算法用于在每个 N 查询和每个 N 注释之间创建最优的一对一映射。接下来，使用标准交叉熵（对于类别）以及 L1 和通用 IoU 损失的线性组合（对于框）来优化模型参数。\n\nimport io\nimport requests\nfrom PIL import Image\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\n\n20.5.1 使用 Pipeline\n\n# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\n# 加载对象检测流程\nobject_detector = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\", device=device)\n\n# 执行对象检测\ndetection_results = object_detector(image)\npprint(detection_results)\n\n[{'box': {'xmax': 175, 'xmin': 40, 'ymax': 117, 'ymin': 70},\n  'label': 'remote',\n  'score': 0.9982203841209412},\n {'box': {'xmax': 368, 'xmin': 333, 'ymax': 187, 'ymin': 72},\n  'label': 'remote',\n  'score': 0.9960022568702698},\n {'box': {'xmax': 639, 'xmin': 0, 'ymax': 473, 'ymin': 1},\n  'label': 'couch',\n  'score': 0.9954743981361389},\n {'box': {'xmax': 314, 'xmin': 13, 'ymax': 470, 'ymin': 52},\n  'label': 'cat',\n  'score': 0.99880051612854},\n {'box': {'xmax': 640, 'xmin': 345, 'ymax': 368, 'ymin': 23},\n  'label': 'cat',\n  'score': 0.9986782670021057}]\n\n\n\n\n20.5.2 直接使用模型\n\n# 加载图片处理器和模型构建器\nfrom transformers import AutoImageProcessor, AutoModelForObjectDetection\n\n# 加载图片处理器\nimage_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", device=device)\n\n# 加载模型\nmodel = AutoModelForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n\n# 准备图片\ninputs = image_processor(images=image, return_tensors=\"pt\")\n\n# 执行前向传播\noutputs = model(**inputs)\n\n# 转换输出（边界框和类别得分）到 COCO API\n# 只保留得分大于 0.9 的检测结果\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = image_processor.post_process_object_detection(\n    outputs, \n    target_sizes=target_sizes, \n    threshold=0.9)[0]\n\n\n\n20.5.3 结果比较\n下面是使用第一种调用方式调用 DETR 模型时结果的处理示例。\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.patches as patches\n\ndef random_color():\n    \"\"\"Generate a random color.\"\"\"\n    return np.random.rand(3,)\n\n# Create a figure and axis for plotting\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\n\n# Display the original image\nax.imshow(image)\n\n# Overlay bounding boxes and labels with random colors\nfor result in detection_results:\n    score = result['score']\n    label = result['label']\n    box = result['box']\n    \n    # Generate a random color\n    color = random_color()\n    \n    # Draw bounding box\n    rect = patches.Rectangle(\n        (box['xmin'], box['ymin']),\n        box['xmax'] - box['xmin'],\n        box['ymax'] - box['ymin'],\n        linewidth=2,\n        edgecolor=color,\n        facecolor='none'\n    )\n    ax.add_patch(rect)\n    \n    # Draw label and score with the same color as the rectangle\n    label_text = f\"{label}: {score:.2f}\"\n    ax.text(\n        box['xmin'],\n        box['ymin'] - 10,\n        label_text,\n        color=color,\n        fontsize=12,\n        bbox=dict(facecolor='white', alpha=0.5, \n                  edgecolor=color, boxstyle='round,pad=0.5')\n    )\n\n\n# Hide axis\nplt.axis('off')\n\n# Show the plot with bounding boxes and labels\nplt.show()\n\n\n\n\n\n\n\n\n下面是对第二种调用方式结果处理的方法。\n\n# 绘制结果\nfig, ax = plt.subplots(1, figsize=(10, 6))\nax.imshow(image)  # image 需要是检测的原始图像\n\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    box = [round(i, 2) for i in box.tolist()]\n    x, y, w, h = box\n    rect = patches.Rectangle((x, y), w - x, h - y, linewidth=2, edgecolor='r', facecolor='none')\n    ax.add_patch(rect)\n    ax.text(x, y - 5, f\"{model.config.id2label[label.item()]}: {round(score.item(), 3)}\",\n            fontsize=12, color='white', bbox=dict(facecolor='red', alpha=0.5))\n\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n两种调用方式的结果是一致的。",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hugging Face</span>"
    ]
  },
  {
    "objectID": "huggingface.html#yolov8",
    "href": "huggingface.html#yolov8",
    "title": "20  Hugging Face",
    "section": "20.6 YOLOv8",
    "text": "20.6 YOLOv8\nYOLOv8 需要使用 pip 或者 conda 安装。安装后提供 cli 和 Python 等两种运行方式。详情参见：https://docs.ultralytics.com/quickstart/。",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hugging Face</span>"
    ]
  },
  {
    "objectID": "huggingface.html#vit-图像分类",
    "href": "huggingface.html#vit-图像分类",
    "title": "20  Hugging Face",
    "section": "20.7 ViT 图像分类",
    "text": "20.7 ViT 图像分类\nThe Vision Transformer（ViT）是一种以监督方式在大量图像集合（即 ImageNet - 21k）上进行预训练的 Transformer 编码器模型（类似于 BERT），图像分辨率为 224x224 像素。接下来，该模型在 ImageNet（也称为 ILSVRC2012）上进行微调，这是一个包含 100 万张图像和 1000 个类别的数据集，图像分辨率同样为 224x224。\n图像作为一系列固定大小的补丁（分辨率为 16x16）呈现给模型，这些补丁是线性嵌入的。还在序列开头添加一个[CLS]标记，用于分类任务。在将序列输入到 Transformer 编码器的层之前，还添加了绝对位置嵌入。\n通过对模型进行预训练，它学习到图像的内部表示，然后可用于提取对下游任务有用的特征：例如，如果您有一个带标签图像的数据集，则可以通过在预训练的编码器顶部放置一个线性层来训练标准分类器。通常会在 [CLS] 标记的顶部放置一个线性层，因为此标记的最后一个隐藏状态可以视为整个图像的表示。\n\nimage_classifier = pipeline(\"image-classification\", \n                            model=\"google/vit-base-patch16-224\", \n                            device=device)\n\nclass_results = image_classifier(image)\npprint(class_results)\n\n[{'label': 'Egyptian cat', 'score': 0.9374418258666992},\n {'label': 'tabby, tabby cat', 'score': 0.03844244033098221},\n {'label': 'tiger cat', 'score': 0.01441139355301857},\n {'label': 'lynx, catamount', 'score': 0.003274328075349331},\n {'label': 'Siamese cat, Siamese', 'score': 0.0006795933004468679}]",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hugging Face</span>"
    ]
  },
  {
    "objectID": "huggingface.html#vit-特征提取",
    "href": "huggingface.html#vit-特征提取",
    "title": "20  Hugging Face",
    "section": "20.8 ViT 特征提取",
    "text": "20.8 ViT 特征提取\n视觉转换器（ViT）模型在 ImageNet-21k 上预训练，包含 1.4 亿张图片和 21843 个类别。其分辨率为 224 x 224。\n\nfrom transformers import ViTImageProcessor, ViTModel\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\ninputs = processor(images=image, return_tensors=\"pt\")\n\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\n\nBaseModelOutputWithPooling 是 Hugging Face 的 transformers 库中的一个类，用于模型输出的表示。这个类通常在模型返回的输出中包含了池化层的结果，这对于一些任务，比如文本分类或嵌入生成，特别有用。\n\n20.8.1 主要功能\nBaseModelOutputWithPooling 类是从 BaseModelOutput 派生而来的，它包含以下几个重要的组件：\n\nlast_hidden_state：模型在所有隐藏层的输出，这些输出通常用于获取序列的特征表示。\npooler_output：经过池化层（通常是池化后的第一个 token）的输出，用于获得序列的整体表示。对于 BERT 等模型，这通常是 [CLS] token 的输出经过池化操作的结果。\nhidden_states（可选）：模型在每个隐藏层的输出（如果 output_hidden_states=True 时会返回）。\n\n\n\n20.8.2 用途\n\npooler_output：这个输出是用来获取序列的整体表示的，例如用于分类任务。对于很多预训练模型来说，这个输出是对 [CLS] token 的表示经过池化后的结果。\nlast_hidden_state：如果你需要对每个 token 的表示进行进一步的处理或分析（例如，进行序列标注任务），这个输出将是有用的。\n\n\n\n20.8.3 示例\n以下是如何在使用 Hugging Face 模型时，利用 BaseModelOutputWithPooling 获取模型输出的一个例子：\n\n# Extract the output\nlast_hidden_state = outputs.last_hidden_state  # Shape: [batch_size, sequence_length, hidden_size]\npooler_output = outputs.pooler_output  # Shape: [batch_size, hidden_size]\n\nprint(\"Last hidden state:\", last_hidden_state.shape)\nprint(\"Pooler output:\", pooler_output.shape)\n\nLast hidden state: torch.Size([1, 197, 768])\nPooler output: torch.Size([1, 768])\n\n\n\n\n20.8.4 说明：\n\nlast_hidden_state：通常是三维张量，形状为 [batch_size, sequence_length, hidden_size]。\npooler_output：通常是二维张量，形状为 [batch_size, hidden_size]，用于表示整个序列的特征。\n\nBaseModelOutputWithPooling 是一个结构化的返回对象，帮助你从模型中提取有用的特征表示，特别是当需要处理序列数据时。",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hugging Face</span>"
    ]
  },
  {
    "objectID": "huggingface.html#总结",
    "href": "huggingface.html#总结",
    "title": "20  Hugging Face",
    "section": "20.9 总结",
    "text": "20.9 总结\nHugging Face 是世界上最大的开源 AI 社区，提供了大量的预训练模型和数据集，支持多种 AI 任务，如自然语言处理、计算机视觉、语音识别等。",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hugging Face</span>"
    ]
  },
  {
    "objectID": "huggingface.html#参考资料",
    "href": "huggingface.html#参考资料",
    "title": "20  Hugging Face",
    "section": "20.10 参考资料",
    "text": "20.10 参考资料\n\nHugging Face 官网\nHugging Face 文档\nHuggingFace 10分钟快速入门（一），利用Transformers，Pipeline探索AI",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Hugging Face</span>"
    ]
  },
  {
    "objectID": "chatanywhere.html",
    "href": "chatanywhere.html",
    "title": "21  ChatAnyWhere 服务",
    "section": "",
    "text": "21.1 模型列表\nChatAnyWhere 提供的模型名字可能会与 OpenAI 不同。例如 *-ca 模型会有更优惠的价格。为了配置调用参数时写对模型名称，需要查询提供的模型列表。\nlibrary(httr)\nlibrary(glue)\n\n# 读取 CHATANYWHERE_API_KEY 环境变量\nOPENAI_API_KEY = Sys.getenv(\"CHATANYWHERE_API_KEY\")\n\nheaders = c(\n   'Authorization' = glue('Bearer {OPENAI_API_KEY}'),\n   'User-Agent' = 'Apifox/1.0.0 (https://apifox.com)',\n   'Content-Type' = 'application/json'\n)\n\n# 使用 GET 方法获取\nres &lt;- VERB(\"GET\", \n            url = \"https://api.chatanywhere.tech/v1/models\", \n            add_headers(headers))\n将 JSON 输出为表格。\nlibrary(jsonlite)\nlibrary(tidyverse)\n\nmodels = jsonlite::fromJSON(content(res, as = \"text\", encoding = \"UTF-8\"))$data\n\nmodels |&gt; \n  arrange(desc(owned_by), id) |&gt; \n  mutate(created = as_datetime(created) |&gt; as_date()) |&gt; \n  kableExtra::kable()\n\n\n\n\nid\nobject\ncreated\nowned_by\n\n\n\n\nchatgpt-4o-latest\nmodel\n2024-05-10\nsystem\n\n\nchatgpt-4o-latest-ca\nmodel\n2024-05-10\nsystem\n\n\ndall-e-2\nmodel\n2023-11-01\nsystem\n\n\ndall-e-3\nmodel\n2023-10-31\nsystem\n\n\ngpt-3.5-turbo-0125\nmodel\n2024-01-23\nsystem\n\n\ngpt-3.5-turbo-1106\nmodel\n2023-11-02\nsystem\n\n\ngpt-3.5-turbo-instruct\nmodel\n2023-08-24\nsystem\n\n\ngpt-3.5-turbo-instruct-0914\nmodel\n2023-09-07\nsystem\n\n\ngpt-4-0125-preview\nmodel\n2024-01-23\nsystem\n\n\ngpt-4-1106-preview\nmodel\n2023-11-02\nsystem\n\n\ngpt-4-turbo\nmodel\n2024-04-05\nsystem\n\n\ngpt-4-turbo-2024-04-09\nmodel\n2024-04-08\nsystem\n\n\ngpt-4-turbo-preview\nmodel\n2024-01-23\nsystem\n\n\ngpt-4o\nmodel\n2024-05-10\nsystem\n\n\ngpt-4o-2024-05-13\nmodel\n2024-05-10\nsystem\n\n\ngpt-4o-2024-08-06\nmodel\n2024-05-10\nsystem\n\n\ngpt-4o-2024-11-20\nmodel\n2024-05-10\nsystem\n\n\ngpt-4o-audio-preview\nmodel\n2024-05-10\nsystem\n\n\ngpt-4o-audio-preview-2024-10-01\nmodel\n2024-05-10\nsystem\n\n\ngpt-4o-mini\nmodel\n2024-05-10\nsystem\n\n\ngpt-4o-mini-2024-07-18\nmodel\n2024-05-10\nsystem\n\n\ngpt-4o-mini-2024-07-18-ca\nmodel\n2024-05-10\nsystem\n\n\ngpt-4o-mini-ca\nmodel\n2024-05-10\nsystem\n\n\ntext-embedding-3-large\nmodel\n2024-01-22\nsystem\n\n\ntext-embedding-3-small\nmodel\n2024-01-22\nsystem\n\n\ntts-1-1106\nmodel\n2023-11-03\nsystem\n\n\ntts-1-hd\nmodel\n2023-11-03\nsystem\n\n\ntts-1-hd-1106\nmodel\n2023-11-03\nsystem\n\n\ngpt-3.5-turbo-16k\nmodel\n2023-05-10\nopenai-internal\n\n\ntext-embedding-ada-002\nmodel\n2022-12-16\nopenai-internal\n\n\ntts-1\nmodel\n2023-04-19\nopenai-internal\n\n\nwhisper-1\nmodel\n2023-02-27\nopenai-internal\n\n\ngpt-3.5-turbo\nmodel\n2023-02-28\nopenai\n\n\ngpt-3.5-turbo-16k-0613\nmodel\n2023-05-30\nopenai\n\n\ngpt-4\nmodel\n2023-03-12\nopenai\n\n\ngpt-4-0613\nmodel\n2023-06-12\nopenai\n\n\nclaude-3-5-haiku-20241022\nmodel\n2023-03-12\nca\n\n\nclaude-3-5-sonnet-20240620\nmodel\n2023-03-12\nca\n\n\nclaude-3-5-sonnet-20241022\nmodel\n2023-03-12\nca\n\n\ndeepseek-reasoner\nmodel\n2024-01-23\nca\n\n\ngemini-1.5-flash-latest\nmodel\n2023-03-12\nca\n\n\ngemini-1.5-pro-latest\nmodel\n2023-03-12\nca\n\n\ngemini-2.0-flash\nmodel\n2023-03-12\nca\n\n\ngemini-2.0-flash-exp\nmodel\n2023-03-12\nca\n\n\ngemini-2.0-flash-thinking-exp\nmodel\n2023-03-12\nca\n\n\ngemini-2.0-pro-exp-02-05\nmodel\n2023-03-12\nca\n\n\ngemini-exp-1206\nmodel\n2023-03-12\nca\n\n\ngpt-3.5-turbo-ca\nmodel\n2024-01-23\nca\n\n\ngpt-4-ca\nmodel\n2023-03-12\nca\n\n\ngpt-4-turbo-ca\nmodel\n2024-01-23\nca\n\n\ngpt-4-turbo-preview-ca\nmodel\n2024-01-23\nca\n\n\ngpt-4o-ca\nmodel\n2024-01-23\nca\n\n\no1\nmodel\n2024-01-23\nca\n\n\no1-2024-12-17\nmodel\n2024-01-23\nca\n\n\no1-mini\nmodel\n2024-01-23\nca\n\n\no1-mini-2024-09-12\nmodel\n2024-01-23\nca\n\n\no1-mini-ca\nmodel\n2024-01-23\nca\n\n\no1-preview\nmodel\n2024-01-23\nca\n\n\no1-preview-2024-09-12\nmodel\n2024-01-23\nca\n\n\no1-preview-ca\nmodel\n2024-01-23\nca\n\n\no3-mini\nmodel\n2024-01-23\nca\n\n\no3-mini\nmodel\n2024-01-23\nca\n下面依次介绍这些模型的用法。",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>ChatAnyWhere 服务</span>"
    ]
  },
  {
    "objectID": "chatanywhere.html#对话模型",
    "href": "chatanywhere.html#对话模型",
    "title": "21  ChatAnyWhere 服务",
    "section": "21.2 对话模型",
    "text": "21.2 对话模型\n以 gpt-* 开头的都是文本对话模型。调用 OpenAI 的模型时，通常需要配置以下一些关键参数来控制模型的行为和生成结果的方式。\n\nPrompt（提示）\n\n模型的输入文本，通常称为“提示”。\n可以是简单的文本，或者带有一些问题或任务描述，告诉模型生成哪类内容。\n\nMax Tokens（最大令牌数）\n\n指定生成的文本中最多包含多少个令牌（tokens）。一个令牌大约对应一个英文单词或标点符号。\n该参数可以控制生成的响应长度，但包括输入和输出在内的令牌总数不能超过模型的上下文长度限制。\n\nTemperature（温度）\n\n控制生成文本的随机性。范围是 0 到 2：\n\ntemperature=0 时，输出更加确定和保守，偏向生成常见的或“最可能”的答案。\n较高的 temperature 值（如 0.7）会让生成的内容更加随机和多样化。\n\n\nn（生成次数）\n\n控制生成多少个不同的响应。\nn=1 只生成一个响应；n=2 会生成多个响应，适合比较或选择最合适的内容。\n\n\n调用 OpenAI 模型时，通常需要设置 模型名称、提示、最大令牌数、温度、top-p、生成次数 等参数，视任务需求还可以调整 出现惩罚、频率惩罚、停止序列 等其他配置，以控制生成的内容质量和行为。\n下面这个例子，展示了 ChatGPT 数不清楚“temperature”这个单词里面有几个字母“e”。\n\nbody = '{\n   \"model\": \"gpt-4o-mini\",\n   \"messages\": [\n      {\n         \"role\": \"system\",\n         \"content\": \"You are a helpful assistant.\"\n      },\n      {\n         \"role\": \"user\",\n         \"content\": \"Temperature这个单词中含有几个字母e？\"\n      }\n   ],\n   \"temperature\": 2\n}';\n\nres &lt;- VERB(\"POST\", \n            url = \"https://api.chatanywhere.tech/v1/chat/completions\", \n            body = body, \n            add_headers(headers))\n\ncontent(res, 'text', encoding = \"UTF-8\") |&gt; \n  fromJSON() |&gt; \n  str()\n\nList of 7\n $ id                : chr \"chatcmpl-B2EwCpPaf2ogu7rCBvY18JrNcfXrT\"\n $ choices           :'data.frame': 1 obs. of  4 variables:\n  ..$ index        : int 0\n  ..$ message      :'data.frame':   1 obs. of  2 variables:\n  .. ..$ role   : chr \"assistant\"\n  .. ..$ content: chr \"单词 \\\"temperature\\\" 中含有三个字母 \\\"e\\\"。\"\n  ..$ logprobs     : logi NA\n  ..$ finish_reason: chr \"stop\"\n $ created           : int 1739873776\n $ model             : chr \"gpt-4o-mini-2024-07-18\"\n $ object            : chr \"chat.completion\"\n $ usage             :List of 5\n  ..$ prompt_tokens            : int 29\n  ..$ completion_tokens        : int 15\n  ..$ total_tokens             : int 44\n  ..$ completion_tokens_details:List of 2\n  .. ..$ audio_tokens    : int 0\n  .. ..$ reasoning_tokens: int 0\n  ..$ prompt_tokens_details    :List of 2\n  .. ..$ audio_tokens : int 0\n  .. ..$ cached_tokens: int 0\n $ system_fingerprint: chr \"fp_b045b4af17\"",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>ChatAnyWhere 服务</span>"
    ]
  },
  {
    "objectID": "chatanywhere.html#使用-openai-api",
    "href": "chatanywhere.html#使用-openai-api",
    "title": "21  ChatAnyWhere 服务",
    "section": "21.3 使用 OpenAI API",
    "text": "21.3 使用 OpenAI API\nOpenAI 的原生 API 使用 Python 语言，以下是使用 Python 的示例代码。\n\nfrom openai import OpenAI\nimport os\nfrom IPython.display import Markdown\n\n# 创建 client\nclient = OpenAI(\n    api_key=os.getenv(\"CHATANYWHERE_API_KEY\"), # 如果您没有配置环境变量，请在此处用您的API Key进行替换\n    base_url=\"https://api.chatanywhere.tech\",  # 填写 openAI 服务的 base_url\n)\n\n# 生成对话\ncompletion = client.chat.completions.create(\n    model=\"gpt-4o-ca\",\n    messages=[\n        {'role': 'system', 'content': 'You are a helpful assistant.'},\n        {'role': 'user', 'content': '你是谁'}],\n    temperature=0.8,\n    top_p=0.8\n    )\n\nMarkdown(completion.choices[0].message.content)\n\n我是一个人工智能助手，旨在为你提供帮助、回答问题、解决问题或陪你聊天！你可以叫我助手，或者随便给我取个名字哦～有什么我可以帮你的吗？\n\n\n在 R 中，ellmer 包主要用于构建和管理 LLM 交互，你可以使用 ellmer 达到类似的效果。以下是等效的 R 代码：\n\nlibrary(ellmer)\n\n# 创建客户端\nclient &lt;- chat_openai(\n  api_key = Sys.getenv(\"CHATANYWHERE_API_KEY\"), # 也可以直接填入 API Key\n  system_prompt = \"You are a helpful assistant.\",\n  base_url = \"https://api.chatanywhere.tech\",\n  model = \"gpt-4o-ca\"\n)\n\n# 生成对话\nclient$chat(\"你是谁\")\n\n我是一个人工智能助手，可以用来回答问题、提供建议并协助你处理各种任务。有任何需要帮助的地方，请随时告诉我！😊\n\n\n这段代码使用 ellmer 包的 OpenAIClient 进行 API 调用，chat_completions_create() 生成对话，与 Python 代码的逻辑基本一致。",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>ChatAnyWhere 服务</span>"
    ]
  },
  {
    "objectID": "chatanywhere.html#词嵌入模型",
    "href": "chatanywhere.html#词嵌入模型",
    "title": "21  ChatAnyWhere 服务",
    "section": "21.4 词嵌入模型",
    "text": "21.4 词嵌入模型\n所有三个词嵌入模型都是基于 Transformer 架构，这使得它们在处理自然语言时具有良好的性能。下表比较了 text-embedding-ada-002、text-embedding-3-small 和 text-embedding-3-large 这三个词嵌入模型的特点：\n\n\n\n\n\n\n\n\n\n\n\n\n模型名称\n参数量\n嵌入维度\n性能\n适用场景\n优点\n缺点\n\n\n\n\ntext-embedding-ada-002\n中等\n1536\n高效，性能优秀\n通用文本嵌入，适用于广泛的 NLP 任务\n高精度嵌入，适合各种语义匹配任务\n相比小型模型，计算资源需求较高\n\n\ntext-embedding-3-small\n小\n512\n较快，资源效率高\n资源受限的应用场景，低计算成本的嵌入生成\n计算效率高，适合实时或资源有限的场景\n嵌入维度较低，可能影响语义表达能力\n\n\ntext-embedding-3-large\n大\n2048\n更高性能，精度极高\n高端应用场景，如高精度语义搜索和推荐系统\n嵌入维度更高，能够捕捉复杂语义关系\n资源消耗大，适合计算资源充足的场景\n\n\n\n用 R 语言调用词嵌入模型。\n\nbody = '{\n   \"model\": \"text-embedding-ada-002\",\n   \"input\": \"The food was delicious and the waiter...\"\n}';\n\nres = VERB(\"POST\", \n           url = \"https://api.chatanywhere.tech/v1/embeddings\", \n           body = body, \n           add_headers(headers))\n\ncontent = content(res, 'text', encoding = \"UTF-8\")\n\nembedding = fromJSON(content)\nstr(embedding)\n\nList of 4\n $ object: chr \"list\"\n $ data  :'data.frame': 1 obs. of  3 variables:\n  ..$ object   : chr \"embedding\"\n  ..$ embedding:List of 1\n  .. ..$ : num [1:1536] 0.00231 -0.00933 0.0158 -0.00778 -0.00469 ...\n  ..$ index    : int 0\n $ model : chr \"ada\"\n $ usage :List of 2\n  ..$ prompt_tokens: int 8\n  ..$ total_tokens : int 8\n\n\n等效的 Python 代码如下：\n\nimport requests\nimport json\nimport os\n\n# API 端点和密钥\nurl = \"https://api.chatanywhere.tech/v1/embeddings\"\nheaders = {\n    \"Authorization\": f\"Bearer {os.getenv('CHATANYWHERE_API_KEY')}\",\n    \"Content-Type\": \"application/json\"\n}\n\n# 请求体\ndata = {\n    \"model\": \"text-embedding-ada-002\",\n    \"input\": \"The food was delicious and the waiter...\"\n}\n\n# 发送请求\nresponse = requests.post(url, headers=headers, json=data)\n\n# 解析响应\nembedding = response.json()\nprint(len(embedding[\"data\"][0][\"embedding\"]))\n\n1536\n\n\n这个 Python 版本使用 requests 发送 POST 请求，并解析返回的 JSON，效果和你的 R 代码相同。\n可以使用 openai 模块，代码会更简洁：\n\nfrom openai import OpenAI\nfrom pprint import pprint\nimport os\n\nclient = OpenAI(api_key=os.getenv(\"CHATANYWHERE_API_KEY\"), base_url=\"https://api.chatanywhere.tech\")\n\nresponse = client.embeddings.create(model=\"text-embedding-ada-002\", input=\"The food was delicious and the waiter...\")\n\npprint(response.dict(), depth=3)  # 以字典格式输出\n\n{'data': [{'embedding': [...], 'index': 0, 'object': 'embedding'}],\n 'model': 'ada',\n 'object': 'list',\n 'usage': {'prompt_tokens': 8, 'total_tokens': 8}}",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>ChatAnyWhere 服务</span>"
    ]
  },
  {
    "objectID": "chatanywhere.html#文生图模型",
    "href": "chatanywhere.html#文生图模型",
    "title": "21  ChatAnyWhere 服务",
    "section": "21.5 文生图模型",
    "text": "21.5 文生图模型\ndall-e-2 和 dall-e-3 是文生图模型。\nDALL-E 2 支持以下三种图像尺寸：\n\n256x256\n512x512\n1024x1024\n\nDALL-E 3 支持以下图像尺寸：\n\n1024x1024: 正方形图像，适合大多数使用场景，是默认推荐的尺寸。\n1792x1024: 宽屏图像，适合需要更宽视野的场景或横向布局的设计。\n1024x1792: 纵向图像，适合需要更高视野的场景或纵向布局的设计。\n\n你可以根据具体需求选择合适的图像尺寸进行生成。\n\nurl = \"https://api.chatanywhere.tech/v1/images/generations\"\n\nbody = '{\n   \"prompt\": \"A colorful sunset over the snow mountains\",\n   \"n\": 1,\n   \"model\":  \"dall-e-3\",\n   \"size\": \"1792x1024\"\n}';\n\nresponse = VERB(\"POST\", url, body = body, add_headers(headers))\n\ncontent = content(response, \"text\", encoding = \"UTF-8\")\ncontent |&gt; fromJSON()\n\n$created\n[1] 1739873805\n\n$data\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       url\n1 https://oaidalleapiprodscus.blob.core.windows.net/private/org-ZNOJ52bwkaITA98ta51mnxPY/user-lGnbjlc7o2MKy3tKztNgR2Qc/img-vIJm3RQInWWKL7ORFxg5bG6J.png?st=2025-02-18T09%3A16%3A44Z&se=2025-02-18T11%3A16%3A44Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-02-17T16%3A50%3A07Z&ske=2025-02-18T16%3A50%3A07Z&sks=b&skv=2024-08-04&sig=pnTLPWMhdGR1wkAfau9FSulTKSaygUCTbhhZshoaWpQ%3D\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              revised_prompt\n1 Vivid hues of orange, pink, and purple fill the sky, setting an enchanting backdrop for the day's end. These colors are reflected in a serene, undulating pattern on the glossy surface of a calm lake in the foreground. In the middle distance, a range of snow-capped mountains stretches across the horizontal plane, their peaks appearing as sharp, silhouetted triangles against the dramatic skyscape. The pure white snow on the mountains contrasting with the darkening twilight sky, all bathed in the diminishing warm glow of the sunset, creates a breathtaking panorama of natural beauty.\n\n\n获取图片。\n# 获取生成的图像 URL\nimage_url = fromJSON(content)[[\"data\"]][[\"url\"]]\n\n# 下载图片\nresponse &lt;- GET(image_url)\n\n# 检查请求是否成功\nif (status_code(response) == 200) {\n  # 将图片保存到磁盘\n  writeBin(content(response, \"raw\"), \"output/sunset.png\")\n  cat(\"图片已成功保存到 `output/sunset.png`。\")\n} else {\n  cat(\"下载图片失败，状态码：\", status_code(response), \"\\n\")\n}\n图片已成功保存到 output/sunset.png。",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>ChatAnyWhere 服务</span>"
    ]
  },
  {
    "objectID": "chatanywhere.html#识图功能不支持",
    "href": "chatanywhere.html#识图功能不支持",
    "title": "21  ChatAnyWhere 服务",
    "section": "21.6 识图功能（不支持）",
    "text": "21.6 识图功能（不支持）\n使用多模态模型，可以识别图片中的信息。\n\n# 设置请求体\nbody = list(\n  model = \"gpt-4o-ca\",\n  file = upload_file(\"output/sunset.png\"),  # 文件路径\n  prompt = \"这是什么?\",  # 提示\n  encode = \"multipart\"\n  )\n\nres = VERB(\"POST\", \n            url = \"https://api.chatanywhere.tech/v1/chat/completions\", \n            body = body, \n            add_headers(headers))\n\ncontent(res, 'text', encoding = \"UTF-8\") |&gt; fromJSON()\n\n$error\n$error$message\n[1] \"JSON parse error: Unexpected character ('-' (code 45)) in numeric value: expected digit (0-9) to follow minus sign, for valid numeric value; nested exception is com.fasterxml.jackson.core.JsonParseException: Unexpected character ('-' (code 45)) in numeric value: expected digit (0-9) to follow minus sign, for valid numeric value\\n at [Source: (org.springframework.util.StreamUtils$NonClosingInputStream); line: 1, column: 3]\"\n\n$error$type\n[1] \"chatanywhere_error\"\n\n$error$param\nNULL\n\n$error$code\n[1] \"500 INTERNAL_SERVER_ERROR\"",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>ChatAnyWhere 服务</span>"
    ]
  },
  {
    "objectID": "chatanywhere.html#文字转语音模型",
    "href": "chatanywhere.html#文字转语音模型",
    "title": "21  ChatAnyWhere 服务",
    "section": "21.7 文字转语音模型",
    "text": "21.7 文字转语音模型\n将一段文字转变为语音，支持中英文混合。\n\nbody = '{\n   \"model\": \"tts-1\",\n   \"input\": \"今天天气不错。It is a nice day today.\",\n   \"voice\": \"alloy\"\n}';\n\nres &lt;- VERB(\"POST\", \n            url = \"https://api.chatanywhere.tech/v1/audio/speech\", \n            body = body, \n            add_headers(headers))\n\n# 检查请求是否成功\nif (status_code(res) == 200) {\n  # 将响应保存为音频文件（假设返回的是二进制音频数据）\n  audio_file &lt;- \"output/audio-goodday.mp3\"  # 你可以更改文件名和扩展名\n  writeBin(content(res, \"raw\"), audio_file)\n  message(\"Audio saved successfully as: \", audio_file)\n} else {\n  message(\"Request failed with status: \", status_code(res))\n}",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>ChatAnyWhere 服务</span>"
    ]
  },
  {
    "objectID": "chatanywhere.html#语音识别模型",
    "href": "chatanywhere.html#语音识别模型",
    "title": "21  ChatAnyWhere 服务",
    "section": "21.8 语音识别模型",
    "text": "21.8 语音识别模型\nwhisper-1 是 OpenAI 开发的一个强大的语音识别模型。它主要用于将语音转换为文本（也称为语音转文字，Speech-to-Text，简称 STT）。该模型能够处理多种语言的语音输入，并能够识别不同的口音和语音风格，非常适用于各种音频转录任务。\n\nheaders_multipart = c(\n   'Authorization' = glue('Bearer {OPENAI_API_KEY}'),\n   'User-Agent' = 'Apifox/1.0.0 (https://apifox.com)',\n   'Content-Type' = 'multipart/form-data'\n)\n\nbody = list(\n   'file' = upload_file('output/audio-goodday.mp3'),\n   'model' = 'whisper-1',\n   'prompt' = 'eiusmod nulla',\n   'response_format' = 'json',\n   'temperature' = '0',\n   'language' = ''\n)\n\nres = VERB(\"POST\", \n            url = \"https://api.chatanywhere.tech/v1/audio/transcriptions\", \n            body = body, \n            add_headers(headers_multipart),\n            encode = 'multipart')\n\ncat(content(res, 'text', encoding = \"UTF-8\"))\n\n{\"text\":\"今天天氣不錯 It is a nice day today\"}",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>ChatAnyWhere 服务</span>"
    ]
  },
  {
    "objectID": "chatanywhere.html#claude-模型",
    "href": "chatanywhere.html#claude-模型",
    "title": "21  ChatAnyWhere 服务",
    "section": "21.9 Claude 模型",
    "text": "21.9 Claude 模型\nChatAnywhere 提供了一个 claude-3-5-sonnet-20240620 模型。\n\nbody = '{\n   \"model\": \"claude-3-5-sonnet-20240620\",\n   \"messages\": [\n      {\n         \"role\": \"system\",\n         \"content\": \"You are a helpful assistant.\"\n      },\n      {\n         \"role\": \"user\",\n         \"content\": \"Temperature这个单词中含有几个字母e？\"\n      }\n   ],\n   \"temperature\": 2\n}';\n\nres &lt;- VERB(\"POST\", \n            url = \"https://api.chatanywhere.tech/v1/chat/completions\", \n            body = body, \n            add_headers(headers))\n\ncontent(res, 'text', encoding = \"UTF-8\") |&gt; \n  fromJSON() |&gt; \n  str()\n\nList of 7\n $ id                : chr \"chatcmpl-89DL5Y5ZpqhtEco6nQIrwEDEeLCWq\"\n $ choices           :'data.frame': 1 obs. of  4 variables:\n  ..$ index        : int 0\n  ..$ message      :'data.frame':   1 obs. of  2 variables:\n  .. ..$ role   : chr \"assistant\"\n  .. ..$ content: chr \" 在\\\"Temperature\\\"这个单词中有3个字母\\\"e\\\"。\"\n  ..$ logprobs     : logi NA\n  ..$ finish_reason: chr \"stop\"\n $ created           : int 1739873840\n $ model             : chr \"claude-3-5-sonnet-20240620\"\n $ object            : chr \"chat.completion\"\n $ usage             :List of 5\n  ..$ prompt_tokens            : int 30\n  ..$ completion_tokens        : int 20\n  ..$ total_tokens             : int 50\n  ..$ completion_tokens_details: Named list()\n  ..$ prompt_tokens_details    : Named list()\n $ system_fingerprint: NULL",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>ChatAnyWhere 服务</span>"
    ]
  },
  {
    "objectID": "chatanywhere.html#语音翻译模型",
    "href": "chatanywhere.html#语音翻译模型",
    "title": "21  ChatAnyWhere 服务",
    "section": "21.10 语音翻译模型",
    "text": "21.10 语音翻译模型\n将音频翻译成文字（从例子可以看出该模型支持中英文混合音频）。\n\nbody = list(\n   'file' = upload_file('output/audio-goodday.mp3'),\n   'model' = 'whisper-1',\n   'prompt' = '',\n   'response_format' = 'json',\n   'temperature' = '0'\n)\n\nres &lt;- VERB(\"POST\", \n            url = \"https://api.chatanywhere.tech/v1/audio/translations\", \n            body = body, \n            add_headers(headers_multipart), \n            encode = 'multipart')\n\ncat(content(res, 'text', encoding = \"UTF-8\"))\n\n{\"text\":\"今天天气不错。It is a nice day today.\"}",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>ChatAnyWhere 服务</span>"
    ]
  },
  {
    "objectID": "chatanywhere.html#总结",
    "href": "chatanywhere.html#总结",
    "title": "21  ChatAnyWhere 服务",
    "section": "21.11 总结",
    "text": "21.11 总结\n因为特殊原因，在国内不能直接访问到包括 OpenAI、Claude 等在内的大模型服务。\nChatAnyWhere 提供了一个替代方案，可以让你在国内访问这些大模型服务。",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>ChatAnyWhere 服务</span>"
    ]
  },
  {
    "objectID": "chatanywhere.html#参考文献",
    "href": "chatanywhere.html#参考文献",
    "title": "21  ChatAnyWhere 服务",
    "section": "21.12 参考文献",
    "text": "21.12 参考文献\n\nChatAnyWhere API 文档",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>ChatAnyWhere 服务</span>"
    ]
  },
  {
    "objectID": "deepseek.html",
    "href": "deepseek.html",
    "title": "22  DeepSeek R1 模型",
    "section": "",
    "text": "22.1 模型性能\n根据 DeepSeek-R1 论文，DeepSeek-R1 进行了多个基准测试，主要涵盖 数学、编程、常识问答、推理能力 等方面。以下是具体的基准测试类别和对应的数据集：",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>DeepSeek R1 模型</span>"
    ]
  },
  {
    "objectID": "deepseek.html#模型性能",
    "href": "deepseek.html#模型性能",
    "title": "22  DeepSeek R1 模型",
    "section": "",
    "text": "22.1.1 1. 数学（Math）\n\nAIME 2024（American Invitational Mathematics Examination）\n\nMATH-500（数学解题能力测试）\n\nCNMO 2024（中国数学奥林匹克）\n\n\n\n22.1.2 2. 编程（Coding）\n\nLiveCodeBench（编程任务，通过 Chain-of-Thought 方式进行评估）\n\nCodeforces（代码竞赛平台，评估模型在算法竞赛中的 Elo rating 和百分位排名）\n\nSWE-bench Verified（软件工程任务，评估问题解决能力）\n\nAider-Polyglot（多语言代码生成）\n\n\n\n22.1.3 3. 知识与常识问答（General Knowledge & Reasoning）\n\nMMLU（Massive Multitask Language Understanding，多学科知识测评）\n\nMMLU-Redux（MMLU 的加强版，使用 ZeroEval 框架评估）\n\nMMLU-Pro（MMLU 进阶版，考察更复杂的推理能力）\n\nGPQA Diamond（Graduate-level Google-Proof Q&A，评估复杂推理能力）\n\nSimpleQA（基础事实问答）\n\nC-SimpleQA（中文事实问答）\n\n\n\n22.1.4 4. 自然语言处理与生成任务（NLP & Generation Tasks）\n\nDROP（Discrepancy-based Reading Comprehension，阅读理解）\n\nFRAMES（长文本依赖的问答）\n\nIF-Eval（Instruction Following Evaluation，测试指令跟随能力）\n\nAlpacaEval 2.0（长度受控的评估，用 GPT-4-Turbo 作为评判）\n\nArenaHard（基于 GPT-4-Turbo 1106 的对比评测）\n\n\n\n22.1.5 5. 中文任务（Chinese NLP Tasks）\n\nCLUEWSC（中文 Winograd Schema Challenge）\n\nC-Eval（中文大规模学科知识评测）\n\n这些测试覆盖了数学、编程、推理、常识问答、长文本理解以及中文理解等多个领域，并与 OpenAI-o1-1217、Claude 3.5 Sonnet、GPT-4o 进行了对比。",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>DeepSeek R1 模型</span>"
    ]
  },
  {
    "objectID": "deepseek.html#模型训练",
    "href": "deepseek.html#模型训练",
    "title": "22  DeepSeek R1 模型",
    "section": "22.2 模型训练",
    "text": "22.2 模型训练\nDeepSeek R1 的训练过程包括以下几个关键阶段：\n\nDeepSeek-R1-Zero：基于强化学习（RL）的训练\n\n直接对基模型（DeepSeek-V3-Base）进行大规模强化学习（RL），而不依赖监督微调（SFT）。\n采用 Group Relative Policy Optimization (GRPO) 算法来优化策略模型，以减少计算成本。\n通过基于规则的奖励系统进行训练，包括：\n\n准确性奖励（判断答案是否正确，如数学计算或编程测试）。\n格式奖励（确保推理过程遵循特定格式，如 &lt;think&gt; reasoning &lt;/think&gt;）。\n\n经过 RL 训练后，DeepSeek-R1-Zero 在推理基准测试中表现优异，但存在可读性差、语言混杂等问题。\n\nDeepSeek-R1：引入冷启动数据的多阶段训练\n\n冷启动数据（Cold Start Data）：收集大量长链式思维（CoT）数据，对 DeepSeek-V3-Base 进行微调，提供初始推理能力。\n推理强化学习（RL）：\n\n在预训练基础上应用 RL，进一步提升推理能力。\n引入 语言一致性奖励，减少混合语言问题，提高可读性。\n\n拒绝采样与监督微调（SFT）：\n\n使用 RL 训练后的模型生成数据，通过拒绝采样（Rejection Sampling）筛选高质量推理数据（约 60 万条）。\n结合 DeepSeek-V3 在写作、问答等领域的数据进行 SFT。\n\n全场景 RL 调优：\n\n进行二次 RL 训练，以进一步增强推理能力，并优化用户友好性。\n\n\n蒸馏（Distillation）\n\n使用 DeepSeek-R1 生成的 80 万条数据，对小模型（如 Qwen 和 Llama 系列）进行蒸馏。\n结果表明，直接从 DeepSeek-R1 蒸馏出的模型性能优于对小模型直接应用 RL。\n\n\n总结来说，DeepSeek R1 通过强化学习（RL）+冷启动数据（CoT 预训练）+拒绝采样+监督微调+二次 RL的方式训练得到，从而在推理能力上达到了与 OpenAI-o1-1217 相当的水平，同时通过蒸馏技术将推理能力迁移到小模型上。",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>DeepSeek R1 模型</span>"
    ]
  },
  {
    "objectID": "deepseek.html#cot",
    "href": "deepseek.html#cot",
    "title": "22  DeepSeek R1 模型",
    "section": "22.3 CoT",
    "text": "22.3 CoT\nDeepSeek-R1 训练过程中使用的 长链式思维（CoT）数据 主要来自两个阶段：\n\n冷启动数据（Cold Start Data）\n\n数据量：数千条（用于初始化模型的推理能力）\n\n收集方式：\n\n少样本提示（Few-shot Prompting）：使用已有的长 CoT 示例引导模型生成新数据。\n\n直接提示（Direct Prompting）：要求模型生成带有反思和验证的详细回答。\n\n从 DeepSeek-R1-Zero 生成：筛选格式可读的输出作为数据。\n\n人工后处理（Human Post-processing）：人工审查并优化数据质量。\n\n\n拒绝采样（Rejection Sampling）+ 监督微调（SFT）数据\n\n数据量：\n\n推理相关数据：约 60 万条（主要来自 RL 训练后的模型输出）\n\n非推理数据（写作、问答等）：约 20 万条\n\n总计约 80 万条训练样本\n\n\n收集方式：\n\n模型自生成（Self-generation）：从 RL 训练后的模型采样多个答案，并筛选正确的回答。\n\n基于规则的评估（Rule-based Evaluation）：如数学问题使用确定性答案，代码问题使用编译器测试。\n\n生成式奖励模型（Generative Reward Model）：部分数据通过 DeepSeek-V3 进行判断。\n\n格式筛选：去除语言混杂、段落过长、代码格式混乱的 CoT 数据。\n\n\n\n这些数据用于对 DeepSeek-V3-Base 进行 SFT，并进一步强化学习，最终得到 DeepSeek-R1。",
    "crumbs": [
      "大模型API调用",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>DeepSeek R1 模型</span>"
    ]
  },
  {
    "objectID": "neural-network-from-scratch.html",
    "href": "neural-network-from-scratch.html",
    "title": "23  手搓神经网络模型",
    "section": "",
    "text": "23.1 先决条件\n在开始之前，请确保您具备以下知识和环境配置：\n如果您还没有准备好这些环境，请先按照 [./setup-a-reproducible-envionment.qmd] 部分的说明进行设置。",
    "crumbs": [
      "从零开始神经网络",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>手搓神经网络模型</span>"
    ]
  },
  {
    "objectID": "neural-network-from-scratch.html#先决条件",
    "href": "neural-network-from-scratch.html#先决条件",
    "title": "23  手搓神经网络模型",
    "section": "",
    "text": "Python 编程基础：熟悉基本的 Python 语法和常用库\n环境配置：\n\nPython 3.10 或更高版本\nConda 包管理工具\nPyTorch 2.0 或更高版本\nGPU 支持（可选，但推荐）",
    "crumbs": [
      "从零开始神经网络",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>手搓神经网络模型</span>"
    ]
  },
  {
    "objectID": "neural-network-from-scratch.html#概述",
    "href": "neural-network-from-scratch.html#概述",
    "title": "23  手搓神经网络模型",
    "section": "23.2 概述",
    "text": "23.2 概述\n本项目将展示如何利用 PyTorch 从零开始构建一个神经网络模型，以解决手写字母（实际上这里使用的是 MNIST 手写数字）识别问题。本文内容不仅介绍了神经网络的基础知识、训练本质和卷积操作的优势，还给出了完整代码实现，让你能够亲自动手构建并训练模型。",
    "crumbs": [
      "从零开始神经网络",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>手搓神经网络模型</span>"
    ]
  },
  {
    "objectID": "neural-network-from-scratch.html#神经网络模型基础",
    "href": "neural-network-from-scratch.html#神经网络模型基础",
    "title": "23  手搓神经网络模型",
    "section": "23.3 神经网络模型基础",
    "text": "23.3 神经网络模型基础\n在开始代码实现之前，我们先介绍一些神经网络的基本概念和原理。\n\n\n\n23.3.1 什么是神经网络？\n神经网络是一种受人脑神经元连接方式启发而构造的数学模型。它由大量节点（神经元）构成，这些节点以不同的层级进行排列：\n\n输入层：接收外界数据。\n\n隐藏层：对数据进行特征提取与变换。\n\n输出层：给出最终的预测结果。\n\n下面的 Mermaid 图展示了一个简单神经网络的基本结构Figure 23.1：\n\n\n\n\n\n\n\n\nflowchart LR\n  %% 输入层\n  subgraph 输入层\n    A1\n    A2\n    A3\n    A4\n    A5\n  end\n\n  %% 隐藏层 1\n  subgraph 隐藏层 1\n    H11\n    H12\n  end\n\n  %% 隐藏层 2\n  subgraph 隐藏层 2\n    H21\n    H22\n  end\n\n  %% 输出层\n  subgraph 输出层\n    O1((y))\n  end\n\n  %% 连接 输入层 → 隐藏层 1\n  A1 --&gt; H11\n  A1 --&gt; H12\n  A2 --&gt; H11\n  A2 --&gt; H12\n  A3 --&gt; H11\n  A3 --&gt; H12\n  A4 --&gt; H11\n  A4 --&gt; H12\n  A5 --&gt; H11\n  A5 --&gt; H12\n\n  %% 连接 隐藏层 1 → 隐藏层 2\n  H11 --&gt; H21\n  H11 --&gt; H22\n  H12 --&gt; H21\n  H12 --&gt; H22\n\n  %% 连接 隐藏层 2 → 输出层\n  H21 --&gt; O1\n  H22 --&gt; O1\n\n\n\n\n\n\n\n\nFigure 23.1: : 神经网络的基本结构。\n\n\n\n\n\n23.3.2 神经网络训练的本质\n神经网络的训练主要分为以下几个步骤：\n\n前向传播：将输入数据通过网络，得到预测输出。\n\n计算损失：比较预测输出和真实标签之间的误差。\n\n反向传播：根据损失计算梯度，确定每个参数应如何调整。\n\n梯度下降：利用梯度更新网络参数，逐步减小预测误差。\n\n这种不断迭代的过程使得模型能够从数据中学习并不断改进。\n\n\n23.3.3 卷积操作为什么有用？\n卷积操作是卷积神经网络（CNN）的核心，主要有以下优点：\n\n局部特征提取：卷积核能捕捉局部区域内的边缘、纹理等特征。\n\n参数共享：同一卷积核在整个图像上滑动，显著减少模型参数数量。\n\n平移不变性：卷积操作能保证特征检测不受物体在图像中位置变化的影响。\n\n下面的图示意了卷积操作的基本原理：\n\n\n\n\n\n\n\n\n%%{init: {'theme': 'default'}}%%\nflowchart LR\n    subgraph Input_Image[输入图像]\n        A[像素矩阵]\n    end\n    subgraph Convolution[卷积操作]\n        B[滤波器]\n        C[特征提取]\n    end\n    A --&gt; B\n    B --&gt; C\n\n\n\n\n\n\n\n\nFigure 23.2: 卷积操作示意图。",
    "crumbs": [
      "从零开始神经网络",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>手搓神经网络模型</span>"
    ]
  },
  {
    "objectID": "neural-network-from-scratch.html#手写字母识别的历史",
    "href": "neural-network-from-scratch.html#手写字母识别的历史",
    "title": "23  手搓神经网络模型",
    "section": "23.4 手写字母识别的历史",
    "text": "23.4 手写字母识别的历史\n手写字母或数字识别一直是人工智能领域的经典问题。早期研究中，专家需要手工设计特征提取方法来识别图像中的字母或数字。\n随着神经网络，尤其是卷积神经网络（CNN）的出现，系统能够自动学习并提取图像特征，大幅提升了识别准确率。\nMNIST 数据集便是一个经典的例子，它包含了 60000 张训练图像和 10000 张测试图像，每张图像为 28x28 像素的灰度图，广泛用于手写数字识别的教学和研究中。",
    "crumbs": [
      "从零开始神经网络",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>手搓神经网络模型</span>"
    ]
  },
  {
    "objectID": "neural-network-from-scratch.html#配置环境和下载数据集",
    "href": "neural-network-from-scratch.html#配置环境和下载数据集",
    "title": "23  手搓神经网络模型",
    "section": "23.5 配置环境和下载数据集",
    "text": "23.5 配置环境和下载数据集\n在动手构建神经网络之前，请确保你已配置好 Conda 环境并安装所需的软件包。\n\n23.5.1 配置 Conda 环境\n# 创建一个新的 Conda 环境，命名为 pytorch_env，使用 Python 3.10 版本\nconda create -n pytorch_env python=3.10\n\n# 激活该环境\nconda activate pytorch_env\n\n\n23.5.2 安装 PyTorch 和 torchvision\n# 使用 conda 安装 PyTorch 及其相关工具包\nconda install pytorch torchvision torchaudio -c pytorch\n\n\n23.5.3 下载 MNIST 数据集\n在下面的代码中，我们将利用 torchvision 自动下载 MNIST 数据集。这个数据集包含手写数字图像，是机器学习领域的经典数据集。\n\n# 导入必要的库\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\n\n定义图像预处理流程：\n\ntransforms.ToTensor() 将 PIL 图像或 numpy 数组转换为 tensor，并将像素值归一化到 [0, 1] 范围内\ntransforms.Normalize() 进一步将数据标准化，均值和标准差是针对 MNIST 数据集计算得到的\n\n\n# 定义图像预处理流程\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\n# 下载并加载数据集\ntrain_dataset = datasets.MNIST(\n    root='./data',\n    train=True,\n    download=True,\n    transform=transform\n)\n\ntest_dataset = datasets.MNIST(\n    root='./data',\n    train=False,\n    download=True,\n    transform=transform\n)\n\n\n\n23.5.4 绘制数据集\n绘制 12 张训练集和 4 张测试集图像，并在图上右下角标出数据集图像的id。\n\nplt.figure(figsize=(10, 6))\n\n# 绘制训练集图像\nplt.title(\"MNIST Dataset Examples\")\nfor i in range(21): # 绘制 21 张训练集图像\n    plt.subplot(4, 7, i+1) # 绘制第 i+1 张图像\n    plt.axis(\"off\") # 不显示坐标轴\n    img = train_dataset[i][0].squeeze() # 获取第 i 张图像\n    label = train_dataset[i][1] # 获取第 i 张图像的标签\n    plt.imshow(img, cmap=\"gray\") # 绘制第 i 张图像\n    plt.text(18, 26, f\"{label}\", fontsize=10, color=\"red\") # 在图像右下角标出红色标签\n\n# 绘制测试集图像\nfor i in range(7): # 绘制 7 张测试集图像\n    plt.subplot(4, 7, i+22) # 绘制第 i+22 张图像\n    plt.axis(\"off\") # 不显示坐标轴\n    img = test_dataset[i][0].squeeze() # 获取第 i 张图像\n    label = test_dataset[i][1] # 获取第 i 张图像的标签\n    plt.imshow(img, cmap=\"gray\") # 绘制第 i 张图像\n    plt.text(20, 25, f\"{label}\", fontsize=10, color=\"green\") # 在图像右下角标出绿色标签\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 23.3: 手写数字示例（红色取自训练集，绿色取自测试集）",
    "crumbs": [
      "从零开始神经网络",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>手搓神经网络模型</span>"
    ]
  },
  {
    "objectID": "neural-network-from-scratch.html#构建-lenet-神经网络模型",
    "href": "neural-network-from-scratch.html#构建-lenet-神经网络模型",
    "title": "23  手搓神经网络模型",
    "section": "23.6 构建 LeNet 神经网络模型",
    "text": "23.6 构建 LeNet 神经网络模型\nLeNet 是最早用于手写数字识别的卷积神经网络之一，其结构包括卷积层、池化层和全连接层。下面我们将从零开始搭建 LeNet 模型。\n\n23.6.1 构建 LeNet 模型\n下面代码定义了 LeNet 模型，其中包含两个卷积层、两个池化层和三个全连接层。每一步均附有详细注释。\n\n# 定义 LeNet 神经网络模型类，继承自 nn.Module\nclass LeNet(nn.Module):\n    def __init__(self):\n        # 初始化父类 nn.Module\n        super(LeNet, self).__init__()\n        # 第一个卷积层：\n        # 输入通道：1（灰度图像），输出通道：6，卷积核大小：5x5\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n        \n        # 定义池化层：\n        # 使用 2x2 的最大池化，能够减小特征图的尺寸\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        # 第二个卷积层：\n        # 输入通道：6，输出通道：16，卷积核大小：5x5\n        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n        \n        # 第一个全连接层：\n        # 输入特征数为 16*4*4（经过两次卷积和池化后的特征图尺寸），输出特征数为 120\n        self.fc1 = nn.Linear(in_features=16*4*4, out_features=120)\n        \n        # 第二个全连接层：将 120 个特征映射到 84 个特征\n        self.fc2 = nn.Linear(in_features=120, out_features=84)\n        \n        # 第三个全连接层：输出 10 个类别，对应 MNIST 中 10 个数字\n        self.fc3 = nn.Linear(in_features=84, out_features=10)\n\n    def forward(self, x):\n        # 将输入通过第一个卷积层，并使用 ReLU 激活函数增加非线性\n        x = torch.relu(self.conv1(x))\n        # 应用池化层，减小特征图尺寸\n        x = self.pool(x)\n        # 第二个卷积层 + ReLU 激活\n        x = torch.relu(self.conv2(x))\n        # 再次池化\n        x = self.pool(x)\n        # 将多维特征图展平为一维向量，为全连接层做准备\n        x = x.view(-1, 16*4*4)\n        # 第一个全连接层 + ReLU 激活\n        x = torch.relu(self.fc1(x))\n        # 第二个全连接层 + ReLU 激活\n        x = torch.relu(self.fc2(x))\n        # 第三个全连接层得到最终输出（未经过激活，后续会结合损失函数使用）\n        x = self.fc3(x)\n        return x\n\n代码说明：\n本部分代码定义了 LeNet 模型。通过两个卷积层和池化层逐步提取图像特征，再通过全连接层进行分类。注意，由于 MNIST 图像尺寸为 28×28，经过两次卷积和池化后，特征图尺寸正好为 4×4（通道数为 16），因此全连接层的输入特征数为 16*4*4。\n\n\n23.6.2 LeNet 模型结构图\n初始化一个 LeNet 模型，并打印其结构。\n\n# 打印模型结构\nmodel = LeNet()\nprint(model)\n\nLeNet(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=256, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=10, bias=True)\n)\n\n\n让我们详细解释一下模型的每一层结构：\n\n第一个卷积层 (conv1)：\n\n输入：1 个通道（灰度图像）\n输出：6 个特征图\n卷积核：5×5\n步长：1\n输入尺寸：28×28 → 输出尺寸：24×24\n\n第一个池化层 (pool)：\n\n池化窗口：2×2\n步长：2\n输入尺寸：24×24 → 输出尺寸：12×12\n\n第二个卷积层 (conv2)：\n\n输入：6 个通道\n输出：16 个特征图\n卷积核：5×5\n步长：1\n输入尺寸：12×12 → 输出尺寸：8×8\n\n第二个池化层 (pool)：\n\n池化窗口：2×2\n步长：2\n输入尺寸：8×8 → 输出尺寸：4×4\n\n第一个全连接层 (fc1)：\n\n输入：256 个特征（16×4×4）\n输出：120 个神经元\n\n第二个全连接层 (fc2)：\n\n输入：120 个特征\n输出：84 个神经元\n\n第三个全连接层 (fc3)：\n\n输入：84 个特征\n输出：10 个神经元（对应 10 个数字类别）\n\n\n数据流向说明：\n\n输入的 28×28 图像首先经过第一个卷积层，生成 6 个 24×24 的特征图\n经过池化层后，特征图变为 6 个 12×12\n第二个卷积层将特征图转换为 16 个 8×8 的特征图\n再次池化后，得到 16 个 4×4 的特征图\n将特征图展平为一维向量（16×4×4 = 256）\n通过三个全连接层逐步将特征降维，最终输出 10 个类别的概率分布\n\n这种结构设计使得网络能够逐层提取图像的特征，从低级的边缘特征到高级的抽象特征，最终实现手写数字的分类。",
    "crumbs": [
      "从零开始神经网络",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>手搓神经网络模型</span>"
    ]
  },
  {
    "objectID": "neural-network-from-scratch.html#模型训练和评估",
    "href": "neural-network-from-scratch.html#模型训练和评估",
    "title": "23  手搓神经网络模型",
    "section": "23.7 模型训练和评估",
    "text": "23.7 模型训练和评估\n接下来，我们将编写训练和测试的代码，并整合到主函数中，实现对模型的训练和评估。\n\n23.7.1 创建数据加载器\n在训练深度学习模型时，我们通常需要 创建数据加载器（DataLoader），其主要作用如下：\n1. 方便批量处理（Mini-Batch）\n训练时，我们不会一次性输入所有数据，而是 按批次（Batch）输入，这样可以：\n\n提高计算效率：GPU 并行处理多个样本，比逐个样本计算更快。\n稳定梯度下降：批量计算梯度，减少随机性，提高模型收敛速度。\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n上面代码表示 每次取 64 个样本进行训练。\n2. 随机打乱数据（Shuffle）\n如果不打乱数据，模型可能会 学习到数据的顺序，而不是特征模式。\nshuffle=True 确保每个 epoch 训练时，样本顺序是随机的，防止模型过拟合于数据的排列方式。\n3. 自动并行加载数据\nDataLoader 允许使用 多线程并行加载数据，这样可以加快训练：\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n\nnum_workers=4 表示使用 4 个进程同时加载数据，提高效率。\n\n4. 方便数据预处理\n在 DataLoader 中，我们可以添加 transforms（数据增强），比如：\n\n标准化（Normalization）\n数据扩增（Random Flip, Crop, Rotate）\n\n例如，对 MNIST 进行标准化：\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\ntrain_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n这样，我们就能高效地进行深度学习训练！🚀\n\n# 创建数据加载器\ntrain_loader = torch.utils.data.DataLoader(\n    dataset=train_dataset,\n    batch_size=64,\n    shuffle=True\n)\n\ntest_loader = torch.utils.data.DataLoader(\n    dataset=test_dataset,\n    batch_size=1000,\n    shuffle=False\n)\n\n\n\n23.7.2 定义训练函数\n训练函数中，模型对每个批次数据进行前向传播，计算损失后进行反向传播，并使用优化器更新权重。每隔一定批次输出当前损失，方便观察训练进度。\n\n# 定义训练函数，用于在训练集上训练模型\ndef train(model, device, train_loader, optimizer, criterion, epoch):\n    model.train()\n    train_loss = 0\n    correct = 0\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        \n        # 累计损失和正确预测数\n        train_loss += loss.item() * data.size(0)\n        pred = output.argmax(dim=1, keepdim=True)\n        correct += pred.eq(target.view_as(pred)).sum().item()\n        \n        if batch_idx % 5000 == 0:\n            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}]\\tLoss: {loss.item():.6f}\")\n    \n    # 计算平均损失和准确率\n    train_loss /= len(train_loader.dataset)\n    accuracy = 100. * correct / len(train_loader.dataset)\n    return train_loss, accuracy\n\n函数原理分析\nLeNet训练函数的核心流程可分为前向传播、损失计算、反向传播、参数更新四个阶段。以下是代码各环节与CNN训练原理的对应关系：\n\n前向传播阶段\n\nmodel(data) 执行卷积神经网络的前向计算\nLeNet结构依次执行：卷积→池化→卷积→池化→全连接→全连接\n卷积层通过滤波器提取空间特征，池化层降低特征图维度，全连接层完成分类\n\n损失计算阶段\n\ncriterion(output, target) 使用交叉熵损失函数\n该损失函数适用于多分类任务，衡量预测概率分布与真实标签的差异\n损失值反映当前参数下模型的预测误差程度\n\n反向传播阶段\n\nloss.backward() 自动计算梯度\n通过链式法则逐层计算卷积核参数和全连接层权重的梯度\n梯度值表征各参数对最终损失的贡献程度\n\n参数优化阶段\n\noptimizer.step() 根据梯度更新参数\n典型优化器如SGD的更新公式：\\(w_{t+1} = w_t - \\eta \\nabla L(w_t)\\)\n学习率\\(\\eta\\)控制参数更新步长，需合理设置避免震荡或收敛过慢\n\n\n关键实现细节\n-梯度管理\n\noptimizer.zero_grad() 在每次迭代前清零梯度，防止梯度累积\nPyTorch默认会累加梯度，手动清零确保每次更新基于当前批次数据\n设备迁移\n\ndata.to(device) 将数据转移到GPU/CPU\n利用GPU并行计算加速卷积运算，这对大规模数据训练至关重要\n\n训练监控\n\n每5000批次输出进度信息，帮助监控训练过程\n累计损失计算需乘以data.size(0)，因PyTorch损失默认返回批次平均值\n准确率计算通过比较预测最大值索引与真实标签实现\n\n\nLeNet训练特点\n\n特征学习机制\n\n通过交替的卷积和池化操作，网络自动学习层次化特征\n浅层卷积捕捉边缘等低级特征，深层卷积提取复杂模式\n\n参数优化策略\n\n卷积核参数通过梯度下降自动优化\n权重初始化通常采用Xavier或He方法，保证训练稳定性\n\n泛化能力提升\n\n池化操作增强平移不变性\n后续改进版本可加入Dropout层防止过拟合\n\n训练效果评估\n\n最终返回epoch平均损失和准确率\n这些指标用于跟踪模型在训练集上的学习进度\n需配合验证集评估真实泛化能力\n\n\n该训练函数实现了标准监督学习流程，通过多次epoch迭代不断优化网络参数，使模型逐步提升特征提取和分类能力。实际应用中还需配合验证集监控、学习率调整等策略以获得最佳效果。\n\n\n23.7.3 定义测试函数\n\n# 定义测试函数，用于评估模型在测试集上的表现\ndef test(model, device, test_loader, criterion):\n    model.eval()  # 将模型设置为评估模式，关闭 dropout 等训练特性\n    test_loss = 0  # 初始化测试损失\n    correct = 0    # 初始化预测正确的样本计数\n    all_preds = []  # 用于存储所有预测结果\n    all_targets = []  # 用于存储所有真实标签\n    \n    # 在测试阶段不计算梯度，节省内存和加快计算速度\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += criterion(output, target).item() * data.size(0)\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n            \n            # 收集预测结果和真实标签\n            all_preds.extend(pred.cpu().numpy().flatten())\n            all_targets.extend(target.cpu().numpy())\n    \n    test_loss /= len(test_loader.dataset)  # 计算平均损失\n    accuracy = 100. * correct / len(test_loader.dataset)  # 计算准确率\n    \n    # 计算混淆矩阵\n    from sklearn.metrics import confusion_matrix\n    cm = confusion_matrix(all_targets, all_preds)\n    \n    return test_loss, accuracy, cm\n\n代码说明：\n测试函数中，模型在测试集上进行前向传播，并累计计算总体损失与正确预测数量，最终输出平均损失及准确率，以评估模型的泛化能力。\n\n\n23.7.4 训练过程中的损失指标\n在训练神经网络时，我们主要关注两个重要的损失指标：\n\n训练损失（Training Loss）：\n\n表示模型在训练数据集上的预测误差\n反映了模型对训练数据的拟合程度\n训练损失持续下降表明模型正在学习数据中的模式\n但过低的训练损失可能意味着过拟合\n\n测试损失（Test Loss）：\n\n表示模型在从未见过的测试数据上的预测误差\n反映了模型的泛化能力\n测试损失应该与训练损失保持相近\n如果测试损失明显高于训练损失，说明模型可能过拟合\n\n\n理想的训练过程应该表现为： - 训练损失和测试损失同时下降 - 两者之间保持较小的差距 - 最终都收敛到一个较低的水平\n如果观察到以下情况，则需要调整模型或训练策略： - 训练损失持续下降但测试损失上升：过拟合的典型特征 - 两种损失都居高不下：欠拟合，可能需要增加模型复杂度 - 损失剧烈波动：学习率可能过大\n\n\n23.7.5 主函数：训练与评估模型\n模型训练推荐使用 CUDA 或 MPS 进行训练（GPU），如果 CUDA 或 MPS 不可用，则使用 CPU 进行训练。\n\n# 主函数：训练与评估模型\n# 检查是否有 GPU 可用，否则使用 CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n\n# 实例化 LeNet 模型，并移动到指定设备上\nmodel = LeNet().to(device)\n\n\n\n23.7.6 优化器与损失函数\n在神经网络训练中，优化器和损失函数是两个核心组件：\n\n随机梯度下降优化器（SGD）：\n\n原理：通过计算损失函数对模型参数的梯度，沿着梯度的反方向更新参数\n学习率：控制每次参数更新的步长（这里设为 0.01）\n动量：\n\n作用：累积之前的梯度方向，帮助模型跳出局部最小值\n数值：这里设为 0.9，表示保留 90% 的历史梯度信息\n优势：加速收敛，减少震荡\n\n\n交叉熵损失函数（CrossEntropyLoss）：\n\n适用场景：多分类问题（如本例中的 10 个数字分类）\n计算过程：\n\n首先对模型输出进行 softmax 归一化，得到每个类别的概率\n然后计算预测概率分布与真实标签分布的交叉熵\n\n特点：\n\n能有效处理多分类问题\n对错误预测施加更大的惩罚\n输出值在 [0, ∞) 范围内，0 表示完美预测\n\n\n\n\n# 定义优化器：使用随机梯度下降（SGD），学习率为 0.01，动量为 0.9\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n# 定义损失函数：交叉熵损失函数常用于分类问题\ncriterion = nn.CrossEntropyLoss()\n\n\n\n23.7.7 开始训练\n训练过程中，我们记录了训练损失、训练准确率、测试损失和测试准确率。\n\n# 用于记录训练过程的指标\ntrain_losses = []\ntrain_accs = []\ntest_losses = []\ntest_accs = []\n\nepochs = 20  # 设定训练轮数为 20\n# 循环训练和测试模型\nfor epoch in range(1, epochs + 1):\n    # 训练并记录指标\n    train_loss, train_acc = train(model, device, train_loader, optimizer, criterion, epoch)\n    test_loss, test_acc, cm = test(model, device, test_loader, criterion)\n    \n    # 保存指标\n    train_losses.append(train_loss)\n    train_accs.append(train_acc)\n    test_losses.append(test_loss)\n    test_accs.append(test_acc)\n    \n    print(f\"\\nEpoch {epoch}:\")\n    print(f\"Train - Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n    print(f\"Test  - Loss: {test_loss:.4f}, Accuracy: {test_acc:.2f}%\\n\")\n\nTrain Epoch: 1 [0/60000]    Loss: 2.310042\n\nEpoch 1:\nTrain - Loss: 0.3103, Accuracy: 89.74%\nTest  - Loss: 0.0854, Accuracy: 97.12%\n\nTrain Epoch: 2 [0/60000]    Loss: 0.108747\n\nEpoch 2:\nTrain - Loss: 0.0680, Accuracy: 97.88%\nTest  - Loss: 0.0504, Accuracy: 98.31%\n\nTrain Epoch: 3 [0/60000]    Loss: 0.069749\n\nEpoch 3:\nTrain - Loss: 0.0472, Accuracy: 98.53%\nTest  - Loss: 0.0419, Accuracy: 98.68%\n\nTrain Epoch: 4 [0/60000]    Loss: 0.112128\n\nEpoch 4:\nTrain - Loss: 0.0379, Accuracy: 98.79%\nTest  - Loss: 0.0425, Accuracy: 98.62%\n\nTrain Epoch: 5 [0/60000]    Loss: 0.034649\n\nEpoch 5:\nTrain - Loss: 0.0307, Accuracy: 99.00%\nTest  - Loss: 0.0301, Accuracy: 99.08%\n\nTrain Epoch: 6 [0/60000]    Loss: 0.001299\n\nEpoch 6:\nTrain - Loss: 0.0245, Accuracy: 99.23%\nTest  - Loss: 0.0357, Accuracy: 98.86%\n\nTrain Epoch: 7 [0/60000]    Loss: 0.009209\n\nEpoch 7:\nTrain - Loss: 0.0218, Accuracy: 99.30%\nTest  - Loss: 0.0349, Accuracy: 98.94%\n\nTrain Epoch: 8 [0/60000]    Loss: 0.002920\n\nEpoch 8:\nTrain - Loss: 0.0180, Accuracy: 99.42%\nTest  - Loss: 0.0398, Accuracy: 98.92%\n\nTrain Epoch: 9 [0/60000]    Loss: 0.000881\n\nEpoch 9:\nTrain - Loss: 0.0167, Accuracy: 99.43%\nTest  - Loss: 0.0400, Accuracy: 98.91%\n\nTrain Epoch: 10 [0/60000]   Loss: 0.008575\n\nEpoch 10:\nTrain - Loss: 0.0153, Accuracy: 99.50%\nTest  - Loss: 0.0359, Accuracy: 98.95%\n\nTrain Epoch: 11 [0/60000]   Loss: 0.003086\n\nEpoch 11:\nTrain - Loss: 0.0144, Accuracy: 99.50%\nTest  - Loss: 0.0374, Accuracy: 98.99%\n\nTrain Epoch: 12 [0/60000]   Loss: 0.002294\n\nEpoch 12:\nTrain - Loss: 0.0111, Accuracy: 99.61%\nTest  - Loss: 0.0368, Accuracy: 98.95%\n\nTrain Epoch: 13 [0/60000]   Loss: 0.010362\n\nEpoch 13:\nTrain - Loss: 0.0093, Accuracy: 99.71%\nTest  - Loss: 0.0417, Accuracy: 98.93%\n\nTrain Epoch: 14 [0/60000]   Loss: 0.000296\n\nEpoch 14:\nTrain - Loss: 0.0093, Accuracy: 99.70%\nTest  - Loss: 0.0344, Accuracy: 99.05%\n\nTrain Epoch: 15 [0/60000]   Loss: 0.066797\n\nEpoch 15:\nTrain - Loss: 0.0060, Accuracy: 99.81%\nTest  - Loss: 0.0477, Accuracy: 98.74%\n\nTrain Epoch: 16 [0/60000]   Loss: 0.019175\n\nEpoch 16:\nTrain - Loss: 0.0059, Accuracy: 99.82%\nTest  - Loss: 0.0458, Accuracy: 99.04%\n\nTrain Epoch: 17 [0/60000]   Loss: 0.001929\n\nEpoch 17:\nTrain - Loss: 0.0069, Accuracy: 99.76%\nTest  - Loss: 0.0410, Accuracy: 98.98%\n\nTrain Epoch: 18 [0/60000]   Loss: 0.000602\n\nEpoch 18:\nTrain - Loss: 0.0066, Accuracy: 99.79%\nTest  - Loss: 0.0488, Accuracy: 98.79%\n\nTrain Epoch: 19 [0/60000]   Loss: 0.009205\n\nEpoch 19:\nTrain - Loss: 0.0063, Accuracy: 99.78%\nTest  - Loss: 0.0442, Accuracy: 98.99%\n\nTrain Epoch: 20 [0/60000]   Loss: 0.016622\n\nEpoch 20:\nTrain - Loss: 0.0078, Accuracy: 99.72%\nTest  - Loss: 0.0419, Accuracy: 98.99%\n\n\n\n\n\n\n\n\n\n训练一轮都发生了哪些计算？\n\n\n\n训练一轮（epoch）可以想象成小朋友学一道数学题的完整过程，分成以下几个步骤：\n\n尝试解题（前向传播）\n\n你看到了一道数学题，比如 “5 + 3 = ?”。\n你心里想一下，觉得答案应该是 “8”。\n\n检查答案（计算损失）\n\n你把答案写在作业本上，然后老师告诉你对不对。\n如果你写错了，比如写成 “7”，老师就会告诉你错了 “1”。\n\n找出错在哪里（反向传播）\n\n你想一想，为什么错了？\n\n可能是你心算的时候少加了 1。\n\n改正错误（参数更新）\n\n你下次遇到类似的题目，会更加小心，比如数手指来确认。\n这样，你学得越来越好，错误越来越少。\n\n重复练习\n\n你做完这道题，老师再给你新的题目。\n你继续练习，直到你能快速又准确地做出答案。\n\n\n训练一轮就像这样，让神经网络做题（预测）、检查答案（计算损失）、找错误（反向传播）、改正（更新参数），然后继续学习，直到变得很聪明！📚😊\n\n\n\n\n23.7.8 绘制训练过程图表\n训练结束后，我们可以绘制训练过程的损失曲线和准确率曲线。\n\n# 绘制训练过程图表\nepochs_range = range(1, epochs + 1)\n\nplt.figure(figsize=(12, 5))\n\n# 绘制损失曲线\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, train_losses, 'bo-', label='Training Loss')\nplt.plot(epochs_range, test_losses, 'ro-', label='Test Loss')\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\n# 绘制准确率曲线\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, train_accs, 'bo-', label='Training Accuracy')\nplt.plot(epochs_range, test_accs, 'ro-', label='Test Accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy (%)')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 23.4: 训练损失曲线和准确率变化\n\n\n\n\n\n代码说明：\n在主函数中，我们首先检测计算设备，然后实例化模型、定义优化器和损失函数，并依次调用训练和测试函数。每个 epoch 结束后，终端会输出当前的训练状态和测试结果。",
    "crumbs": [
      "从零开始神经网络",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>手搓神经网络模型</span>"
    ]
  },
  {
    "objectID": "neural-network-from-scratch.html#详解神经元的训练过程",
    "href": "neural-network-from-scratch.html#详解神经元的训练过程",
    "title": "23  手搓神经网络模型",
    "section": "23.8 详解神经元的训练过程",
    "text": "23.8 详解神经元的训练过程\n下面以一个最简单的神经网络——只有一个神经元的单层模型——为例，展示训练过程中神经元参数（权重和偏置）是如何一步步确定下来的。这个例子帮助理解神经网络的基本训练流程，包括前向传播、损失计算、反向传播（梯度计算）和参数更新。\n\n23.8.1 网络结构与设定\n假设我们的神经网络只有一个神经元，该神经元接收一个输入 \\(x\\) 并输出 \\(y\\)。神经元具有两个可训练参数：\n\n权重 \\(w\\)\n偏置 \\(b\\)\n\n采用线性激活函数（即不做非线性变换），则神经元的输出为： \\[\ny = w \\cdot x + b.\n\\]\n同时，设定一个平方误差损失函数（Mean Squared Error, MSE）来衡量输出与目标之间的差距： \\[\nL = \\frac{1}{2}(y - y_{\\text{target}})^2,\n\\] 其中 \\(y_{\\text{target}}\\) 为给定的目标输出。\n\n\n23.8.2 训练流程概述\n整个训练过程可以分为以下几个步骤：\n\n初始化参数\n随机或按照某种策略给定初始的 \\(w\\) 和 \\(b\\)。\n前向传播\n给定输入 \\(x\\)，计算神经元输出： \\[\ny = w \\cdot x + b.\n\\]\n损失计算\n根据神经元输出和目标输出 \\(y_{\\text{target}}\\) 计算损失： \\[\nL = \\frac{1}{2}(y - y_{\\text{target}})^2.\n\\]\n反向传播（梯度计算）\n利用链式法则计算损失关于参数 \\(w\\) 和 \\(b\\) 的梯度，具体如下：\n\n对 \\(y\\) 求导： \\[\n\\frac{\\partial L}{\\partial y} = y - y_{\\text{target}}.\n\\]\n由于 \\(y = w \\cdot x + b\\)，有： \\[\n\\frac{\\partial y}{\\partial w} = x,\\quad \\frac{\\partial y}{\\partial b} = 1.\n\\]\n所以利用链式法则： \\[\n\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial w} = (y - y_{\\text{target}}) \\cdot x,\n\\] \\[\n\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial b} = y - y_{\\text{target}}.\n\\]\n\n参数更新\n利用梯度下降法调整参数： \\[\nw_{\\text{new}} = w - \\eta \\cdot \\frac{\\partial L}{\\partial w},\\quad b_{\\text{new}} = b - \\eta \\cdot \\frac{\\partial L}{\\partial b},\n\\] 其中 \\(\\eta\\) 为学习率，控制每次更新的步长。\n重复迭代\n重复步骤2～5，直至损失足够小或达到预定的迭代次数。\n\n\n\n23.8.3 数值示例\n假设我们有以下设定：\n\n输入：\\(x = 1.0\\)\n目标输出：\\(y_{\\text{target}} = 2.0\\)\n初始参数：\\(w = 0.5\\)，\\(b = 0.1\\)\n学习率：\\(\\eta = 0.1\\)\n\n我们来看几次迭代的具体计算过程。\n\n\n23.8.4 迭代 1\n\n前向传播\n计算输出： \\[\ny = 0.5 \\times 1.0 + 0.1 = 0.6.\n\\]\n损失计算\n\\[\nL = \\frac{1}{2}(0.6 - 2.0)^2 = \\frac{1}{2} \\times (-1.4)^2 = \\frac{1}{2} \\times 1.96 = 0.98.\n\\]\n反向传播（梯度计算）\n\n首先计算： \\[\n\\frac{\\partial L}{\\partial y} = 0.6 - 2.0 = -1.4.\n\\]\n然后： \\[\n\\frac{\\partial L}{\\partial w} = -1.4 \\times 1.0 = -1.4,\n\\] \\[\n\\frac{\\partial L}{\\partial b} = -1.4.\n\\]\n\n参数更新\n\\[\nw_{\\text{new}} = 0.5 - 0.1 \\times (-1.4) = 0.5 + 0.14 = 0.64,\n\\] \\[\nb_{\\text{new}} = 0.1 - 0.1 \\times (-1.4) = 0.1 + 0.14 = 0.24.\n\\]\n\n\n\n23.8.5 迭代 2\n使用更新后的参数 \\(w = 0.64\\) 和 \\(b = 0.24\\)。\n\n前向传播\n\\[\ny = 0.64 \\times 1.0 + 0.24 = 0.88.\n\\]\n损失计算\n\\[\nL = \\frac{1}{2}(0.88 - 2.0)^2 = \\frac{1}{2} \\times (-1.12)^2 = \\frac{1}{2} \\times 1.2544 \\approx 0.6272.\n\\]\n反向传播\n\\[\n\\frac{\\partial L}{\\partial y} = 0.88 - 2.0 = -1.12,\n\\] \\[\n\\frac{\\partial L}{\\partial w} = -1.12 \\times 1.0 = -1.12,\n\\] \\[\n\\frac{\\partial L}{\\partial b} = -1.12.\n\\]\n参数更新\n\\[\nw_{\\text{new}} = 0.64 - 0.1 \\times (-1.12) = 0.64 + 0.112 = 0.752,\n\\] \\[\nb_{\\text{new}} = 0.24 - 0.1 \\times (-1.12) = 0.24 + 0.112 = 0.352.\n\\]\n\n\n\n23.8.6 迭代 3\n使用更新后的参数 \\(w = 0.752\\) 和 \\(b = 0.352\\)。\n\n前向传播\n\\[\ny = 0.752 \\times 1.0 + 0.352 = 1.104.\n\\]\n损失计算\n\\[\nL = \\frac{1}{2}(1.104 - 2.0)^2 = \\frac{1}{2} \\times (-0.896)^2 \\approx \\frac{1}{2} \\times 0.802 = 0.401.\n\\]\n反向传播\n\\[\n\\frac{\\partial L}{\\partial y} = 1.104 - 2.0 = -0.896,\n\\] \\[\n\\frac{\\partial L}{\\partial w} = -0.896 \\times 1.0 = -0.896,\n\\] \\[\n\\frac{\\partial L}{\\partial b} = -0.896.\n\\]\n参数更新\n\\[\nw_{\\text{new}} = 0.752 - 0.1 \\times (-0.896) = 0.752 + 0.0896 \\approx 0.8416,\n\\] \\[\nb_{\\text{new}} = 0.352 - 0.1 \\times (-0.896) = 0.352 + 0.0896 \\approx 0.4416.\n\\]\n\n\n\n23.8.7 训练过程总结\n在这个简单例子中，神经网络的参数更新过程可以总结为：\n\n初始化：随机或预设初始值（本例中 \\(w = 0.5, \\, b = 0.1\\)）。\n前向传播：利用当前参数计算输出 \\(y = w \\cdot x + b\\)。\n计算损失：用损失函数衡量输出与目标的差异。\n反向传播：计算损失对各参数的梯度，得到更新方向。\n参数更新：利用梯度下降公式更新 \\(w\\) 和 \\(b\\)。\n迭代训练：重复上述步骤，直至损失减小到可以接受的程度或达到预定的迭代次数。\n\n经过多次迭代后，神经元的参数会逐渐调整，使得神经元的输出越来越接近目标输出，从而达到训练的目的。\n\n\n23.8.8 拓展：多层神经网络\n在实际应用中，我们通常使用多层神经网络（即深度神经网络）。其基本原理与上述单神经元相同，只不过：\n\n每一层都有多个神经元，每个神经元都有各自的参数；\n激活函数可能为非线性函数（如ReLU、Sigmoid、Tanh等）；\n反向传播时需要利用链式法则将梯度从输出层依次传递到各个隐藏层，计算每个参数对最终损失的贡献。\n\n无论网络有多复杂，核心思想都是：通过不断前向计算输出、衡量输出与目标之间的误差，再通过反向传播调整参数，从而使得网络能够更好地拟合数据。\n通过上述极简示例，我们可以直观地看到神经网络训练过程中参数是如何一步步从初始值调整到能够较好地“解释”训练数据的。这就是神经网络训练中参数确定的基本机制。",
    "crumbs": [
      "从零开始神经网络",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>手搓神经网络模型</span>"
    ]
  },
  {
    "objectID": "neural-network-from-scratch.html#详解卷积滤波器的训练过程",
    "href": "neural-network-from-scratch.html#详解卷积滤波器的训练过程",
    "title": "23  手搓神经网络模型",
    "section": "23.9 详解卷积滤波器的训练过程",
    "text": "23.9 详解卷积滤波器的训练过程\n神经网络中的滤波器（Filter）本质上是一个可学习的参数矩阵，其作用类似于图像处理中的特征检测器。下面通过具体示例说明其工作原理：\n\n23.9.1 滤波器基本结构\n典型尺寸为3x3或5x5的二维矩阵，例如：\n水平边缘检测滤波器：\n[[-1, -1, -1],\n [ 0,  0,  0],\n [ 1,  1,  1]]\n该滤波器会对水平方向灰度变化剧烈的区域产生强响应\n\n\n23.9.2 工作原理示例\n假设输入为7x7的字母”X”图像：\n0 0 0 1 0 0 0\n0 0 1 0 1 0 0\n0 1 0 0 0 1 0\n1 0 0 0 0 0 1\n0 1 0 0 0 1 0\n0 0 1 0 1 0 0\n0 0 0 1 0 0 0\n应用3x3滤波器进行卷积运算：\n\n在图像左上角3x3区域：\n\n0 0 0\n0 0 1\n0 1 0\n与滤波器逐元素相乘后求和：\n(0*-1)+(0*-1)+(0*-1) + (0*0)+(0*0)+(1*0) + (0*1)+(1*1)+(0*1) = 1\n\n滑动到中心区域：\n\n0 0 0\n0 0 0\n0 0 0\n计算结果为0（无特征响应）\n最终输出特征图将突出显示原始图像中的水平边缘。\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import convolve2d\n\n# 原始矩阵\nmatrix = test_dataset[0][0].squeeze()\n\n# 3x3 卷积核\nkernel = np.array([[-1, -1, -1],\n                    [ 0,  0,  0],\n                    [ 1,  1,  1]])\n\n# 进行卷积运算\nconvolved = convolve2d(matrix, kernel, mode='valid')\n\n# 计算子图尺寸比例\noriginal_shape = matrix.shape\nconvolved_shape = convolved.shape\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\n# 计算比例因子，使卷积后的小图与原图比例协调\nscale_factor = original_shape[0] / convolved_shape[0]\n\n# 调整原始矩阵子图\naxes[0].imshow(matrix, cmap='gray', interpolation='nearest', aspect=1)\naxes[0].set_title(\"Original Matrix\")\naxes[0].axis(\"off\")\n\n# 调整卷积后矩阵子图，缩放至与原图比例协调\naxes[1].imshow(convolved, cmap='gray', interpolation='nearest', aspect=1/scale_factor)\naxes[1].set_title(\"Convolved Matrix\")\naxes[1].axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\nFigure 23.5: 图像的卷积操作\n\n\n\n\n\n\n\n23.9.3 可视化理解\n实际训练后的滤波器示例（MNIST数据集）：\n层1滤波器1：  \n[[ 0.21,  0.34, -0.12],  \n [ 0.18,  0.29, -0.25],  \n [-0.15, -0.22,  0.31]]  \n\n层1滤波器2：  \n[[-0.33,  0.19,  0.27],  \n [ 0.12, -0.28,  0.14],  \n [ 0.25,  0.17, -0.31]]\n输出训练好的模型中卷积核的参数：\n\nconv1_weights = model.conv1.weight.data\nprint(conv1_weights)\n\ntensor([[[[-0.1273, -0.0421, -0.3127, -0.1300, -0.0118],\n          [ 0.1297, -0.1157, -0.1868, -0.1506, -0.2473],\n          [ 0.1916, -0.2949, -0.0050, -0.2697, -0.2054],\n          [ 0.1425, -0.2843, -0.1750, -0.2480, -0.0311],\n          [-0.0233, -0.1813, -0.2648,  0.0153,  0.1723]]],\n\n\n        [[[ 0.1151,  0.3466, -0.2346, -0.1624, -0.1952],\n          [ 0.4736,  0.1470, -0.2044, -0.2904,  0.0583],\n          [ 0.5162,  0.1926, -0.1791, -0.3443, -0.2680],\n          [ 0.1368,  0.3871, -0.1407, -0.1877,  0.0163],\n          [ 0.1189,  0.4222, -0.1104, -0.2362,  0.0373]]],\n\n\n        [[[-0.0837, -0.2277, -0.5687, -0.2844,  0.1753],\n          [-0.3154, -0.4132, -0.2678,  0.2049,  0.2307],\n          [-0.3179, -0.2399, -0.0928,  0.5653,  0.3217],\n          [-0.3993, -0.2604,  0.5348,  0.4581, -0.1129],\n          [-0.2141,  0.2926,  0.4899,  0.0306, -0.0577]]],\n\n\n        [[[ 0.3045, -0.0845, -0.5271, -0.3703,  0.0889],\n          [-0.1209, -0.0081, -0.2831,  0.2770,  0.1535],\n          [-0.0055,  0.4796,  0.7221,  0.6526,  0.2727],\n          [ 0.2660,  0.3361,  0.4320, -0.0282, -0.1670],\n          [-0.0319, -0.3057, -0.4165, -0.3214, -0.2963]]],\n\n\n        [[[-0.0520, -0.3755, -0.3632, -0.4254, -0.1126],\n          [ 0.1772, -0.2188, -0.7799, -0.4041, -0.3539],\n          [ 0.6291,  0.3481,  0.1588,  0.4091,  0.0408],\n          [ 0.1987,  0.4856,  0.6113,  0.1712, -0.1505],\n          [-0.1531, -0.2018,  0.0523,  0.1042,  0.2720]]],\n\n\n        [[[-0.0366,  0.0519,  0.0103,  0.3432,  0.0164],\n          [-0.0017,  0.1497,  0.2175,  0.2794,  0.1798],\n          [-0.4238,  0.0462, -0.0041,  0.5519,  0.1029],\n          [-0.1912, -0.1742, -0.0396,  0.4950,  0.0999],\n          [-0.3413, -0.2570, -0.0313,  0.2665,  0.2804]]]], device='mps:0')\n\n\n这些数值组合在实际运算中会产生类似边缘检测、角点检测的效果。\n\nfrom scipy.signal import convolve2d\nfrom torch.nn import MaxPool2d\nimport math\n\n# 最大池化层\nmaxpool = MaxPool2d(kernel_size=2, stride=2)\n\nnum_kernels = len(conv1_weights)\nnum_cols = 6\nnum_rows = math.ceil(num_kernels * 3 / num_cols)\n\nplt.figure(figsize=(num_cols * 2, num_rows * 2))\n\nfor kernel_idx, kernel in enumerate(conv1_weights):\n    convolved_image = convolve2d(matrix, kernel.squeeze().cpu().detach().numpy(), mode='valid')\n    pooled_image = maxpool(torch.tensor(convolved_image).unsqueeze(0)).squeeze(0).cpu().detach().numpy()\n\n    plt.subplot(num_rows, num_cols, kernel_idx * 3 + 1)\n    plt.imshow(matrix, cmap='gray', interpolation='nearest')\n    plt.title(\"Original\")\n    plt.axis(\"off\")\n\n    plt.subplot(num_rows, num_cols, kernel_idx * 3 + 2)\n    plt.imshow(convolved_image, cmap='gray', interpolation='nearest')\n    plt.title(f\"Convolved ({kernel_idx + 1})\")\n    plt.axis(\"off\")\n\n    plt.subplot(num_rows, num_cols, kernel_idx * 3 + 3)\n    plt.imshow(pooled_image, cmap='gray', interpolation='nearest')\n    plt.title(f\"Pooled ({kernel_idx + 1})\")\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 23.6: 图像的卷积和池化\n\n\n\n\n\n通过这种局部感受野的滑动计算，CNN能够逐层提取从简单到复杂的空间特征，最终形成对输入数据的层次化理解。",
    "crumbs": [
      "从零开始神经网络",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>手搓神经网络模型</span>"
    ]
  },
  {
    "objectID": "neural-network-from-scratch.html#总结",
    "href": "neural-network-from-scratch.html#总结",
    "title": "23  手搓神经网络模型",
    "section": "23.10 总结",
    "text": "23.10 总结\n本项目详细介绍了：\n\n神经网络基础知识：从基本结构、训练过程到卷积操作的优势，帮助你了解神经网络的工作原理。\n\n数据集：通过 MNIST 数据集的介绍，了解了手写数字识别问题的背景。\n\n环境配置：如何利用 Conda 创建环境，并安装 PyTorch、torchvision 等必备工具。\n\n完整代码实现：从数据加载、模型构建到训练和评估，每一步均有详细注释，确保即使是初学者也能理解和上手。\n\n通过本项目的学习，你不仅能掌握如何用 PyTorch 实现一个简单的 LeNet 神经网络，还能理解神经网络训练的基本原理及卷积操作在图像处理中的优势。希望本章内容能激发你对人工智能和深度学习的兴趣，并为进一步探索打下坚实基础！",
    "crumbs": [
      "从零开始神经网络",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>手搓神经网络模型</span>"
    ]
  },
  {
    "objectID": "two-dimensional-classification.html",
    "href": "two-dimensional-classification.html",
    "title": "24  二维平面拟合",
    "section": "",
    "text": "24.1 问题\n随机生成50个点，分成两类。取40个点训练一个含有3个隐藏层的全连接网络，用10个点的类别进行测试。batch_size设成4，绘制出训练过程中准确率的变化。\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# 随机生成数据\nnp.random.seed(42)\nX, y = make_classification(n_samples=50, n_features=2, n_informative=2, \n                          n_redundant=0, n_clusters_per_class=1, flip_y=0.1)\n\n# 可视化原始数据分布\nplt.figure(figsize=(6, 4))\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolors='k')\nplt.title(\"Original Data Distribution\")\n\nText(0.5, 1.0, 'Original Data Distribution')",
    "crumbs": [
      "从零开始神经网络",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>二维平面拟合</span>"
    ]
  },
  {
    "objectID": "two-dimensional-classification.html#实现方法",
    "href": "two-dimensional-classification.html#实现方法",
    "title": "24  二维平面拟合",
    "section": "24.2 实现方法",
    "text": "24.2 实现方法\n以下是用Python和PyTorch实现的完整代码，包含数据生成、模型训练和准确率可视化：\n\n# 数据预处理\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# 转换为PyTorch张量\nX_tensor = torch.tensor(X, dtype=torch.float32)\ny_tensor = torch.tensor(y, dtype=torch.long)\n\n# 分割训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(\n    X_tensor, y_tensor, test_size=10, random_state=42)\n\n# 定义神经网络\nclass Classifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(2, 16),\n            nn.ReLU(),\n            nn.Linear(16, 16),\n            nn.ReLU(),\n            nn.Linear(16, 8),\n            nn.ReLU(),\n            nn.Linear(8, 2)\n        )\n        \n    def forward(self, x):\n        return self.net(x)\n\n# 训练参数\nmodel = Classifier()\n\nprint(model)\n\nClassifier(\n  (net): Sequential(\n    (0): Linear(in_features=2, out_features=16, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=16, out_features=16, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=16, out_features=8, bias=True)\n    (5): ReLU()\n    (6): Linear(in_features=8, out_features=2, bias=True)\n  )\n)\n\n\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\nbatch_size = 4\nepochs = 300\n\n# 训练过程记录\ntrain_acc_history = []\ntest_acc_history = []\n\n# 训练循环\nfor epoch in range(epochs):\n    # 训练步骤\n    model.train()\n    permutation = torch.randperm(X_train.size(0))\n    \n    for i in range(0, X_train.size(0), batch_size):\n        indices = permutation[i:i+batch_size]\n        batch_X = X_train[indices]\n        batch_y = y_train[indices]\n        \n        optimizer.zero_grad()\n        outputs = model(batch_X)\n        loss = criterion(outputs, batch_y)\n        loss.backward()\n        optimizer.step()\n    \n    # 记录准确率\n    model.eval()\n    with torch.no_grad():\n        # 训练集准确率\n        train_outputs = model(X_train)\n        train_preds = torch.argmax(train_outputs, dim=1)\n        train_acc = (train_preds == y_train).float().mean().item()\n        train_acc_history.append(train_acc)\n        \n        # 测试集准确率\n        test_outputs = model(X_test)\n        test_preds = torch.argmax(test_outputs, dim=1)\n        test_acc = (test_preds == y_test).float().mean().item()\n        test_acc_history.append(test_acc)\n\n# 可视化训练过程\nplt.figure(figsize=(6, 4))\nplt.plot(train_acc_history, label='Train Accuracy')\nplt.plot(test_acc_history, label='Test Accuracy', alpha=0.7)\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training Process (batch_size=4)')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# 输出最终测试准确率\nprint(f\"Final Test Accuracy: {test_acc_history[-1]:.2%}\")\n\n\n\n\n\n\n\n\nFinal Test Accuracy: 60.00%\n\n\n\n24.2.1 代码说明及注意事项：\n\n数据生成：\n\n使用make_classification生成具有明确决策边界的二维数据\nflip_y=0.1添加10%的噪声以增加挑战性\n标准化处理确保数据均值为0，方差为1\n\n网络结构：\nSequential(\n  (0): Linear(in_features=2, out_features=16, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=16, out_features=16, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=16, out_features=8, bias=True)\n  (5): ReLU()\n  (6): Linear(in_features=8, out_features=2, bias=True)\n)\n\n包含3个隐藏层（16-16-8节点）\n使用ReLU激活函数避免梯度消失\n\n训练细节：\n\n使用Adam优化器（学习率0.01）\n交叉熵损失函数\n300个训练周期\n每epoch记录训练/测试准确率\n\n可视化结果：\n\n左图：原始数据分布（标准化前）\n右图：训练过程中准确率变化曲线\n最终打印测试集准确率\n\n关键注意事项：\n\n小样本问题容易过拟合（可见测试准确率的波动）\n适当调整学习率：太大导致震荡，太小收敛慢\n可以尝试添加Dropout层或L2正则化\n不同随机种子可能导致结果差异\n\n\n最终测试准确率通常在80%-95%之间，具体取决于噪声点的分布情况。由于数据量非常小，建议多次运行观察稳定性（可在代码开头设置不同的随机种子测试）。\n如果需要进一步优化，可以考虑： 1. 添加学习率调度器（如ReduceLROnPlateau） 2. 使用早停法（Early Stopping） 3. 尝试不同的隐藏层结构 4. 增加数据增强（如添加高斯噪声）",
    "crumbs": [
      "从零开始神经网络",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>二维平面拟合</span>"
    ]
  },
  {
    "objectID": "two-dimensional-classification.html#画出超平面",
    "href": "two-dimensional-classification.html#画出超平面",
    "title": "24  二维平面拟合",
    "section": "24.3 画出超平面",
    "text": "24.3 画出超平面\n以下是在分类结果图上绘制神经网络决策边界（超平面）的完整实现，基于之前的代码补充可视化部分：\n\n# 在之前的代码训练结束后添加以下内容\n\n# 生成网格点以绘制决策边界\ndef plot_decision_boundary(model, X, y):\n    # 设置网格范围和步长\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n    h = 0.02  # 步长\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    \n    # 将网格点转换为PyTorch张量\n    grid_tensor = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n    \n    # 模型预测\n    model.eval()\n    with torch.no_grad():\n        Z = model(grid_tensor)\n        Z = torch.argmax(Z, dim=1).numpy()\n    Z = Z.reshape(xx.shape)\n    \n    # 绘制填充轮廓\n    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n    plt.contour(xx, yy, Z, colors='black', linewidths=0.5, linestyles='dashed')\n\n# 创建新的可视化结果图\nplt.figure(figsize=(8, 6))\n\n# 绘制训练集和测试集数据点\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='RdBu', \n           edgecolors='k', label='Train Data', s=60)\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='RdBu', \n           edgecolors='k', marker='s', linewidth=1.5, label='Test Data', s=80)\n\n# 绘制决策边界\nplot_decision_boundary(model, X_tensor.numpy(), y_tensor.numpy())\n\n# 添加图例和标签\nplt.title(f\"Classification Result (Test Acc: {test_acc_history[-1]:.1%})\")\nplt.xlabel(\"Feature 1 (Standardized)\")\nplt.ylabel(\"Feature 2 (Standardized)\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n24.3.1 关键代码解析\n\n网格生成逻辑：\nx_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\ny_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n根据数据范围扩展0.5的边界确保全覆盖\n使用0.02的步长生成密集网格点\n\n预测与形状转换：\ngrid_tensor = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\nZ = model(grid_tensor)\nZ = torch.argmax(Z, dim=1).numpy().reshape(xx.shape)\n\n将网格点从(N, N)展开为(N*N, 2)的矩阵格式\n预测后重新reshape回网格形状\n\n可视化增强：\nplt.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')  # 填充颜色\nplt.contour(xx, yy, Z, colors='black', linewidths=0.5, linestyles='dashed')  # 绘制边界线\n\n使用半透明填充显示分类区域\n黑色虚线标记精确决策边界\n\n\n\n\n24.3.2 注意事项\n\n标准化一致性：\n\n由于数据经过StandardScaler处理，决策边界坐标轴显示的是标准化后的值\n若需要原始尺度显示，可以通过scaler.inverse_transform转换网格坐标\n\n小样本过拟合：\n# 添加以下代码观察决策边界形状\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, \n           cmap='RdBu', marker='s', s=100, edgecolor='k')\n\n当测试样本较少时，可能出现决策边界”完美”拟合噪声的情况\n\n高分辨率绘制：\nh = 0.01  # 更小的步长\nplt.savefig('decision_boundary.png', dpi=300)  # 保存高清图\n\n调整步长参数h可获得更平滑的边界",
    "crumbs": [
      "从零开始神经网络",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>二维平面拟合</span>"
    ]
  },
  {
    "objectID": "pathogen-identification-with-raman-spectroscopy.html",
    "href": "pathogen-identification-with-raman-spectroscopy.html",
    "title": "26  利用拉曼光谱识别病原菌",
    "section": "",
    "text": "26.1 研究内容概述",
    "crumbs": [
      "拉曼光谱识别病原菌",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>利用拉曼光谱识别病原菌</span>"
    ]
  },
  {
    "objectID": "pathogen-identification-with-raman-spectroscopy.html#研究内容概述",
    "href": "pathogen-identification-with-raman-spectroscopy.html#研究内容概述",
    "title": "26  利用拉曼光谱识别病原菌",
    "section": "",
    "text": "文章标题: “Rapid identification of pathogenic bacteria using Raman spectroscopy and deep learning”\n作者: Chi-Sing Ho, Neal Jean, Catherine A. Hogan, Lena Blackmon, Stefanie S. Jeffrey, Niaz Banaei, Amr A.E. Saleh, Stefano Ermon, Jennifer Dionne 等。\n发表年份: 2019\n期刊名称: Nature Communications\nDOI: https://doi.org/10.1038/s41467-019-12898-9\n\n\n26.1.1 研究背景\n细菌感染是全球范围内导致死亡的主要原因之一，每年造成超过670万人死亡。传统诊断方法依赖于样本培养，耗时较长（通常需要数天），且在等待结果期间，患者常被广泛使用广谱抗生素。这种做法不仅增加了医疗成本，还加剧了抗生素耐药性问题。因此，快速、无培养的病原体鉴定和抗生素敏感性测试成为亟需解决的问题。\n拉曼光谱技术因其无需标记即可检测细菌的潜力而备受关注。然而，由于拉曼信号弱且噪声较高，其临床应用面临挑战。此外，现有研究多集中于区分不同菌种或菌株，缺乏涵盖多种病原体及其抗生素敏感性的大规模数据集。\n\n\n26.1.2 研究目的\n本文旨在利用拉曼光谱结合深度学习技术，开发一种快速、准确的病原体鉴定和抗生素敏感性测试方法。具体目标包括： 1. 构建一个包含30种常见病原体的大规模拉曼光谱数据集。 2. 使用卷积神经网络（CNN）对低信噪比（SNR）的光谱进行分类，实现病原体鉴定和抗生素治疗推荐。 3. 验证该方法在临床样本中的适用性，并探讨其潜在的实际应用价值。\n\n\n26.1.3 研究方法\n\n26.1.3.1 数据采集\n\n样本制备: 将细菌细胞沉积在镀金硅基底上，使用短时间（1秒）测量获取单细胞拉曼光谱。\n数据集构建:\n\n参考数据集：包含30种病原体（覆盖斯坦福医院2016-2017年94%的感染病例），每种病原体收集2000条光谱。\n临床数据集：包含50名患者的临床分离株，每种分离株收集400条光谱。\n\n实验设计: 使用独立样本验证模型性能，并通过留一患者交叉验证（LOOCV）策略对模型进行微调。\n\n\n\n26.1.3.2 分析方法\n\n深度学习模型: 采用25层1D卷积神经网络（CNN），并引入残差连接以增强模型性能。\n分类任务:\n\n病原体鉴定：30类病原体分类。\n抗生素治疗推荐：根据经验治疗分组分类。\n耐药性检测：二分类任务（MRSA vs. MSSA）。\n\n基线模型: 对比逻辑回归（LR）和支持向量机（SVM）的性能。\n\n\n\n26.1.3.3 方法优势与局限性\n\n优势:\n\nCNN能够有效处理低信噪比光谱数据。\n残差连接和步幅卷积保留了光谱峰值的位置信息。\n微调策略提高了模型在新临床数据集上的泛化能力。\n\n局限性:\n\n数据采集需要高精度设备，可能限制实际应用。\n当前数据集仅涵盖部分病原体和抗生素敏感性模式，未来需扩展。\n\n\n\n\n\n26.1.4 主要发现与结果\n\n26.1.4.1 核心发现\n\n病原体鉴定:\n\n在30类病原体分类任务中，平均准确率达到82.2±0.3%（SNR=4.1）。\n大多数误分类发生在同一类别内（如革兰氏阴性菌之间或同一属的菌株之间）。\n\n抗生素治疗推荐:\n\n经验治疗分组分类准确率达到97.0±0.3%，显著优于LR（93.3%）和SVM（92.2%）。\n\n耐药性检测:\n\nMRSA/MSSA二分类准确率为89.1±0.1%，ROC曲线下面积（AUC）为0.953。\n\n临床样本验证:\n\n使用10条光谱即可达到99.0%的鉴定准确率，与使用400条光谱的结果相当。\n第二临床数据集的治疗分组识别准确率达到99.7±1.1%。\n\n\n\n\n26.1.4.2 图表分析\n\n图2: 显示了30类病原体的混淆矩阵，表明误分类主要集中在同一抗生素治疗组内。\n图3: 展示了MRSA/MSSA分类的ROC曲线，证明模型具有较高的灵敏度和特异性。\n图4: 验证了微调策略的有效性，尤其是在新临床数据集上的表现。\n\n\n\n\n26.1.5 讨论与结论\n\n26.1.5.1 结果讨论\n作者强调，该方法能够在低信噪比条件下实现高精度分类，展示了深度学习在光谱数据分析中的潜力。此外，模型在临床样本中的表现表明其具备实际应用的可行性。\n\n\n26.1.5.2 结论\n本文成功开发了一种基于拉曼光谱和深度学习的快速病原体鉴定方法，具有以下特点： 1. 高准确性：即使在低信噪比条件下也能实现高精度分类。 2. 快速性：仅需10条光谱即可完成鉴定。 3. 泛化能力：通过微调可适应新的临床数据集。\n\n\n26.1.5.3 研究局限性与未来方向\n\n局限性:\n\n数据集覆盖范围有限，需进一步扩展以涵盖更多病原体和抗生素敏感性模式。\n实验条件（如样本制备和测量时间）可能影响结果的普适性。\n\n未来方向:\n\n开发自动化系统，用于血液、尿液和痰液样本的快速检测。\n探索其他生物标志物（如代谢产物）的检测潜力。\n\n\n\n\n\n26.1.6 创新点与贡献\n\n理论创新: 提出了一种结合拉曼光谱和深度学习的新方法，解决了低信噪比数据分类的难题。\n方法创新: 引入残差连接和步幅卷积，显著提升了模型性能。\n实践意义: 该方法有望应用于临床诊断，缩短病原体鉴定和抗生素敏感性测试的时间，从而改善患者预后并降低医疗成本。\n\n\n\n26.1.7 个人评价与启示\n\n26.1.7.1 整体评价\n本文的研究设计严谨，数据详实，结果可信。作者通过大规模数据集和深度学习模型，成功解决了拉曼光谱在病原体鉴定中的关键问题。\n\n\n26.1.7.2 不足之处\n\n数据集的多样性有待提高，尤其是针对罕见病原体和复杂临床样本。\n文章未充分讨论模型在不同设备或实验室条件下的可重复性。\n\n\n\n26.1.7.3 启示与建议\n\n未来研究方向:\n\n扩展数据集，涵盖更多病原体和抗生素敏感性模式。\n探索多模态数据融合（如结合基因组学和代谢组学）的可能性。\n\n实际应用:\n\n开发便携式拉曼光谱设备，推动该技术在基层医疗机构的应用。\n建立标准化流程，确保模型在不同环境下的稳定性和可靠性。\n\n\n综上所述，本文为快速病原体鉴定提供了一种创新且实用的方法，具有重要的科学和临床价值。",
    "crumbs": [
      "拉曼光谱识别病原菌",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>利用拉曼光谱识别病原菌</span>"
    ]
  },
  {
    "objectID": "pathogen-identification-with-raman-spectroscopy.html#神经网络的应用",
    "href": "pathogen-identification-with-raman-spectroscopy.html#神经网络的应用",
    "title": "26  利用拉曼光谱识别病原菌",
    "section": "26.2 神经网络的应用",
    "text": "26.2 神经网络的应用\n本文中使用的神经网络模型技术是基于深度学习的卷积神经网络（CNN），专门设计用于处理拉曼光谱数据。以下是详细阐述：\n\n26.2.1 模型架构\n\n1D卷积层：模型由25层一维卷积层组成，与传统的二维图像分类任务不同，该模型直接处理一维的光谱数据。\n残差连接：引入了残差连接（Residual Connections），这种技术有助于解决深层网络中的梯度消失问题，并提升模型性能。残差连接允许信息在多层之间更有效地传递。\n步幅卷积：为了保留光谱峰值的精确位置信息，模型没有使用池化层，而是采用了步幅卷积（Strided Convolutions）。这种方法在降低数据维度的同时，保留了重要的特征位置。\n\n\n\n26.2.2 输入和输出\n\n输入：模型的输入是一维的拉曼光谱数据。\n输出：模型输出一个概率分布，表示各个类别的可能性。通过取最大值确定预测类别。\n\n\n\n26.2.3 训练策略\n\n预训练和微调：首先在大规模参考数据集上进行预训练，然后在临床数据集上进行微调。具体采用留一患者交叉验证（Leave-One-Patient-Out Cross-Validation, LOOCV）策略进行微调，确保模型能够适应新的临床样本。\n数据增强：为了提高模型的泛化能力，对数据进行了增强处理，包括增加测量时间和采集更多光谱数据。\n\n\n\n26.2.4 性能评估\n\n分类任务：\n\n病原体鉴定：30类病原体分类任务，平均准确率达到82.2±0.3%。\n抗生素治疗推荐：经验治疗分组分类准确率达到97.0±0.3%。\n耐药性检测：MRSA/MSSA二分类准确率为89.1±0.1%，ROC曲线下面积（AUC）为0.953。\n\n基线对比：与逻辑回归（LR）和支持向量机（SVM）相比，CNN在所有任务上的表现均显著优于传统方法。\n\n\n\n26.2.5 优势\n\n低信噪比处理能力：即使在低信噪比（SNR=4.1）条件下，模型仍能实现高精度分类。\n少量数据高效利用：仅需10条光谱即可达到接近使用400条光谱的分类效果，显著提高了实际应用的可行性。\n持续改进潜力：通过不断微调，模型在新临床数据集上的表现可以进一步提升。\n\n\n\n26.2.6 局限性\n\n设备依赖性：高精度的数据采集需要昂贵的设备，可能限制其在资源有限环境中的应用。\n数据集覆盖范围：当前数据集主要涵盖常见病原体和抗生素敏感性模式，未来需要扩展以覆盖更多种类。\n\n综上所述，本文中使用的神经网络模型技术结合了先进的深度学习架构和针对光谱数据的优化策略，在病原体鉴定和抗生素敏感性测试方面展现了巨大的潜力。\n\n\n\n\nHo, Chi-Sing, Neal Jean, Catherine A. Hogan, Lena Blackmon, Stefanie S. Jeffrey, Mark Holodniy, Niaz Banaei, Amr A. E. Saleh, Stefano Ermon, and Jennifer Dionne. 2019. “Rapid Identification of Pathogenic Bacteria Using Raman Spectroscopy and Deep Learning.” Nature Communications 10 (1): 4927. https://doi.org/10.1038/s41467-019-12898-9.",
    "crumbs": [
      "拉曼光谱识别病原菌",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>利用拉曼光谱识别病原菌</span>"
    ]
  },
  {
    "objectID": "bacteria-id-annotation.html",
    "href": "bacteria-id-annotation.html",
    "title": "27  开源代码释读",
    "section": "",
    "text": "27.1 Notebooks\n代码库中包含了 3 个 *.ipynb 文件，它们的内容大同小异，分别执行不同的任务。\n以第 1 个 Notebook 为例，解释说明一下：\n第一块代码：导入使用到的基本模块。\nfrom time import time\nt00 = time()\nimport numpy as np\n第二块代码：导入数据，打印数据的形状。X 包含了 3000 行数据，每行有 1000 列。这说明拉曼光谱一共采集了 1000 个波段的数据。y 的行数与 X 一样，但是只有一维。说明它保存的是物种信息。查看它的值，恰好是 0 - 29 之间的数字，应当是分别代表 30 个物种的编号。\nX_fn = './data/X_finetune.npy'\ny_fn = './data/y_finetune.npy'\nX = np.load(X_fn)\ny = np.load(y_fn)\nprint(X.shape, y.shape)\n\n(3000, 1000) (3000,)\n第三块代码：导入神经网络相关的模块。\nfrom resnet import ResNet\nimport os\nimport torch\n第四块代码：设置模型的参数。\n# CNN 参数\nlayers = 6\nhidden_size = 100\nblock_size = 2\nhidden_sizes = [hidden_size] * layers\nnum_blocks = [block_size] * layers\ninput_dim = 1000\nin_channels = 64\nn_classes = 30\nos.environ['CUDA_VISIBLE_DEVICES'] = '{}'.format(0)\ncuda = torch.cuda.is_available()\n第五块代码：创建模型，并加载保存的权重。\n# 为演示加载训练好的权重\ncnn = ResNet(hidden_sizes, num_blocks, input_dim=input_dim,\n                in_channels=in_channels, n_classes=n_classes)\nif cuda: cnn.cuda()\ncnn.load_state_dict(torch.load(\n    './pretrained_model.ckpt', map_location=lambda storage, loc: storage))\n\n&lt;All keys matched successfully&gt;\n第六块代码：导入额外的模块，包括自定义的 spectral_dataloader 和 run_epoch 方法。\nfrom datasets import spectral_dataloader\nfrom training import run_epoch\nfrom torch import optim\n第七块代码：生成两个索引 idx_val 和 idx_tr，以便于将数据随机产分成训练集和测试集。\np_val = 0.1\nn_val = int(3000 * p_val)\nidx_tr = list(range(3000))\nnp.random.shuffle(idx_tr)\nidx_val = idx_tr[:n_val]\nidx_tr = idx_tr[n_val:]\n第八块代码：拆分数据集，并进行训练和验证。\n# 微调 CNN\nepochs = 1 # 将这个数字改为约30来进行完整训练\nbatch_size = 10\nt0 = time()\n# 设置 Adam 优化器\noptimizer = optim.Adam(cnn.parameters(), lr=1e-3, betas=(0.5, 0.999))\n# 设置数据加载器\ndl_tr = spectral_dataloader(X, y, idxs=idx_tr, batch_size=batch_size, shuffle=True)\ndl_val = spectral_dataloader(X, y, idxs=idx_val, batch_size=batch_size, shuffle=False)\n# 微调 CNN 的第一阶段\nbest_val = 0\nno_improvement = 0\nmax_no_improvement = 5\nprint('开始微调！')\nfor epoch in range(epochs):\n    print(' Epoch {}: {:0.2f}s'.format(epoch+1, time()-t0))\n    # 训练\n    acc_tr, loss_tr = run_epoch(epoch, cnn, dl_tr, cuda,\n        training=True, optimizer=optimizer)\n    print('  训练准确率: {:0.2f}'.format(acc_tr))\n    # 验证\n    acc_val, loss_val = run_epoch(epoch, cnn, dl_val, cuda,\n        training=False, optimizer=optimizer)\n    print('  验证准确率: {:0.2f}'.format(acc_val))\n    # 早停检查性能\n    if acc_val &gt; best_val or epoch == 0:\n        best_val = acc_val\n        no_improvement = 0\n    else:\n        no_improvement += 1\n    if no_improvement &gt;= max_no_improvement:\n        print('在 {} 轮后结束！'.format(epoch+1))\n        break\nprint('\\n 这个演示完成耗时: {:0.2f}s'.format(time()-t00))",
    "crumbs": [
      "拉曼光谱识别病原菌",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>开源代码释读</span>"
    ]
  },
  {
    "objectID": "bacteria-id-annotation.html#notebooks",
    "href": "bacteria-id-annotation.html#notebooks",
    "title": "27  开源代码释读",
    "section": "",
    "text": "1_reference_finetuning.ipynb - 展示如何对预训练的卷积神经网络进行微调，以实施 30 种菌株分类任务\n2_prediction.ipynb- 展示如何使用调优后的卷积神经网络进行预测\n3_clinical_finetuning.ipynb - 展示如何使用临床数据对预训练的卷积神经网络进行调优，并对个体患者进行预测",
    "crumbs": [
      "拉曼光谱识别病原菌",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>开源代码释读</span>"
    ]
  },
  {
    "objectID": "bacteria-id-annotation.html#config.py---包含提供数据集的相关信息",
    "href": "bacteria-id-annotation.html#config.py---包含提供数据集的相关信息",
    "title": "27  开源代码释读",
    "section": "27.2 config.py - 包含提供数据集的相关信息",
    "text": "27.2 config.py - 包含提供数据集的相关信息\n这个文件定义了数据集中编号对应的菌株。\n例如：\nSTRAINS[0] = \"C. albicans\"\nSTRAINS[1] = \"C. glabrata\"\nSTRAINS[2] = \"K. aerogenes\"\nSTRAINS[3] = \"E. coli 1\"",
    "crumbs": [
      "拉曼光谱识别病原菌",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>开源代码释读</span>"
    ]
  },
  {
    "objectID": "bacteria-id-annotation.html#datasets.py---包含设置光谱数据的数据集和数据加载器的代码",
    "href": "bacteria-id-annotation.html#datasets.py---包含设置光谱数据的数据集和数据加载器的代码",
    "title": "27  开源代码释读",
    "section": "27.3 datasets.py - 包含设置光谱数据的数据集和数据加载器的代码",
    "text": "27.3 datasets.py - 包含设置光谱数据的数据集和数据加载器的代码\n这段代码定义了如何在PyTorch中加载和处理光谱数据，以便用于机器学习模型的训练和评估。让我们逐步分析这里的组件和功能。\n\n导入库和模块：\n\n使用PyTorch的Dataset和DataLoader来处理数据集与数据加载。\n使用PyTorch的transforms模块来进行数据转换。\n还导入了torch和numpy库以支持数组和张量操作。\n\nSpectralDataset 类：\n\n这是一个继承自Dataset的自定义类，用于建立光谱数据集。\n它可以接收文件名（字符串形式，然后从中加载NumPy数组）或直接接收NumPy数组来初始化。\nidxs 参数定义了从数据集中使用那些样本，这可以用于随机拆分数据集到训练、验证和测试集。\ntransform 参数允许在读取样本时应用一连串的数据变换操作。\n__len__ 方法返回数据集中样本的数量。\n__getitem__ 方法根据索引获取单个样本，并且可以通过自定义的变换来增加一个新的维度（如果提供了变换）。\n\n数据转换（Transforms）：\n\nGetInterval 类从每个光谱中获取一段指定的区间。\nToFloatTensor 类将NumPy数组转换成PyTorch的浮点型张量（tensor）。\n这些类实现了__call__方法，使得它们可以像函数一样被调用，并直接作用于数据。\n\nspectral_dataloader 函数：\n\n基于提供的文件名或NumPy数组、索引和其它参数，这个函数创建并返回一个DataLoader对象。\n在创建DataLoader前，它使用了一个转换列表，并将这些转换合并(Compose)成一个执行链。\n\nspectral_dataloaders 函数：\n\n通过随机分割数据集，这个函数返回训练集、验证集和测试集的DataLoader对象。\n如果没有指定数据集的细分数量，它将依据百分比来计算训练和验证集的大小。\n根据提供的参数，分别使用spectral_dataloader函数来创建并返回训练集、验证集和测试集的DataLoader对象。\n\n\n总而言之，这段代码提供了一个完整的管道，从加载和变换光谱数据到为机器学习模型的训练、验证和测试准备好数据迭代器(DataLoader)。",
    "crumbs": [
      "拉曼光谱识别病原菌",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>开源代码释读</span>"
    ]
  },
  {
    "objectID": "bacteria-id-annotation.html#resnet.py---包含-resnet-卷积神经网络模型类",
    "href": "bacteria-id-annotation.html#resnet.py---包含-resnet-卷积神经网络模型类",
    "title": "27  开源代码释读",
    "section": "27.4 resnet.py - 包含 ResNet 卷积神经网络模型类",
    "text": "27.4 resnet.py - 包含 ResNet 卷积神经网络模型类\n这段代码定义了两个神经网络模型（类）和一个函数，用于构建残差网络（ResNet）结构主要用于处理一维信号。下面是对这段代码的具体解释：\n\n27.4.1 ResidualBlock 类\nResidualBlock 是一个继承自 nn.Module 的类，代表一个残差模块，是构成 ResNet 的基本单元。一个残差模块通常包括两个卷积层，并引入一条“捷径”（shortcut）或“旁路”，其目的是为了解决深层网络训练中的退化问题（Degradation Problem）。\n\n__init__ 方法初始化残差模块，接受以下参数：\n\nin_channels: 输入信号的通道数。\nout_channels: 输出信号的通道数。\nstride: 卷积层的步长（stride）。\n\n构建两个含有批量归一化（Batch Normalization）的卷积层 conv1 和 conv2。\n对于捷径路径，如果输入和输出信道数不同或步长不为1，需要一个额外的卷积层和批量归一化来匹配维度。\nforward 方法定义了数据在残差模块中的前向传播过程。输入 x 首先通过第一个卷积层、批量归一化和ReLU激活函数，然后再通过第二个卷积层和批量归一化。与此同时，输入 x 也通过捷径路径。最后将两个输出相加，再次应用 ReLU 激活函数，并返回结果。\n\n\n\n27.4.2 ResNet 类\nResNet 也是继承自 nn.Module 的类，代表整个残差网络模型。\n\n__init__ 方法初始化网络，接收以下参数：\n\nhidden_sizes: 各个残差模块的隐藏层大小的列表。\nnum_blocks: 每个隐藏层大小对应的残差模块数量的列表。\ninput_dim: 输入数据的维度。\nin_channels: 输入信号的通道数。\nn_classes: 用于分类任务的输出类别数量。\n\nconv1 和 bn1 分别表示第一个卷积层和其批量归一化层，用于处理原始输入信号。\nlayers 列表和 strides 列表分别构建了网络的不同隐藏层的残差模块和对应的步长。\nencoder 表示一个由残差模块组成的编码器。\nz_dim 是编码后的维度，通过在一些随机输入上运行 encode 方法获取。\nlinear 是一个全连接层，将编码后的输出转换为类别预测。\nencode 方法执行编码过程，将输入 x 通过第一个卷积层、批量归一化、编码器，最后将编码后的数据展平。\nforward 方法将输入信号编码并通过全连接层获取最终类别预测。\n_make_layer 方法构建残差模块的层。\n_get_encoding_size 通过输入一些随机数据来计算编码器的输出维度。\n\n\n\n27.4.3 add_activation 函数\n这个函数是一个特殊的辅助函数，返回指定名称的激活函数的实例。它接受一个字符串参数 activation，用于选择所需的激活函数。.FloatTensor\n这些组件一起使用可以构建一个能够适应不同输入输出需求的弹性神经网络模型，特别适合于一维信号的处理任务，如语音识别、时间序列分析等。\n总之，这个文件的作用就是仿照 ResNet 的架构构建了一个可用于一维信号处理任务的 ResNet。前面的 ResNet 指的是用于进行图片分类任务的 ResNet 本体；后面的 ResNet 指的是利用残差网络思想的一类 CNN 的名称。\n\n\n\n\n\n\nNote\n\n\n\nResNet（Residual Network，残差网络）是一个深层的卷积神经网络（CNN）架构。它由微软研究院的研究者 Kaiming He 等人在2015年提出，并在同年的 ImageNet 竞赛中获得了胜利，大幅提升了图像识别任务的准确率。\n核心概念：残差学习 (Residual Learning)\n随着网络深度的增加，理论上模型的表达能力也应随之提高。然而，在实际操作中，直接训练深度很深的网络往往会遇到两个主要的问题： 1. 梯度消失或者梯度爆炸：随着网络层数的增加，梯度在反向传播过程中会逐渐消失或者爆炸，导致网络训练更加困难，甚至无法收敛。 2. 网络退化问题 (Degradation Problem)：网络深度增加到一定程度后，准确率会饱和甚至下降，而不是因为过拟合。\n为了解决这些问题，ResNet 引入了残差学习框架。如果一个浅层模型已经足够优秀，那么通过堆叠更多层来获得理论上更强大的表达能力时，除了学习新特征外，新增的层只需学习与浅层模型的输出差异（即残差），这比学习一个全新的特征映射要容易得多。\nResNet 的结构\n一个标准的残差单元包括一个或多个卷积层，以及在这些卷积层之间的快捷连接（skip connection 或 shortcut），该连接直接跳过一些层。\n这样，在模型的学习过程中，即使有些层没有学到有用的信息，快捷连接仍然可以容许较低/较浅层次的信息向前传递，从而有效缓解梯度消失问题，并有助于信号直接传递，使得模型可以训练得更深。\n不同版本的 ResNet\nResNet 有不同层数的变体，包括但不限于 ResNet-18, ResNet-34, ResNet-50, ResNet-101 和 ResNet-152。其中数字表示网络层数。ResNet-50 之后的模型使用了瓶颈设计（bottleneck design），使用3个卷积层代替每个残差块中的2个，这样在增加更深的层数的同时，保持了计算复杂度。\n此外，ResNet 常常用作许多视觉任务的基础架构，并且已被证明即使在非图像任务中也有不错的效果。由于其出色的性能和广泛的适用性，ResNet 成为深度学习领域的一个里程碑，并为后来的很多研究提供了灵感。",
    "crumbs": [
      "拉曼光谱识别病原菌",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>开源代码释读</span>"
    ]
  },
  {
    "objectID": "bacteria-id-annotation.html#training.py---包含训练卷积神经网络和进行预测的代码",
    "href": "bacteria-id-annotation.html#training.py---包含训练卷积神经网络和进行预测的代码",
    "title": "27  开源代码释读",
    "section": "27.5 training.py - 包含训练卷积神经网络和进行预测的代码",
    "text": "27.5 training.py - 包含训练卷积神经网络和进行预测的代码\n这段代码包含了两个函数 run_epoch 和 get_predictions，它们用于在使用 PyTorch 进行机器学习工作流程中的训练和预测阶段。\n\n27.5.1 run_epoch 函数：\nrun_epoch 函数负责在给定数据集上训练模型一个epoch或者评估模型。\n参数：\n\nepoch：当前的epoch索引。这在函数中没有被使用，但可以用于日志或回调。\nmodel：要使用的PyTorch神经网络模型。\ndataloader：一个提供数据批次的PyTorch DataLoader。\ncuda：一个布尔值，指示是否在GPU上进行处理。如果为 True，则会将数据和模型移至GPU内存。\ntraining：一个布尔值，指示模型应处于训练模式（True）还是评估模式（False）。\noptimizer：用于在训练期间执行反向传播的优化器。仅当 training 设置为 True 时使用。\n\n行为：\n\n根据 training 标志，设置模型为训练模式或评估模式。\n初始化 total_loss、correct 和 total 来跟踪损失，正确预测的数量和数据点的总数。\n在 dataloader 中进行迭代，必要时将数据移至GPU，并将张量封装在 Variable 中。（注：从PyTorch 0.4.0开始，Variable 和张量已合并，这一行不是必需的）。\n对于每一批数据，使用模型进行预测，计算交叉熵损失，并且如果在训练模式下，执行反向传播和优化器步骤。\n跟踪总损失和正确预测的数量。\n计算并返回epoch的准确率和平均损失。\n\n\n\n27.5.2 get_predictions 函数：\nget_predictions 函数使用训练好的模型对提供的数据进行预测，或者获取类别的概率。\n参数：\n\nmodel：训练好的PyTorch神经网络模型。\ndataloader：一个提供数据批次的PyTorch DataLoader。\ncuda：一个指示是否在GPU上处理的布尔值。\nget_probs：一个布尔值，指示是否检索原始softmax概率（True）而不是预测类别索引（False）。\n\n行为：\n\n将模型设置为评估模式。\n在 dataloader 中进行迭代，必要时将数据移至GPU，并将张量封装在 Variable 中。（注：从PyTorch 0.4.0开始，这也不是必需的）。\n使用模型进行预测。如果 get_probs 为 True，它会对输出应用 softmax 函数来获取预测概率；否则，它取argmax来获取预测的类别索引。\n将结果附加到列表 preds 中。\n收集所有预测或概率后，将它们作为NumPy数组返回。如果请求了概率，则将所有批次沿第一维度堆叠；否则，只是从预测类别的列表中创建一个数组。\n\n注意：\n\n正如所提到的，使用 Variable 已经过时，在PyTorch 0.4.0及以上版本不再需要。\n对于 cuda 的检查以及数据/模型在GPU之间的传输，可以用更优雅的 PyTorch 功能来管理，比如 to(device)，其中 device 可以是 'cuda' 或 'cpu'。\nget_predictions 函数可以根据 get_probs 标志返回类概率或类索引。",
    "crumbs": [
      "拉曼光谱识别病原菌",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>开源代码释读</span>"
    ]
  },
  {
    "objectID": "bacteria-id-annotation.html#模型权重文件",
    "href": "bacteria-id-annotation.html#模型权重文件",
    "title": "27  开源代码释读",
    "section": "27.6 模型权重文件",
    "text": "27.6 模型权重文件\n代码库中提供了 3 个预训练模型的权重文件。分别是：\n\npretrained_model.ckpt：\nclinical_pretrained_model.ckpt：\nfinetuned_model.ckpt：\n\n\n\n\n\n\n\nNote\n\n\n\n.ckpt 文件扩展名是一个约定俗成的用来表示 checkpoint（检查点）的文件扩展名。在机器学习和深度学习中，checkpoint 文件用于存储训练模型的状态，可以用于恢复或者继续训练进程，或者用于模型的评估和部署。\n当使用如 TensorFlow、PyTorch 这样的深度学习框架时，.ckpt 文件通常包含了以下信息：\n\n模型参数（Weights and Biases）：这是训练过程中学习到的网络的核心，包括每一层的权重（weights）和偏差（biases）。\n优化器状态：这包括了用于训练过程中优化算法的状态信息（如动量、学习率等），这对于继续中断的训练非常重要。\n其他训练状态相关信息：例如，当前的epoch数、执行迭代的次数、最新的loss函数值等。\n\n这样，如果训练过程因为某些原因（如硬件故障、电源中断等）被迫中断，.ckpt 文件可以用来重载训练时的状态，从而无缝地继续训练，而不是从头开始。另外，也可以使用这个文件来对训练后的模型性能进行评估和测试。\n在实际操作中，根据使用的框架不同，.ckpt 文件的详细格式会有所不同。例如在 TensorFlow 中，检查点可能会由多个文件组成，并包含 .index 文件和 .data 文件的组合，以及一个 .meta 文件保存了模型的图信息。在 PyTorch 中，通常是一个单一的 .p 或 .pt 文件，或者是多个 .ckpt 文件，这取决于保存检查点的具体方法。\n简而言之，.ckpt 文件是深度学习中一个非常接重要的组件，允许数据科学家保存和恢复训练模型的状态。\n\n\n要比较三个 .ckpt 文件（假设是使用 PyTorch 保存的模型权重）之间的差异，你可以加载每一个文件，并提取模型的权重，然后逐个比较权重矩阵。以下是一个PyTorch的基本的步骤，用于比较三个检查点文件的权重差异：\n\nimport torch\n\n# 列出你的 .ckpt 文件路径\nfile_paths = ['pretrained_model.ckpt', 'clinical_pretrained_model.ckpt', 'finetuned_model.ckpt']\n\nimport torch\nimport numpy as np\n\n# 创建一个函数来加载所有的 .ckpt 文件\ndef load_checkpoints(file_paths):\n    checkpoints = []\n    for path in file_paths:\n        checkpoints.append(torch.load(path, map_location=torch.device('cpu')))\n    return checkpoints\n\n# 检查 keys 是否相同，并计算权重值相同和不同的 keys 数量\ndef compare_checkpoints(checkpoints):\n    keys_list = [set(ckpt.keys()) for ckpt in checkpoints]\n    intersection_keys = set.intersection(*keys_list)\n\n    # 列出共有的和特有的 keys 数量\n    unique_keys = [keys - intersection_keys for keys in keys_list]\n    print(f\"Number of common keys: {len(intersection_keys)}\")\n    for i, unique in enumerate(unique_keys):\n        print(f\"Number of keys only in checkpoint {i+1}: {len(unique)}\")\n        \n    # 如果所有 keys 都相同\n    if all(keys == intersection_keys for keys in keys_list):\n        print(\"All checkpoints have the same keys.\")\n        same_values_keys = []\n        different_values_keys = []\n        for key in intersection_keys:\n            # 比较每个检查点中对应键的权重值\n            weights = [ckpt[key].numpy() for ckpt in checkpoints]\n            if all(np.array_equal(weights[0], w) for w in weights[1:]):\n                same_values_keys.append(key)\n            else:\n                different_values_keys.append(key)\n\n        print(f\"Number of keys with the exact same weights: {len(same_values_keys)}\")\n        print(f\"Number of keys with differing weights: {len(different_values_keys)}\")\n\n        return same_values_keys, different_values_keys\n\n比较 3 个权重文件。\n\n# 加载所有 .ckpt 文件\ncheckpoints = load_checkpoints(file_paths)\n\n# 获取比较结果\ncompare_checkpoints(checkpoints)\n\nNumber of common keys: 157\nNumber of keys only in checkpoint 1: 0\nNumber of keys only in checkpoint 2: 0\nNumber of keys only in checkpoint 3: 31\n\n\n比较前 2 个权重文件。\n\n# 获取比较结果\nsame_values_keys, different_values_keys = compare_checkpoints(checkpoints[0:2])\n\n# 如果需要，可以进一步处理或打印相同和不同的 keys 信息\n# 例如：\n# print(\"Keys with the same weights:\", same_values_keys)\n# print(\"Keys with differing weights:\", different_values_keys)\n\nNumber of common keys: 157\nNumber of keys only in checkpoint 1: 0\nNumber of keys only in checkpoint 2: 0\nAll checkpoints have the same keys.\nNumber of keys with the exact same weights: 0\nNumber of keys with differing weights: 157\n\n\n在这个脚本中，我们首先加载了每个 .ckpt 文件。之后定义了一个函数 compare_weights，它会遍历每组权重的键（通常对应模型中的每一层），并计算两两之间的Frobenius范数（使用 numpy.linalg.norm）。Frobenius范数是度量矩阵（在这种情况下指权重矩阵）元素值的一种手段，可以作为比较它们相似度的一个指标。\n最后，我们输出每个权重矩阵之间比较的结果，这样可以直观地看到不同检查点之间权重的具体差异。\n请注意，上述代码片段假定所有 .ckpt 文件中包含相同的权重键，且这些权重是保存在同一模型结构中。如果模型结构不同，需要做进一步的处理来确保权重可以匹配对比。\n\n\n\n\n“Csho33/Bacteria-ID: Source Code and Demos for \"Rapid Identification of Pathogenic Bacteria Using Raman Spectroscopy and Deep Learning\".” n.d. https://github.com/csho33/bacteria-ID.",
    "crumbs": [
      "拉曼光谱识别病原菌",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>开源代码释读</span>"
    ]
  },
  {
    "objectID": "pretrained-model.html",
    "href": "pretrained-model.html",
    "title": "28  使用模型进行病原菌检测",
    "section": "",
    "text": "28.1 载入预训练模型\n首先，使用相同的参数重建模型，并载入权重。\nfrom resnet import ResNet\nimport os\nimport torch\n\n# CNN parameters\nlayers = 6\nhidden_size = 100\nblock_size = 2\nhidden_sizes = [hidden_size] * layers\nnum_blocks = [block_size] * layers\ninput_dim = 1000\nin_channels = 64\nn_classes = 30 # instead of 30, we use the 8 empiric groupings\n\n\n# Load trained weights for demo\ncnn = ResNet(hidden_sizes, num_blocks, input_dim=input_dim,\n                in_channels=in_channels, n_classes=n_classes)\n\n# 选择设备\n# select the device for computation\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\n\n# 载入模型权重\ncnn.load_state_dict(torch.load('./finetuned_model.ckpt', \n        map_location=lambda storage, loc: storage))\n\n# 将模型移动到指定设备\ncnn.to(device)\n\nResNet(\n  (conv1): Conv1d(1, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (encoder): Sequential(\n    (0): Sequential(\n      (0): ResidualBlock(\n        (conv1): Conv1d(64, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential(\n          (0): Conv1d(64, 100, kernel_size=(1,), stride=(1,), bias=False)\n          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential()\n      )\n    )\n    (1): Sequential(\n      (0): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential(\n          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential()\n      )\n    )\n    (2): Sequential(\n      (0): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential(\n          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential()\n      )\n    )\n    (3): Sequential(\n      (0): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential(\n          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential()\n      )\n    )\n    (4): Sequential(\n      (0): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential(\n          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential()\n      )\n    )\n    (5): Sequential(\n      (0): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential(\n          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential()\n      )\n    )\n  )\n  (linear): Linear(in_features=3200, out_features=30, bias=True)\n)",
    "crumbs": [
      "拉曼光谱识别病原菌",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>使用模型进行病原菌检测</span>"
    ]
  },
  {
    "objectID": "pretrained-model.html#模型结构解析",
    "href": "pretrained-model.html#模型结构解析",
    "title": "28  使用模型进行病原菌检测",
    "section": "28.2 模型结构解析",
    "text": "28.2 模型结构解析\ntorchviz 库是用来可视化 PyTorch 模型的图的工具。通常，make_dot 函数会生成模型中所有操作和张量的图，对于大型模型，图会变得非常复杂。\n\nfrom torchviz import make_dot\n\ny = cnn(torch.randn(4, 1, 1000).to(device))  # 随机生成一个输入来通过模型\nmake_dot(y, params=dict(cnn.named_parameters()))\n\n\n\n\n\n\n\n\n网络的完整结构展示出来非常大，观感不好。我们不妨看一下论文中的介绍。\n\n\n\nFigure 1\n\n\n\nCNN architecture\nThe CNN architecture is adapted from the Resnet architecture37 that has been widely successful across a range of computer vision tasks. It consists of an initial convolution layer followed by 6 residual layers and a final fully connected classification layer — a block diagram can be seen in Fig. 1. The residual layers contain shortcut connections between the input and output of each residual block, allowing for better gradient propagation and stable training (refer to reference 37 for details). Each residual layer contains 4 convolutional layers, so the total depth of the network is 26 layers. The initial convolution layer has 64 convolutional filters, while each of the hidden layers has 100 filters. These architecture hyperparameters were selected via grid search using one training and validation split on the isolate classification task. We also experimented with simple MLP (multi-layer perceptron) and CNN architectures but found that the Resnet-based architecture performed best.\n\n这里说明，所用的 CNN 架构是基于已广泛成功应用于多种计算机视觉任务的 Resnet 架构（参考文献37）进行改进的。它包括一个初始的卷积层，后跟 6 个残差层，以及一个最终的全连接分类层——这一结构在图 1 中有所展示。残差层包含了输入和每个残差块输出之间的快捷连接，这样的设计允许更好的梯度传播和稳定的训练（详细信息请参阅参考文献 37）。每个残差层包含 4 个卷积层，因此整个网络的总深度为 26 层。初始卷积层设有 64 个卷积滤波器，而各隐藏层则各有 100 个滤波器。这些架构超参数是通过网格搜索法选定的，使用的是隔离分类任务上的一个训练和验证分割。我们也尝试过简单的多层感知机(MLP)和 CNN 架构，但发现基于 Resnet 的架构表现最佳。\n在 PyTorch 中，.named_modules() 会递归地返回模型中所有模块的迭代器，包括模型本身和它所有的子模块，这可能会包括许多你不感兴趣的内部层。如果你只想要打印出主要层级，可以检查模块的类型或其名称中是否包含特定的分隔符，这通常表明了一个层级的子层。在这里，我们检查模块的名称是否包含点号（点号通常用于分隔子模块的名称）。如果没有点号，我们可以认为这是一个顶级模块。\n\nfor name, module in cnn.named_modules():\n    # 如果名字是空，那么我们是最顶级；如果没有点，那么是顶级；有点的是子模块。\n    if name == '':\n        print(module)\n\nResNet(\n  (conv1): Conv1d(1, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (encoder): Sequential(\n    (0): Sequential(\n      (0): ResidualBlock(\n        (conv1): Conv1d(64, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential(\n          (0): Conv1d(64, 100, kernel_size=(1,), stride=(1,), bias=False)\n          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential()\n      )\n    )\n    (1): Sequential(\n      (0): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential(\n          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential()\n      )\n    )\n    (2): Sequential(\n      (0): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential(\n          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential()\n      )\n    )\n    (3): Sequential(\n      (0): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential(\n          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential()\n      )\n    )\n    (4): Sequential(\n      (0): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential(\n          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential()\n      )\n    )\n    (5): Sequential(\n      (0): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential(\n          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential()\n      )\n    )\n  )\n  (linear): Linear(in_features=3200, out_features=30, bias=True)\n)\n\n\n这段代码定义了一个一维卷积神经网络架构（ResNet），主要用于处理一维数据。这个网络结构中包含了多个残差块，每个残差块由两个卷积层和一个恒等映射（shortcut）组成。\n\n28.2.1 网络架构概述\n\n初始卷积层和批归一化层：\n\nconv1: 一个输入通道（通常为单通道的信号数据）到64个输出通道的卷积层，卷积核大小为5，步幅为1，填充为2。\nbn1: 对64个通道的输出进行批归一化。\n\n编码器（encoder）：\n\nencoder 是一个由6个 Sequential 模块组成的层级结构。每个 Sequential 模块包含两个残差块（ResidualBlock）。\n\n残差块（ResidualBlock）：\n\n每个残差块包含两个卷积层和一个恒等映射（shortcut）。具体结构如下：\n\nconv1 和 conv2: 卷积核大小为5，填充为2，无偏置的卷积层。\nbn1 和 bn2: 对每个卷积层输出进行批归一化。\nshortcut: 在输入和输出通道数不同或步幅不同的情况下，使用卷积层和批归一化层调整尺寸。\n\n\n线性层（linear）：\n\nlinear: 将编码器的输出特征映射到30个输出特征，通常用于分类任务。\n\n\n\n\n28.2.2 详细结构\n\n第一层卷积和批归一化：\n(conv1): Conv1d(1, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n(bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n编码器部分（encoder）：\n\n每个Sequential包含两个残差块。残差块中的卷积层和批归一化层配置如下：\n(0): Sequential(\n  (0): ResidualBlock(\n    (conv1): Conv1d(64, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n    (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n    (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (shortcut): Sequential(\n      (0): Conv1d(64, 100, kernel_size=(1,), stride=(1,), bias=False)\n      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (1): ResidualBlock(\n    (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n    (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n    (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (shortcut): Sequential()\n  )\n)\n\n最后的线性层：\n(linear): Linear(in_features=3200, out_features=30, bias=True)\n\n\n28.2.2.1 关键点\n\n残差连接：通过恒等映射（shortcut）解决梯度消失问题，允许训练更深的网络。\n卷积层：使用多个卷积层提取特征，尤其是卷积核大小为5的卷积层。\n批归一化：在每个卷积层之后使用批归一化层，提高训练的稳定性和速度。\n线性层：最后的线性层将特征映射到30个输出，用于分类或其他任务。\n\n这个 ResNet 变体是一个较为复杂的一维卷积神经网络，适用于处理序列数据或时间序列数据，并具有强大的特征提取和分类能力。",
    "crumbs": [
      "拉曼光谱识别病原菌",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>使用模型进行病原菌检测</span>"
    ]
  },
  {
    "objectID": "pretrained-model.html#使用模型进行预测",
    "href": "pretrained-model.html#使用模型进行预测",
    "title": "28  使用模型进行病原菌检测",
    "section": "28.3 使用模型进行预测",
    "text": "28.3 使用模型进行预测\n现在我们使用训练好的模型进行预测，并报告每个菌株的准确率。这个数字应该接近图2中报告的82.2%，但由于在微调过程中对微调数据集进行了随机采样，所以不会完全相同。\n\nimport numpy as np\n\n# 载入数据\nX = np.load('./data/raman/X_test.npy')\ny = np.load('./data/raman/y_test.npy')\n\n# 打印数据形状\nprint(X.shape, y.shape)\n\n(3000, 1000) (3000,)\n\n\n在这个例子中，我们没有使用 DataLoader，而是直接将整个数据集 X 转换为张量，并将其传递给模型进行预测。\n\ncnn.eval()\n\nX_tensor = torch.tensor(X, dtype=torch.float32)\nX_tensor = X_tensor.unsqueeze(1)\nX_tensor = X_tensor.to(device)\n\nwith torch.no_grad():\n    preds = cnn(X_tensor)\n\n打印预测准确性。\n\n# 计算准确性\ny_hat = preds.argmax(dim=1).cpu().numpy()\nacc = (y_hat == y).mean()\nprint('Accuracy: {:0.1f}%'.format(100*acc))\n\nAccuracy: 75.9%",
    "crumbs": [
      "拉曼光谱识别病原菌",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>使用模型进行病原菌检测</span>"
    ]
  },
  {
    "objectID": "pretrained-model.html#绘制混淆矩阵",
    "href": "pretrained-model.html#绘制混淆矩阵",
    "title": "28  使用模型进行病原菌检测",
    "section": "28.4 绘制混淆矩阵",
    "text": "28.4 绘制混淆矩阵\n\n28.4.1 读取菌株名称\nconfig.py 文件中定义了菌株名称，现在把这些定义读取出来，重新绘制混淆矩阵。\n\nimport config\n\n# 读取菌株名称顺序\norder = config.ORDER\n\n# 读取菌株名称\nstrains = config.STRAINS\n\n# 打印菌株名称顺序\nprint(order)\n\n# 打印菌株名称\nprint(strains)\n\n[16, 17, 14, 18, 15, 20, 21, 24, 23, 26, 27, 28, 29, 25, 6, 7, 5, 3, 4, 9, 10, 2, 8, 11, 22, 19, 12, 13, 0, 1]\n{0: 'C. albicans', 1: 'C. glabrata', 2: 'K. aerogenes', 3: 'E. coli 1', 4: 'E. coli 2', 5: 'E. faecium', 6: 'E. faecalis 1', 7: 'E. faecalis 2', 8: 'E. cloacae', 9: 'K. pneumoniae 1', 10: 'K. pneumoniae 2', 11: 'P. mirabilis', 12: 'P. aeruginosa 1', 13: 'P. aeruginosa 2', 14: 'MSSA 1', 15: 'MSSA 3', 16: 'MRSA 1 (isogenic)', 17: 'MRSA 2', 18: 'MSSA 2', 19: 'S. enterica', 20: 'S. epidermidis', 21: 'S. lugdunensis', 22: 'S. marcescens', 23: 'S. pneumoniae 2', 24: 'S. pneumoniae 1', 25: 'S. sanguinis', 26: 'Group A Strep.', 27: 'Group B Strep.', 28: 'Group C Strep.', 29: 'Group G Strep.'}\n\n\n将 y 和 y_hat 中的数字编号使用 order 调整顺序后，再转变为 STRAINS 中的菌株名称，绘制混淆矩阵。\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 计算混淆矩阵\nconf_matrix = confusion_matrix(y, y_hat, labels=order)\n\n# 获取标签名称\nlabel_names = [strains[i] for i in order]\n\n# 绘制带有菌株名称的混淆矩阵\nplt.figure(figsize=(10, 8))\n\n# 创建热图\nax = sns.heatmap(conf_matrix, \n            annot=True, \n            fmt='d', \n            cmap='YlGnBu',\n            xticklabels=label_names,\n            yticklabels=label_names)\n\n# 将x轴标签移到顶部\nax.xaxis.set_ticks_position('top')\nax.xaxis.set_label_position('top')\n\nplt.xticks(rotation=45, ha='left')\nplt.yticks(rotation=0)\nplt.xlabel('Predicted')\nplt.ylabel('True')\n\n# 调整布局以防止标签被切掉\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "拉曼光谱识别病原菌",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>使用模型进行病原菌检测</span>"
    ]
  },
  {
    "objectID": "pretrained-model.html#模型预测细节",
    "href": "pretrained-model.html#模型预测细节",
    "title": "28  使用模型进行病原菌检测",
    "section": "28.5 模型预测细节",
    "text": "28.5 模型预测细节\n\n28.5.1 切换工作模式\n在 PyTorch 中，你可以通过检查模型的 .training 属性来查看模型当前是在训练模式还是在评估模式。这个属性是一个布尔值，当模型处于训练模式时为 True，而在评估模式（也就是说，进行推理时）为 False。\n调用 model.eval() 可以将模型切换到评估模式，关闭了像 Dropout 和 BatchNorm 这样的层的特定训练时行为。相应地，model.train() 将模型切回训练模式。\n在实际应用中，确保在进行模型评估、验证或测试时调用 model.eval() 来获得正确的预测结果是非常重要的。\n\n# 切换模型模式\ncnn.eval()\n\nResNet(\n  (conv1): Conv1d(1, 64, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (encoder): Sequential(\n    (0): Sequential(\n      (0): ResidualBlock(\n        (conv1): Conv1d(64, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential(\n          (0): Conv1d(64, 100, kernel_size=(1,), stride=(1,), bias=False)\n          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential()\n      )\n    )\n    (1): Sequential(\n      (0): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential(\n          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential()\n      )\n    )\n    (2): Sequential(\n      (0): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential(\n          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential()\n      )\n    )\n    (3): Sequential(\n      (0): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential(\n          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential()\n      )\n    )\n    (4): Sequential(\n      (0): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential(\n          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential()\n      )\n    )\n    (5): Sequential(\n      (0): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(2,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential(\n          (0): Conv1d(100, 100, kernel_size=(1,), stride=(2,), bias=False)\n          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): ResidualBlock(\n        (conv1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (shortcut): Sequential()\n      )\n    )\n  )\n  (linear): Linear(in_features=3200, out_features=30, bias=True)\n)\n\n\n\n\n28.5.2 模型的输出格式\n在 PyTorch 中，模型的各个子模块可以通过 named_modules() 方法来遍历，该方法返回一个迭代器，包括所有子模块的名称和模块对象。如果你想查看最后 5 个named_modules，你可以将迭代器转换成列表，然后选取最后 5 个条目。\n下面这段代码会打印出最后 5 个模块的名称和它们的结构。如果模型中子模块的总数少于 5 个，这段代码仍然会工作，但是它会返回模型中所有的子模块。\n\nimport torch.nn as nn\n\ndef print_last_five_modules(model):\n    # 假设有一个模型实例叫做 model，可以是任何继承自nn.Module的类的实例\n    # model = YourModel()\n\n    # 获取所有named modules的列表\n    named_modules_list = list(model.named_modules())\n\n    # 获取最后5个named modules\n    last_five_named_modules = named_modules_list[-5:]\n\n    # 打印这些modules的名字和结构\n    for name, module in last_five_named_modules:\n        print(name, '-&gt;', module)\n\nprint_last_five_modules(cnn)\n\nencoder.5.1.bn1 -&gt; BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nencoder.5.1.conv2 -&gt; Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\nencoder.5.1.bn2 -&gt; BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nencoder.5.1.shortcut -&gt; Sequential()\nlinear -&gt; Linear(in_features=3200, out_features=30, bias=True)\n\n\n现在网络的最后一个模块的名字是 linear，Linear(in_features=3200, out_features=30, bias=True) 表示这是一个线性层（也称作全连接层或者密集层）的声明，在神经网络中用于变换输入特征的线性映射。下面是参数的具体含义：\n\nin_features=3200: 这指的是输入特征的数量，也就是说这个层期望每个输入数据的维度是3200。在神经网络中，如果这是第一个层，那么每个输入样本应该是一个含有3200个元素的一维张量。如果这个层不是第一个层，那么前一个层的输出特征数量应该是3200。\nout_features=30: 这指的是输出特征的数量，这一层将会输出30个特征值。无论输入的特征有多少个，经过这个层的线性变换后，最后输出的每个样本都是一个含有30个元素的一维张量。\nbias=True: 这一选项表示这一层包含偏置（bias），每个输出特征将会有其相对应的偏置值。偏置是一个可学习的参数，它的默认初始值通常是很小的随机数。在进行线性变换后加上偏置，可以增加模型的灵活性。如果将bias设置为False，那么这一层就不会有偏置值。\n\n\n# 生成一个输入数据\ntest = torch.randn(1, 1, 1000).to(device)\n\n# 打印输出\nprint(cnn(test).shape)\n\ntorch.Size([1, 30])\n\n\n\n\n28.5.3 获取预测结果\n原始的预测结果为 preds，是一个形状为 (3000, 30) 的张量，表示每个样本属于每个类别的概率。\n\n# 打印预测结果的形状\nprint(preds.shape)\n\n# 打印预测结果\nprint(preds[1, :])\n\ntorch.Size([3000, 30])\ntensor([ -2.3633, -16.7902, -31.0268, -36.2345, -46.9973, -19.5463, -16.5988,\n        -36.1822, -33.4462, -34.4168, -19.3850, -45.2024, -36.0918, -27.8640,\n        -21.4089, -48.9349, -37.9030, -30.3979, -41.8903, -18.3937, -24.8710,\n        -32.6808, -31.7885, -18.2239, -14.3602, -33.7739, -23.3159, -28.2033,\n        -40.5117, -41.5156], device='mps:0')\n\n\n使用 torch.argmax 获取预测结果。argmax 返回的是最大值的索引，而不是最大值本身。\n\n# 获取预测结果\ny_hat = preds.argmax(dim=1).cpu().numpy()\n\n# 打印预测结果\nprint(y_hat)\n\n[ 0  0  0 ... 29 29 29]\n\n\n\n\n\n\n“Csho33/Bacteria-ID: Source Code and Demos for \"Rapid Identification of Pathogenic Bacteria Using Raman Spectroscopy and Deep Learning\".” n.d. https://github.com/csho33/bacteria-ID.",
    "crumbs": [
      "拉曼光谱识别病原菌",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>使用模型进行病原菌检测</span>"
    ]
  },
  {
    "objectID": "computer-vision-image-analysis.html",
    "href": "computer-vision-image-analysis.html",
    "title": "29  计算机视觉分析实验图片",
    "section": "",
    "text": "29.1 研究内容简介",
    "crumbs": [
      "计算机视觉分析实验图片",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>计算机视觉分析实验图片</span>"
    ]
  },
  {
    "objectID": "computer-vision-image-analysis.html#研究内容简介",
    "href": "computer-vision-image-analysis.html#研究内容简介",
    "title": "29  计算机视觉分析实验图片",
    "section": "",
    "text": "29.1.1 研究背景\n植被覆盖度（Vegetation Fraction, VF）是描述作物状态和产量的重要指标，但绿色植被覆盖度（Green Fraction, GF）更能反映作物的功能特性。GF 用于估算绿色面积指数（GAI），而衰老植被覆盖度（Senescent Fraction, SF）则用于表征生物或非生物胁迫、营养循环和老化过程。当前的遥感方法在估计这些参数时面临一些挑战，特别是在高分辨率 RGB 图像中准确分割绿色和衰老植被。\n\n\n29.1.2 研究目的\n本文旨在开发一种名为 SegVeg 的两阶段语义分割方法，该方法结合深度学习和浅层学习技术，将高分辨率 RGB 图像分割成背景、绿色植被和衰老植被三类。SegVeg 方法的目标是减少手动标注的工作量，同时保持较高的分割精度。\n\n\n29.1.3 研究方法\nSegVeg 方法分为两个阶段： 1. 使用 U-net 模型将图像分为植被和背景。 2. 使用支持向量机（SVM）将植被像素进一步分为绿色和衰老植被。\n\n\n29.1.4 数据来源与实验设计\n\n数据集:\n\nDataset #1: 包含 8 个子数据集，总共 2015 个 512x512 像素的补丁，用于训练 U-net 2C 模型。\nDataset #2: 包含 441 个带有网格注释的图像，用于训练 SVM 模型。\nDataset #3: 使用 SegVeg 方法生成的完全自动注释的补丁，用于训练 3 类 U-net 模型（U-net 3C）。\n\n\n\n\n29.1.5 实验设计\n\n第一阶段:\n\n使用 U-net 模型将图像分为植被和背景。\n使用 EfficientNet-B2 架构作为骨干网络，通过 Dice loss 函数和 Adam 优化器进行训练。\n\n第二阶段:\n\n使用 SVM 对植被像素进行分类，使用多个颜色空间和变换（如 RGB、HSV、CIELab 等）作为输入特征。\n通过前向包装方法选择最合适的输入特征，并使用网格搜索算法调整超参数。\n\n\n\n\n29.1.6 核心发现\n\nSegVeg 方法能够准确地将图像分割为背景、绿色植被和衰老植被三类。在分割绿色和衰老植被方面表现出较好的性能。\n在某些情况下，背景和衰老植被之间存在混淆，尤其是在图像的暗区和亮区。光照条件对分割结果有显著影响。\nU-net 3C 模型的表现与 SegVeg 方法相似，但在绿色植被的分割上略有下降。",
    "crumbs": [
      "计算机视觉分析实验图片",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>计算机视觉分析实验图片</span>"
    ]
  },
  {
    "objectID": "computer-vision-image-analysis.html#环境依赖",
    "href": "computer-vision-image-analysis.html#环境依赖",
    "title": "29  计算机视觉分析实验图片",
    "section": "29.2 环境依赖",
    "text": "29.2 环境依赖\n本项目需要下列 Python 库的支持：\n\ndatasets：用于从 Huggingface 加载数据集\nPyTorch Lightning: 用于构建神经网络模型和训练循环。\nSegmentation Models PyTorch: 提供了许多预训练的图像分割模型，如 U-net、DeepLabV3、PSPNet 等。\nOpenCV: 用于图像处理和可视化。\nMatplotlib: 用于绘制图表和图像。\n其他库: 用于数据处理、评估指标计算等。",
    "crumbs": [
      "计算机视觉分析实验图片",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>计算机视觉分析实验图片</span>"
    ]
  },
  {
    "objectID": "computer-vision-image-analysis.html#数据集",
    "href": "computer-vision-image-analysis.html#数据集",
    "title": "29  计算机视觉分析实验图片",
    "section": "29.3 数据集",
    "text": "29.3 数据集\nVegAnn 是一个包含 3,775 张多作物 RGB 图像的集合，旨在增强作物植被分割研究。\n\nVegAnn 数据集包含 3775 张图片\n图片尺寸为 512*512 像素\n对应的二值掩膜中，0 表示土壤和作物残留物（背景），255 表示植被（前景）\n该数据集包含 26 种以上作物物种，各物种的图片数量分布不均匀\nVegAnn 数据集由使用不同采集系统和配置拍摄的户外图像编译而成\n\nVegAnn 项目的数据集可以在 Huggingface 上访问。请参阅以下链接：https://huggingface.co/datasets/simonMadec/VegAnn。数据预处理的步骤参见 数据预处理。\n数据来源与组成\n\nDataset #1：\n\n内容：2015个512×512像素的RGB图像块，覆盖8个子数据集（UTokyo、P2S2、Wuhan、CVPPP、GEVES、Phenofix、Phenomobile、Bonirob）。\n\n标注：手动标注为“植被（绿色+衰老）”和“背景”。\n\n多样性：涵盖水稻、小麦、玉米、棉花等多种作物，不同生长阶段（营养期至衰老期），以及不同光照条件和土壤背景。\n\n空间分辨率：0.3–2 mm，确保细节捕捉。\n\nDataset #2：\n\n内容：441张512×512像素图像，通过规则网格（8–11像素间隔）标注像素点，共19,738个标注像素（训练集6132，测试集13,606）。\n\n标注类别：绿色植被、衰老植被、背景、不确定像素（未用于训练）。\n\n子数据集：LITERAL（手持设备）、PHENOMOBILE（无人车+闪光灯）、P2S2（多作物多平台）。\n\n挑战：不确定像素（16%）主要因光照过暗/过亮或混合像素导致。\n\nDataset #3：\n\n生成方式：通过SegVeg对Dataset #1图像生成伪标签（3类：背景、绿色、衰老），用于训练U-net 3C模型。\n\n\n数据特点与局限\n\n优势：覆盖广泛作物、生长阶段和光照条件，增强模型泛化能力。\n\n局限：\n\n标注不一致性：不同操作者对“衰老”与“背景”的主观判断差异。\n\n不确定像素处理：排除部分像素可能导致模型对极端光照场景适应不足。",
    "crumbs": [
      "计算机视觉分析实验图片",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>计算机视觉分析实验图片</span>"
    ]
  },
  {
    "objectID": "computer-vision-image-analysis.html#模型架构",
    "href": "computer-vision-image-analysis.html#模型架构",
    "title": "29  计算机视觉分析实验图片",
    "section": "29.4 模型架构",
    "text": "29.4 模型架构\n作者自定义了一个 VegAnnModel 类。这个类初始化一个 U-net 模型，使用 ResNet34 作为编码器，输入通道数为 3（RGB 图像），输出类别数为 1（二值分割）。\nSegVeg两阶段模型\n\n第一阶段（U-net 2C）：\n\n架构：基于EfficientNet-B2骨干网络（ImageNet预训练），编码器-解码器结构，输出植被与背景二分类分割。\n\n输入/输出：512×512 RGB图像 → 二值掩膜（植被 vs 背景）。\n\n第二阶段（SVM分类器）：\n\n输入特征：从植被像素中提取14个颜色空间特征（R、G、B、H、S、a、b、GE、M、YE、Cb、Cr、I、Q），通过前向选择法筛选。\n\n分类目标：绿色植被 vs 衰老植被。\n\n超参数：RBF核，C=1，γ=10⁻³，通过网格搜索优化。\n\n\n对比模型（U-net 3C）\n\n架构：与U-net 2C相同，但输出三分类（背景、绿色、衰老）。\n\n训练数据：使用SegVeg生成的伪标签（弱监督），未依赖手动标注的全类别掩膜。",
    "crumbs": [
      "计算机视觉分析实验图片",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>计算机视觉分析实验图片</span>"
    ]
  },
  {
    "objectID": "computer-vision-image-analysis.html#训练过程",
    "href": "computer-vision-image-analysis.html#训练过程",
    "title": "29  计算机视觉分析实验图片",
    "section": "29.5 训练过程",
    "text": "29.5 训练过程\nU-net 2C训练细节\n\n损失函数：Dice Loss（优化分割边界）。\n\n优化器：Adam，初始学习率0.01，逐步降至10⁻⁶。\n\n数据增强：Albumentations库（旋转、缩放、翻转等），提升模型鲁棒性。\n\n硬件：NVIDIA GeForce RTX 3090 GPU，批大小32。\n\nSVM训练细节\n\n特征归一化：标准化颜色空间特征以消除量纲差异。\n\n交叉验证：留一法验证，确保泛化能力。\n\nU-net 3C训练\n\n弱监督策略：利用SegVeg生成的伪标签，减少人工标注成本。\n\n潜在问题：伪标签误差可能传播至U-net 3C训练中。",
    "crumbs": [
      "计算机视觉分析实验图片",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>计算机视觉分析实验图片</span>"
    ]
  },
  {
    "objectID": "computer-vision-image-analysis.html#预测性能",
    "href": "computer-vision-image-analysis.html#预测性能",
    "title": "29  计算机视觉分析实验图片",
    "section": "29.6 预测性能",
    "text": "29.6 预测性能\nSegVeg性能\n\n像素级精度：\n\n绿色植被：F1=94%（R²=0.94），背景：F1=73%（R²=0.73），衰老植被：F1=70%（R²=0.70）。\n\n主要混淆：衰老植被与背景（尤其暗区/高光区）。\n\n\n图像级分数预测：\n\n绿色分数误差：1%（95%置信区间），衰老和背景误差：2.1–2.7%。\n\n\nU-net 3C性能对比\n\n像素级精度：与SegVeg接近，但绿色植被F1略低（90% vs 94%）。\n\n细节处理：卷积特性导致边缘模糊，而SVM像素分类更清晰（如图像细小结构保留）。",
    "crumbs": [
      "计算机视觉分析实验图片",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>计算机视觉分析实验图片</span>"
    ]
  },
  {
    "objectID": "computer-vision-image-analysis.html#结果分析",
    "href": "computer-vision-image-analysis.html#结果分析",
    "title": "29  计算机视觉分析实验图片",
    "section": "29.7 结果分析",
    "text": "29.7 结果分析\n优点\n\n标注效率：两阶段设计减少全图像标注需求（仅需部分像素标注）。\n\n颜色空间融合：SVM结合多颜色空间特征（如CMYK、YIQ），提升衰老植被区分能力。\n\n适用性：公开代码与预训练模型（GitHub），支持快速部署。\n\n缺点\n\n光照敏感性：暗区/高光区分类性能下降（如PHENOMOBILE数据集因闪光灯导致暗区混淆）。\n\n颜色连续性挑战：衰老早期与绿色植被颜色过渡区域易误判。\n\n弱监督限制：U-net 3C依赖SegVeg伪标签，可能继承其误差。",
    "crumbs": [
      "计算机视觉分析实验图片",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>计算机视觉分析实验图片</span>"
    ]
  },
  {
    "objectID": "computer-vision-image-analysis.html#预测性能-1",
    "href": "computer-vision-image-analysis.html#预测性能-1",
    "title": "29  计算机视觉分析实验图片",
    "section": "29.8 预测性能",
    "text": "29.8 预测性能\n\nimport torch\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load the model - 统一使用一种加载方式\nckt_path = \"data/vegann/epoch5.ckpt\"\ncheckpoint = torch.load(ckt_path, map_location=torch.device('cpu'))\nmodel = VegAnnModel(\"Unet\",\"resnet34\",in_channels = 3, out_classes=1 )\nmodel.load_state_dict(checkpoint[\"state_dict\"])\n# 只保留一个预处理函数\npreprocess_input = get_preprocessing_fn('resnet34', pretrained='imagenet')\nmodel.eval()\n\n\nimname = \"data/vegann/test.jpg\"\n\nimage = cv2.imread(imname)\nim = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n# 使用已定义的预处理函数\nimage = preprocess_input(im)\nimage = image.astype('float32')\n\n\ninputs = torch.tensor(image) # , dtype=float\n# print(inputs.size)\ninputs = inputs.permute(2,0,1)\ninputs = inputs[None,:,:,:]\n# print(inputs.shape)\nlogits = model(inputs)\npr_mask = logits.sigmoid()\n\npred = (pr_mask &gt; 0.5).numpy().astype(np.uint8) \n\nim1_pred = colorTransform_VegGround(im,pred,0.8,0.2)\nim2_pred = colorTransform_VegGround(im,pred,0.2,0.8)\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.imshow(im)\nax1.set_title(\"Input Image\")\n\nax2.imshow(im2_pred)\nax2.set_title(\"Prediction\")\nplt.show()\n\n\n\n\n\nSerouart, Mario, Simon Madec, Etienne David, Kaaviya Velumani, Raul Lopez Lozano, Marie Weiss, and Frédéric Baret. 2022. “SegVeg: Segmenting RGB Images into Green and Senescent Vegetation by Combining Deep and Shallow Methods.” Plant Phenomics (Washington, D.C.) 2022: 9803570. https://doi.org/10.34133/2022/9803570.",
    "crumbs": [
      "计算机视觉分析实验图片",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>计算机视觉分析实验图片</span>"
    ]
  },
  {
    "objectID": "segveg-data-preparation.html",
    "href": "segveg-data-preparation.html",
    "title": "30  数据预处理",
    "section": "",
    "text": "30.1 数据集简介\nVegAnn 是一个包含 3,775 张多作物 RGB 图像的集合，旨在增强作物植被分割研究。\n这些数据是由包括 Arvalis、INRAe、东京大学、昆士兰大学、NEON 和 EOLAB 等机构的合作提供的。\n这些图像涵盖了不同的物候阶段，并在各种照明条件下使用不同的系统和平台捕获。通过聚合来自不同项目和机构的子数据集，VegAnn 代表了广泛的测量条件、作物种类和发育阶段。\nVegAnn 项目的数据集可以在 Huggingface 上访问。请参阅以下链接：https://huggingface.co/datasets/simonMadec/VegAnn。\n# 使用 Huggingface Datasets 加载数据集\nfrom datasets import load_dataset\n\n# VegAnn 只有 train 集\nds = load_dataset(\"simonMadec/VegAnn\", split=\"train\")\n\nprint(ds)\n\nDataset({\n    features: ['image', 'mask', 'System', 'Orientation', 'latitude', 'longitude', 'date', 'LocAcc', 'Species', 'Owner', 'Dataset-Name', 'TVT-split1', 'TVT-split2', 'TVT-split3', 'TVT-split4', 'TVT-split5'],\n    num_rows: 3775\n})",
    "crumbs": [
      "计算机视觉分析实验图片",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>数据预处理</span>"
    ]
  },
  {
    "objectID": "segveg-data-preparation.html#数据集简介",
    "href": "segveg-data-preparation.html#数据集简介",
    "title": "30  数据预处理",
    "section": "",
    "text": "VegAnn 数据集包含 3775 张图片\n图片尺寸为 512*512 像素\n对应的二值掩膜中，0 表示土壤和作物残留物（背景），255 表示植被（前景）\n该数据集包含 26 种以上作物物种，各物种的图片数量分布不均匀\nVegAnn 数据集由使用不同采集系统和配置拍摄的户外图像编译而成",
    "crumbs": [
      "计算机视觉分析实验图片",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>数据预处理</span>"
    ]
  },
  {
    "objectID": "segveg-data-preparation.html#数据字段",
    "href": "segveg-data-preparation.html#数据字段",
    "title": "30  数据预处理",
    "section": "30.2 数据字段",
    "text": "30.2 数据字段\n数据集中的每一条记录包含以下字段：\n\nid: 每个图像补丁的唯一标识符【注：可将索引设为 id】。\nSystem: 用于获取照片的成像系统（例如，手持相机、DHP、无人机）。\nOrientation: 图像捕捉时相机的方向（例如，正下方，45 度）。\nlatitude 和 longitude: 拍摄图像的地理坐标。\ndate: 图像获取日期。\nLocAcc: 位置准确性标志（1 表示高准确性，0 表示低或不确定的准确性）。\nSpecies: 图像中展示的作物种类（例如，小麦、玉米、大豆）。\nOwner: 提供图像的机构或实体（例如，Arvalis，INRAe）。\nDataset-Name: 图像来源的子数据集或项目（例如，Phenomobile，Easypcc）。\nTVT-split1 到 TVT-split5: 表示训练/验证/测试划分配置的字段，便于各种实验设置。\n\n\n# 统计 ds 中各个 Dataset-Name 的数量\nfrom collections import Counter\n\ndataset_names = ds['Dataset-Name']\ncounts = Counter(dataset_names)\n\n# Print sorted counts\nfor name, count in sorted(counts.items()):\n    print(f\"{name}: {count}\")\n\nCAN-Eye: 948\nEasypcc: 583\nINVITA: 500\nLiteral: 938\nP2S2: 499\nPhenomobile: 225\nweb: 82\n\n\n\n\n\n\n\n\nNote\n\n\n\n给图片添加 id\n\n# 给每一行添加 'id'，其值为行号\nds = ds.map(lambda example, idx: { \"id\": idx, **example}, with_indices=True, num_proc=10)\n\n在 Huggingface Datasets 中，map 方法可以用于对数据集进行转换。在上面的代码中，我们使用 map 方法为数据集的每一行添加一个 id 字段，其值为行号。num_proc 参数表示使用的进程数，可以加快数据处理速度。\n**example 语法用于将 example 字典中的所有键值对解包为关键字参数。这样可以将 id 字段添加到每个示例中，而不会影响其他字段。",
    "crumbs": [
      "计算机视觉分析实验图片",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>数据预处理</span>"
    ]
  },
  {
    "objectID": "segveg-data-preparation.html#绘制示例图像",
    "href": "segveg-data-preparation.html#绘制示例图像",
    "title": "30  数据预处理",
    "section": "30.3 绘制示例图像",
    "text": "30.3 绘制示例图像\n首先，我们绘制 VegAnn 数据集的一些信息Figure 30.1。\n\n# 绘制 VegAnn 数据集的信息\nimport matplotlib.pyplot as plt\n\n# Get the first image and its metadata\nsample = ds[0]\nimage_data = sample['image']\n\n# Create figure with two subplots\nfig, ax = plt.subplots(figsize=(6, 6))\n\n# Display the image\nax.imshow(image_data)\n\n# Add metadata as text annotations in red\nmetadata_text = f\"ID: {0}\\n\"\nmetadata_text += f\"System: {sample['System']}\\n\"\nmetadata_text += f\"Orientation: {sample['Orientation']}\\n\"\nmetadata_text += f\"Location: ({sample['latitude']}, {sample['longitude']})\\n\"\nmetadata_text += f\"Date: {sample['date']}\\n\"\nmetadata_text += f\"LocAcc: {sample['LocAcc']}\\n\"\nmetadata_text += f\"Species: {sample['Species']}\\n\"\nmetadata_text += f\"Owner: {sample['Owner']}\\n\"\nmetadata_text += f\"Dataset: {sample['Dataset-Name']}\"\n\n# Add text to the plot in red color at bottom left\nax.text(40, 460, metadata_text, color='red', fontsize=10, \n    bbox=dict(facecolor='white', alpha=0.7))\n\n# Set title and remove axis\nax.set_title(\"Sample Image with Metadata\")\nax.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 30.1: VegAnn 数据集的示例图像和元数据\n\n\n\n\n\n此外，我们还可以绘制 VegAnn 数据集的示例图像和对应掩膜，以便更好地了解数据集的内容Figure 30.2。\n\nimport matplotlib.pyplot as plt\n\n# 绘制示例图像 \nfig, axs = plt.subplots(3, 4, figsize=(7, 6))\n\nfor i in range(3):\n    for j in range(2):\n        idx = i * 2 + j\n        # 获取图像和掩膜\n        image = ds[idx]['image']\n        mask = ds[idx]['mask']\n        \n        # 绘制原始图像\n        axs[i,j*2].imshow(image)\n        axs[i,j*2].set_title(f\"Image {idx}\")\n        axs[i,j*2].axis('off')\n        \n        # 绘制掩膜\n        axs[i,j*2+1].imshow(mask, cmap='binary')\n        axs[i,j*2+1].set_title(f\"Mask {idx}\")\n        axs[i,j*2+1].axis('off')\n\nfig.suptitle(\"Sample Images and Masks\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 30.2: VegAnn 数据集的示例图像和对应掩膜",
    "crumbs": [
      "计算机视觉分析实验图片",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>数据预处理</span>"
    ]
  },
  {
    "objectID": "segveg-data-preparation.html#扩展数据集类",
    "href": "segveg-data-preparation.html#扩展数据集类",
    "title": "30  数据预处理",
    "section": "30.4 扩展数据集类",
    "text": "30.4 扩展数据集类\n我们需要扩展 Dataset，定义一个自定义的数据集类来处理从 Hugging Face 加载的数据。\n\n# 定义 VegAnnDataset 类\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport torchvision.transforms as transforms\n\nclass VegAnnDataset(Dataset):\n    def __init__(self, dataset, transform=None):\n        self.dataset = dataset\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        image = np.array(item['image'])\n        mask = np.array(item['mask'])\n\n        if self.transform:\n            augmented = self.transform(image=image, mask=mask)\n            image = augmented['image']\n            mask = augmented['mask']\n\n        image = transforms.ToTensor()(image)\n        mask = torch.tensor(mask, dtype=torch.long).unsqueeze(0)\n\n        # 返回字典格式，与VegAnnModel的shared_step方法期望的输入格式匹配\n        return {\"id\": idx, \"image\": image, \"mask\": mask}",
    "crumbs": [
      "计算机视觉分析实验图片",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>数据预处理</span>"
    ]
  },
  {
    "objectID": "segveg-data-preparation.html#定义数据增强",
    "href": "segveg-data-preparation.html#定义数据增强",
    "title": "30  数据预处理",
    "section": "30.5 定义数据增强",
    "text": "30.5 定义数据增强\n我们使用 albumentations 库来定义数据增强的方法。这里我们使用 Resize 和 Normalize 方法。\n\n# 定义数据增强方法\nfrom albumentations import Compose, Resize, Normalize, HorizontalFlip, RandomRotate90, ColorJitter, ToFloat\n\n# 简化数据增强流程\ntransform = Compose([\n    Resize(512, 512),\n    HorizontalFlip(p=0.5),\n    RandomRotate90(p=0.5),\n    ColorJitter(brightness=0.2, contrast=0.2),\n    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nCompose：这是一个容器，它将多个变换组合在一起，并按顺序应用它们。在这个例子中，Compose 包含了多个变换：\n\nResize(512, 512)：将输入图像的尺寸调整为 512x512 像素。这一步骤确保所有输入图像具有相同的尺寸，以便于后续处理。\nHorizontalFlip(p=0.5)：以 50% 的概率对图像进行水平翻转。这有助于增加数据的多样性，从而提高模型的泛化能力。\nRandomRotate90(p=0.5)：以 50% 的概率对图像进行随机旋转 90 度。这有助于模型学习不同角度的特征。\nColorJitter(brightness=0.2, contrast=0.2)：对图像进行颜色抖动，包括亮度和对比度。这有助于模型学习不同光照条件下的特征。\nNormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])：对图像进行归一化处理，使用给定的均值和标准差。这些值通常是基于 ImageNet 数据集计算得出的，适用于大多数自然图像。\n\n图像预处理是计算机视觉任务中的常见步骤，有助于提高模型性能。通过调整图像尺寸和归一化处理，可以使得模型更好地学习图像特征，从而提高模型的训练效果和泛化能力。\n\n\n\n\n\n\nTip\n\n\n\n在 Conda 中安装 albumentations 可以使用 conda-forge 源：\nconda install -c conda-forge albumentations\n\n\n对同一张图做数据增强，查看效果。\n\n# Load the first image and mask from the dataset\nsample = ds[0]\nimage = np.array(sample['image'])\nmask = np.array(sample['mask'])\n\n# Create a figure with a 4x4 grid (8 pairs of image+mask)\nfig, axs = plt.subplots(4, 4, figsize=(8, 8))\nfig.suptitle('Data Augmentation Examples', fontsize=12)\n\n# Generate 16 augmented pairs and display them\nfor i in range(8):\n    row = i // 2\n    col = (i % 2) * 2  # Each sample uses 2 columns (image + mask)\n    \n    # Apply augmentation\n    augmented = transform(image=image, mask=mask)\n    aug_image = augmented['image']\n    aug_mask = augmented['mask']\n\n    # un-normalize image\n    std = [0.229, 0.224, 0.225]\n    mean = [0.485, 0.456, 0.406]\n    aug_image = (aug_image * std) + mean\n    aug_image = np.clip(aug_image, 0, 1)\n\n    # Plot image\n    axs[row, col].imshow(aug_image)\n    axs[row, col].axis('off')\n    \n    # Plot mask\n    axs[row, col+1].imshow(aug_mask, cmap='spring')\n    axs[row, col+1].axis('off')\n\n# Adjust layout and display\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n反归一化\n因为 matplotlib.pyplot.imshow() 需要 RGB 数据在 [0,1]（float）或 [0,255]（int）范围内，而数据增强处理后数值超出了这个范围（如：最小值 -2.1179，最大值 2.64）。所以需要对图像进行反归一化处理，将数据还原到 [0,1] 范围内。\n反归一化有两步操作：\n\n乘以标准差 std：aug_image = aug_image * 0.229\n加上均值 mean：aug_image = aug_image + 0.485\n\n为了确保数据在 [0,1] 范围内，使用 np.clip() 方法进行截断处理：\n\n数据截断处理：aug_image = aug_image.clip(0, 1)",
    "crumbs": [
      "计算机视觉分析实验图片",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>数据预处理</span>"
    ]
  },
  {
    "objectID": "segveg-data-preparation.html#配置数据加载器",
    "href": "segveg-data-preparation.html#配置数据加载器",
    "title": "30  数据预处理",
    "section": "30.6 配置数据加载器",
    "text": "30.6 配置数据加载器\n这里，我们将数据集分为训练集和验证集，并创建相应的数据加载器。\n\nfrom torch.utils.data import DataLoader\n\n# 直接使用 datasets 内置的 train_test_split\nsplit_ds = ds.train_test_split(test_size=0.2, seed=42)\n\n# Create custom datasets\ntrain_dataset = VegAnnDataset(split_ds['train'], transform=transform)\nval_dataset = VegAnnDataset(split_ds['test'], transform=transform)\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)\n\n\n\n\n\n\n\nNote\n\n\n\nDataLoader 类是 PyTorch 中用于加载数据的实用工具。它可以自动对数据进行批处理、打乱和多线程加载，从而提高训练效率。\n要从中读取数据，不能使用 train_loader[0] 这样的方式，而是使用迭代器 iter(train_loader) 和 next(iter(train_loader))。\n在 Jupyter Notebook 中，将 num_workers 设置为 0 可以避免一些问题（因为 multiprocessing 可能会导致 worker 进程异常退出，陷入无尽的等待）。在生产环境中（*.py），可以根据需要调整 num_workers 的值。\n\n\n使用 DataLoader 加载一个 batch，显示其中的一些图片。\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 取一个 batch\nbatch = next(iter(train_loader))\n\n# 获取图像和掩码\nimages = batch['image'][:16]  # 取前 16 张\nmasks = batch['mask'][:16]\n\n# 反归一化参数\nstd = np.array([0.229, 0.224, 0.225])\nmean = np.array([0.485, 0.456, 0.406])\n\n# 创建 4x4 子图\nfig, axes = plt.subplots(4, 4, figsize=(8, 8))\nfig.suptitle('Sample Images and Masks', fontsize=12)\n\n# 使用 zip 函数遍历图像和掩码\nfor ax, img, mask in zip(axes.flat, images, masks):\n    # 将张量转换为 NumPy 数组 (H, W, C)\n    img = img.permute(1, 2, 0).numpy()\n    \n    # 反归一化\n    img = img * std + mean\n    img = np.clip(img, 0, 1)\n\n    # 处理 mask\n    mask = mask.squeeze().numpy()\n    mask = np.ma.masked_where(mask == 0, mask)\n\n    ax.imshow(img)\n    ax.imshow(mask, cmap='spring', alpha=0.4)\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "计算机视觉分析实验图片",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>数据预处理</span>"
    ]
  },
  {
    "objectID": "segveg-data-preparation.html#小结",
    "href": "segveg-data-preparation.html#小结",
    "title": "30  数据预处理",
    "section": "30.7 小结",
    "text": "30.7 小结\n本章介绍了如何使用 Huggingface Datasets 加载 VegAnn 数据集，并对数据进行预处理。我们展示了如何绘制示例图像和掩膜，以及如何配置神经网络模型。接下来，我们将使用VegAnn 数据集对 SegVeg 模型进行训练。",
    "crumbs": [
      "计算机视觉分析实验图片",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>数据预处理</span>"
    ]
  },
  {
    "objectID": "segveg-model-training.html",
    "href": "segveg-model-training.html",
    "title": "31  训练 SegVeg 模型",
    "section": "",
    "text": "31.1 载入需要的库\n这些库包括 PyTorch Lightning、Segmentation Models PyTorch、OpenCV、Matplotlib 等。他们在这个项目中的功能如下：\n# 载入需要的库\nimport pytorch_lightning as pl # 用于构建神经网络模型和训练循环\nimport torch # 用于构建神经网络模型和训练循环\nimport segmentation_models_pytorch as smp # 用于提供预训练的图像分割模型\nimport numpy as np # 用于数据处理\nimport cv2 # 用于图像处理和可视化\nfrom segmentation_models_pytorch.encoders import get_preprocessing_fn # 用于数据预处理\nimport matplotlib.pyplot as plt # 用于绘制图表和图像\nfrom typing import Dict, List # 用于类型提示",
    "crumbs": [
      "计算机视觉分析实验图片",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>训练 SegVeg 模型</span>"
    ]
  },
  {
    "objectID": "segveg-model-training.html#载入需要的库",
    "href": "segveg-model-training.html#载入需要的库",
    "title": "31  训练 SegVeg 模型",
    "section": "",
    "text": "PyTorch Lightning: 用于构建神经网络模型和训练循环。\nSegmentation Models PyTorch: 提供了许多预训练的图像分割模型，如 U-net、DeepLabV3、PSPNet 等。\nOpenCV: 用于图像处理和可视化。\nMatplotlib: 用于绘制图表和图像。\n其他库: 用于数据处理、评估指标计算等。",
    "crumbs": [
      "计算机视觉分析实验图片",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>训练 SegVeg 模型</span>"
    ]
  },
  {
    "objectID": "segveg-model-training.html#模型初始化",
    "href": "segveg-model-training.html#模型初始化",
    "title": "31  训练 SegVeg 模型",
    "section": "31.2 模型初始化",
    "text": "31.2 模型初始化\n首先，配置一个名为 VegAnnModel 的 PyTorch Lightning 模型，用于训练 U-net 模型。这个模型包含以下几个部分：\n\n__init__ 方法：初始化模型，包括选择模型架构、编码器名称、输入通道数、输出类别数等。\nforward 方法：定义前向传播过程，包括图像预处理、模型推理和输出。\nshared_step 方法：定义共享的训练/验证/测试步骤，包括计算损失、评估指标等。\nshared_epoch_end 方法：定义共享的训练/验证/测试 epoch 结束方法，用于计算并输出评估指标。\ntraining_step 方法：定义训练步骤，包括调用 shared_step 方法并保存输出。\non_train_epoch_end 方法：定义训练 epoch 结束方法，用于调用 shared_epoch_end 方法。\nvalidation_step 方法：定义验证步骤，包括调用 shared_step 方法并保存输出。\non_validation_epoch_end 方法：定义验证 epoch 结束方法，用于调用 shared_epoch_end 方法。\ntest_step 方法：定义测试步骤，包括调用 shared_step 方法并保存输出。\non_test_epoch_end 方法：定义测试 epoch 结束方法，用于调用 shared_epoch_end 方法。\nconfigure_optimizers 方法：定义优化器，这里使用 Adam 优化器。\n\n另外，还定义了一个辅助函数：\n\ncolorTransform_VegGround 方法：定义一个颜色转换函数，用于将预测的掩膜可视化。\n\n\n# Initialize the model\nclass VegAnnModel(pl.LightningModule):\n    def __init__(self, arch: str, encoder_name: str, in_channels: int, out_classes: int, **kwargs):\n        super().__init__()\n        self.model = smp.create_model(\n            arch,\n            encoder_name=encoder_name,\n            in_channels=in_channels,\n            classes=out_classes,\n            **kwargs,\n        )\n\n        # preprocessing parameteres for image\n        params = smp.encoders.get_preprocessing_params(encoder_name)\n        self.register_buffer(\"std\", torch.tensor(params[\"std\"]).view(1, 3, 1, 1))\n        self.register_buffer(\"mean\", torch.tensor(params[\"mean\"]).view(1, 3, 1, 1))\n\n        # for image segmentation dice loss could be the best first choice\n        self.loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n        self.train_outputs, self.val_outputs, self.test_outputs = [], [], []\n\n    def forward(self, image: torch.Tensor):\n        # normalize image here #todo\n        image = (image - self.mean) / self.std\n        mask = self.model(image)\n        return mask\n\n    def shared_step(self, batch: Dict, stage: str):\n        image = batch[\"image\"]\n\n        # Shape of the image should be (batch_size, num_channels, height, width)\n        # if you work with grayscale images, expand channels dim to have [batch_size, 1, height, width]\n        assert image.ndim == 4\n\n        # Check that image dimensions are divisible by 32,\n        # encoder and decoder connected by `skip connections` and usually encoder have 5 stages of\n        # downsampling by factor 2 (2 ^ 5 = 32); e.g. if we have image with shape 65x65 we will have\n        # following shapes of features in encoder and decoder: 84, 42, 21, 10, 5 -&gt; 5, 10, 20, 40, 80\n        # and we will get an error trying to concat these features\n        h, w = image.shape[2:]\n        assert h % 32 == 0 and w % 32 == 0\n\n        mask = batch[\"mask\"]\n\n        # Shape of the mask should be [batch_size, num_classes, height, width]\n        # for binary segmentation num_classes = 1\n        assert mask.ndim == 4\n\n        # Check that mask values in between 0 and 1, NOT 0 and 255 for binary segmentation\n        assert mask.max() &lt;= 1.0 and mask.min() &gt;= 0\n\n        logits_mask = self.forward(image)\n\n        # Predicted mask contains logits, and loss_fn param `from_logits` is set to True\n        loss = self.loss_fn(logits_mask, mask)\n\n        # Lets compute metrics for some threshold\n        # first convert mask values to probabilities, then\n        # apply thresholding\n        prob_mask = logits_mask.sigmoid()\n        pred_mask = (prob_mask &gt; 0.5).float()\n\n        # We will compute IoU metric by two ways\n        #   1. dataset-wise\n        #   2. image-wise\n        # but for now we just compute true positive, false positive, false negative and\n        # true negative 'pixels' for each image and class\n        # these values will be aggregated in the end of an epoch\n        tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), mask.long(), mode=\"binary\")\n\n        return {\n            \"loss\": loss,\n            \"tp\": tp,\n            \"fp\": fp,\n            \"fn\": fn,\n            \"tn\": tn,\n        }\n\n    def shared_epoch_end(self, outputs: List[Dict], stage: str):\n        # aggregate step metics\n        tp = torch.cat([x[\"tp\"] for x in outputs])\n        fp = torch.cat([x[\"fp\"] for x in outputs])\n        fn = torch.cat([x[\"fn\"] for x in outputs])\n        tn = torch.cat([x[\"tn\"] for x in outputs])\n\n        # per image IoU means that we first calculate IoU score for each image\n        # and then compute mean over these scores\n        per_image_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n        per_image_f1 = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n        per_image_acc = smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n        # dataset IoU means that we aggregate intersection and union over whole dataset\n        # and then compute IoU score. The difference between dataset_iou and per_image_iou scores\n        # in this particular case will not be much, however for dataset\n        # with \"empty\" images (images without target class) a large gap could be observed.\n        # Empty images influence a lot on per_image_iou and much less on dataset_iou.\n        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n        dataset_f1 = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro\")\n        dataset_acc = smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"micro\")\n\n        metrics = {\n            f\"{stage}_per_image_iou\": per_image_iou,\n            f\"{stage}_dataset_iou\": dataset_iou,\n            f\"{stage}_per_image_f1\": per_image_f1,\n            f\"{stage}_dataset_f1\": dataset_f1,\n            f\"{stage}_per_image_acc\": per_image_acc,\n            f\"{stage}_dataset_acc\": dataset_acc,\n        }\n\n        self.log_dict(metrics, prog_bar=True, sync_dist=True, rank_zero_only=True)\n\n    def training_step(self, batch: Dict, batch_idx: int):\n        step_outputs = self.shared_step(batch, \"train\")\n        self.train_outputs.append(step_outputs)\n        return step_outputs\n\n    def on_train_epoch_end(self):\n        self.shared_epoch_end(self.train_outputs, \"train\")\n        self.train_outputs = []\n\n    def validation_step(self, batch: Dict, batch_idx: int):\n        step_outputs = self.shared_step(batch, \"valid\")\n        self.val_outputs.append(step_outputs)\n        return step_outputs\n\n    def on_validation_epoch_end(self, *args, **kwargs):\n        self.shared_epoch_end(self.val_outputs, \"valid\")\n        self.val_outputs = []\n\n    def test_step(self, batch: Dict, batch_idx: int):\n        step_outputs = self.shared_step(batch, \"test\")\n        self.test_outputs.append(step_outputs)\n        return step_outputs\n\n    def on_test_epoch_end(self):\n        self.shared_epoch_end(self.test_outputs, \"test\")\n        self.test_outputs = []\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=0.0001)\n\n\ndef colorTransform_VegGround(im,X_true,alpha_vert,alpha_g):\n    alpha = alpha_vert\n    color = [97,65,38]\n    # color = [x / 255 for x in color]\n    image=np.copy(im)\n    for c in range(3):\n        image[:, :, c] =np.where(X_true == 0,image[:, :, c] *(1 - alpha) + alpha * color[c] ,image[:, :, c])\n    alpha = alpha_g\n    color = [34,139,34]\n#    color = [x / 255 for x in color]\n    for c in range(3):\n        image[:, :, c] =np.where(X_true == 1,image[:, :, c] *(1 - alpha) + alpha * color[c] ,image[:, :, c])\n    return image \n\n现在，我们可以使用 VegAnnModel 类初始化一个 U-net 模型。这个模型使用 ResNet34 作为编码器，输入通道数为 3（RGB 图像），输出类别数为 1（二值分割）。\n\n# Initialize the model\nmodel = VegAnnModel(\"Unet\", \"resnet34\", in_channels=3, out_classes=1)\n\n接下来，使用 torchinfo 可视化模型的结构。\n\nfrom torchinfo import summary\n\n# Show detailed model summary using torchinfo\nsummary(model, input_size=(1, 3, 512, 512), \n    col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\"],\n    depth=4)\n\n======================================================================================================================================================\nLayer (type:depth-idx)                             Input Shape               Output Shape              Param #                   Kernel Shape\n======================================================================================================================================================\nVegAnnModel                                        [1, 3, 512, 512]          [1, 1, 512, 512]          --                        --\n├─Unet: 1-1                                        [1, 3, 512, 512]          [1, 1, 512, 512]          --                        --\n│    └─ResNetEncoder: 2-1                          [1, 3, 512, 512]          [1, 3, 512, 512]          --                        --\n│    │    └─Conv2d: 3-1                            [1, 3, 512, 512]          [1, 64, 256, 256]         9,408                     [7, 7]\n│    │    └─BatchNorm2d: 3-2                       [1, 64, 256, 256]         [1, 64, 256, 256]         128                       --\n│    │    └─ReLU: 3-3                              [1, 64, 256, 256]         [1, 64, 256, 256]         --                        --\n│    │    └─MaxPool2d: 3-4                         [1, 64, 256, 256]         [1, 64, 128, 128]         --                        3\n│    │    └─Sequential: 3-5                        [1, 64, 128, 128]         [1, 64, 128, 128]         --                        --\n│    │    │    └─BasicBlock: 4-1                   [1, 64, 128, 128]         [1, 64, 128, 128]         73,984                    --\n│    │    │    └─BasicBlock: 4-2                   [1, 64, 128, 128]         [1, 64, 128, 128]         73,984                    --\n│    │    │    └─BasicBlock: 4-3                   [1, 64, 128, 128]         [1, 64, 128, 128]         73,984                    --\n│    │    └─Sequential: 3-6                        [1, 64, 128, 128]         [1, 128, 64, 64]          --                        --\n│    │    │    └─BasicBlock: 4-4                   [1, 64, 128, 128]         [1, 128, 64, 64]          230,144                   --\n│    │    │    └─BasicBlock: 4-5                   [1, 128, 64, 64]          [1, 128, 64, 64]          295,424                   --\n│    │    │    └─BasicBlock: 4-6                   [1, 128, 64, 64]          [1, 128, 64, 64]          295,424                   --\n│    │    │    └─BasicBlock: 4-7                   [1, 128, 64, 64]          [1, 128, 64, 64]          295,424                   --\n│    │    └─Sequential: 3-7                        [1, 128, 64, 64]          [1, 256, 32, 32]          --                        --\n│    │    │    └─BasicBlock: 4-8                   [1, 128, 64, 64]          [1, 256, 32, 32]          919,040                   --\n│    │    │    └─BasicBlock: 4-9                   [1, 256, 32, 32]          [1, 256, 32, 32]          1,180,672                 --\n│    │    │    └─BasicBlock: 4-10                  [1, 256, 32, 32]          [1, 256, 32, 32]          1,180,672                 --\n│    │    │    └─BasicBlock: 4-11                  [1, 256, 32, 32]          [1, 256, 32, 32]          1,180,672                 --\n│    │    │    └─BasicBlock: 4-12                  [1, 256, 32, 32]          [1, 256, 32, 32]          1,180,672                 --\n│    │    │    └─BasicBlock: 4-13                  [1, 256, 32, 32]          [1, 256, 32, 32]          1,180,672                 --\n│    │    └─Sequential: 3-8                        [1, 256, 32, 32]          [1, 512, 16, 16]          --                        --\n│    │    │    └─BasicBlock: 4-14                  [1, 256, 32, 32]          [1, 512, 16, 16]          3,673,088                 --\n│    │    │    └─BasicBlock: 4-15                  [1, 512, 16, 16]          [1, 512, 16, 16]          4,720,640                 --\n│    │    │    └─BasicBlock: 4-16                  [1, 512, 16, 16]          [1, 512, 16, 16]          4,720,640                 --\n│    └─UnetDecoder: 2-2                            [1, 3, 512, 512]          [1, 16, 512, 512]         --                        --\n│    │    └─Identity: 3-9                          [1, 512, 16, 16]          [1, 512, 16, 16]          --                        --\n│    │    └─ModuleList: 3-10                       --                        --                        --                        --\n│    │    │    └─DecoderBlock: 4-17                [1, 512, 16, 16]          [1, 256, 32, 32]          2,360,320                 --\n│    │    │    └─DecoderBlock: 4-18                [1, 256, 32, 32]          [1, 128, 64, 64]          590,336                   --\n│    │    │    └─DecoderBlock: 4-19                [1, 128, 64, 64]          [1, 64, 128, 128]         147,712                   --\n│    │    │    └─DecoderBlock: 4-20                [1, 64, 128, 128]         [1, 32, 256, 256]         46,208                    --\n│    │    │    └─DecoderBlock: 4-21                [1, 32, 256, 256]         [1, 16, 512, 512]         6,976                     --\n│    └─SegmentationHead: 2-3                       [1, 16, 512, 512]         [1, 1, 512, 512]          --                        --\n│    │    └─Conv2d: 3-11                           [1, 16, 512, 512]         [1, 1, 512, 512]          145                       [3, 3]\n│    │    └─Identity: 3-12                         [1, 1, 512, 512]          [1, 1, 512, 512]          --                        --\n│    │    └─Activation: 3-13                       [1, 1, 512, 512]          [1, 1, 512, 512]          --                        --\n│    │    │    └─Identity: 4-22                    [1, 1, 512, 512]          [1, 1, 512, 512]          --                        --\n======================================================================================================================================================\nTotal params: 24,436,369\nTrainable params: 24,436,369\nNon-trainable params: 0\nTotal mult-adds (G): 31.26\n======================================================================================================================================================\nInput size (MB): 3.15\nForward/backward pass size (MB): 574.62\nParams size (MB): 97.75\nEstimated Total Size (MB): 675.51\n======================================================================================================================================================",
    "crumbs": [
      "计算机视觉分析实验图片",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>训练 SegVeg 模型</span>"
    ]
  },
  {
    "objectID": "segveg-model-training.html#加载数据集",
    "href": "segveg-model-training.html#加载数据集",
    "title": "31  训练 SegVeg 模型",
    "section": "31.3 加载数据集",
    "text": "31.3 加载数据集\n我们需要定义一个自定义的数据集类来处理从 Hugging Face 加载的数据。\n\nfrom src.segveg import VegAnnDataset\nfrom albumentations import Compose, Resize, Normalize, HorizontalFlip, RandomRotate90, ColorJitter, ToFloat\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\n\n# 简化数据增强流程\ntransform = Compose([\n    Resize(512, 512),\n    HorizontalFlip(p=0.5),\n    RandomRotate90(p=0.5),\n    ColorJitter(brightness=0.2, contrast=0.2),\n    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Load the VegAnn dataset\nds = load_dataset(\"simonMadec/VegAnn\", split=\"train\")\n\n# sample small size of data to test\n# ds = ds.shuffle(seed=42).select(range(50))\n\n# 直接使用 datasets 内置的 train_test_split\nsplit_ds = ds.train_test_split(test_size=0.2, seed=42)\n\n# Create custom datasets\ntrain_dataset = VegAnnDataset(split_ds['train'], transform=transform)\nval_dataset = VegAnnDataset(split_ds['test'], transform=transform)\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)",
    "crumbs": [
      "计算机视觉分析实验图片",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>训练 SegVeg 模型</span>"
    ]
  },
  {
    "objectID": "segveg-model-training.html#定义损失器和优化器",
    "href": "segveg-model-training.html#定义损失器和优化器",
    "title": "31  训练 SegVeg 模型",
    "section": "31.4 定义损失器和优化器",
    "text": "31.4 定义损失器和优化器\n通过 Dice loss 函数和 Adam 优化器进行训练。\n\n# 定义损失器和优化器\nfrom torch import nn, optim\n\n# Using Dice loss and Adam optimizer as specified\ncriterion = smp.losses.DiceLoss(mode='binary')\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)",
    "crumbs": [
      "计算机视觉分析实验图片",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>训练 SegVeg 模型</span>"
    ]
  },
  {
    "objectID": "segveg-model-training.html#定义训练循环",
    "href": "segveg-model-training.html#定义训练循环",
    "title": "31  训练 SegVeg 模型",
    "section": "31.5 定义训练循环",
    "text": "31.5 定义训练循环\n下面，我们定义一个训练循环，用于训练 U-net 模型。这个训练循环包括以下几个部分：\n\ntrain_model 函数：定义了训练循环，包括模型训练、验证、保存最佳模型等。\ntrain_model 函数中的 wandb.init：初始化 W&B 项目，用于记录训练过程和结果。\ntrain_model 函数中的 wandb.log：记录训练指标到 W&B。\ntrain_model 函数中的 wandb.save：保存最佳模型到 W&B。\ntrain_model 函数中的 torch.amp.GradScaler：启用混合精度训练和优化配置。\ntrain_model 函数中的 torch.backends.cudnn.benchmark：启用 CuDNN 自动调优。\ntrain_model 函数中的 torch.set_float32_matmul_precision：优化矩阵运算。\n\n\n# 定义训练循环\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=3, device=\"mps\"):\n    # Move model to device\n    device = torch.device(device)\n    model = model.to(device)\n    \n    # 启用混合精度训练和优化配置\n    scaler = torch.amp.GradScaler(enabled=(device.type == \"cuda\"))\n    torch.backends.cudnn.benchmark = (device.type == 'cuda')\n    torch.set_float32_matmul_precision('high')  # 优化矩阵运算\n    import wandb  # 新增导入\n    wandb.init(  # 初始化W&B\n        project=\"veg-segmentation\",\n        config={\n            \"architecture\": \"U-Net\",\n            \"encoder\": \"resnet34\",\n            \"learning_rate\": 0.001,\n            \"batch_size\": 32,\n            \"epochs\": num_epochs\n        }\n    )\n    \n    best_val_loss = float('inf')\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for batch in train_loader:\n            # 显式转移数据到设备并添加内存监控\n            images = batch[\"image\"].to(device, non_blocking=True)\n            masks = batch[\"mask\"].to(device, non_blocking=True)\n            \n            optimizer.zero_grad(set_to_none=True)  # 更高效的梯度清零\n            \n            dtype = torch.float16 if device.type == \"mps\" else torch.float32\n            with torch.amp.autocast(device_type=device.type, dtype=dtype, enabled=(device != \"cpu\")):\n                outputs = model(images)\n                loss = criterion(outputs, masks)\n            \n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            \n            # 记录内存使用情况\n            if device.type == 'cuda':\n                wandb.log({\n                    \"gpu_mem_alloc\": torch.cuda.memory_allocated() / 1e9,\n                    \"gpu_mem_reserved\": torch.cuda.memory_reserved() / 1e9\n                })\n\n            running_loss += loss.item() * images.size(0)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}')\n        \n        # 记录训练指标到W&B\n        wandb.log({\n            \"train_loss\": epoch_loss,\n            \"learning_rate\": scheduler.get_last_lr()[0]\n        })\n\n        # Validation\n        model.eval()\n        running_val_loss = 0.0\n        with torch.no_grad():\n            for batch in val_loader:\n                images = batch[\"image\"].to(device)\n                masks = batch[\"mask\"].to(device)\n\n                # 直接使用模型输出，不假设它有'out'键\n                outputs = model(images)\n                loss = criterion(outputs, masks)\n                running_val_loss += loss.item() * images.size(0)\n\n        val_loss = running_val_loss / len(val_loader.dataset)\n        print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}')\n        \n        # 记录验证指标\n        wandb.log({\"val_loss\": val_loss})\n\n        # Save the best model\n        # 保存最佳模型到W&B\n        if val_loss &lt; best_val_loss:\n            wandb.save('best_model.pth')\n            best_val_loss = val_loss\n            torch.save({\"state_dict\": model.state_dict()}, 'best_model.pth')\n\n        scheduler.step()\n\n    print('Training complete.')",
    "crumbs": [
      "计算机视觉分析实验图片",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>训练 SegVeg 模型</span>"
    ]
  },
  {
    "objectID": "segveg-model-training.html#开始训练",
    "href": "segveg-model-training.html#开始训练",
    "title": "31  训练 SegVeg 模型",
    "section": "31.6 开始训练",
    "text": "31.6 开始训练\n首先，检查设备是否可用，然后运行预先定义好的训练循环。\n\n# check device availability\nif torch.mps.is_available:\n    device = torch.device(\"mps\")\nelif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\n# print device\nprint(f'Found device: {device}')\n\nFound device: mps\n\n\n训练模型。\n\ntrain_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=5, device=device)\n\n训练过程耗时较长，可以在 W&B 项目中查看训练过程和结果。",
    "crumbs": [
      "计算机视觉分析实验图片",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>训练 SegVeg 模型</span>"
    ]
  },
  {
    "objectID": "segveg-model-training.html#保存训练结果",
    "href": "segveg-model-training.html#保存训练结果",
    "title": "31  训练 SegVeg 模型",
    "section": "31.7 保存训练结果",
    "text": "31.7 保存训练结果\n\n# Save the model - 使用与前面一致的格式保存\ntorch.save({\"state_dict\": model.state_dict()}, 'data/vegann/epoch5.ckpt')",
    "crumbs": [
      "计算机视觉分析实验图片",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>训练 SegVeg 模型</span>"
    ]
  },
  {
    "objectID": "collaboration-with-global-developers.html",
    "href": "collaboration-with-global-developers.html",
    "title": "32  与全世界开发者协作",
    "section": "",
    "text": "32.1 仓库（Repository）\n仓库是 GitHub 的核心概念，它用于存储项目的代码、文档和资源。每个仓库都可以公开（Public）或私有（Private），支持 Git 版本控制，使开发者可以追踪代码的变更。\n主要功能：",
    "crumbs": [
      "开发协作",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>与全世界开发者协作</span>"
    ]
  },
  {
    "objectID": "collaboration-with-global-developers.html#仓库repository",
    "href": "collaboration-with-global-developers.html#仓库repository",
    "title": "32  与全世界开发者协作",
    "section": "",
    "text": "分支（Branch）：允许开发者创建多个并行的代码版本。\n标签（Tag）：用于标记特定的版本，例如发布（Release）。\nREADME 文件：通常包含项目简介、安装方法和使用指南。\nLICENSE 文件：定义项目的许可协议。",
    "crumbs": [
      "开发协作",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>与全世界开发者协作</span>"
    ]
  },
  {
    "objectID": "collaboration-with-global-developers.html#组织organization",
    "href": "collaboration-with-global-developers.html#组织organization",
    "title": "32  与全世界开发者协作",
    "section": "32.2 组织（Organization）",
    "text": "32.2 组织（Organization）\nGitHub 允许用户创建组织（Organization）来管理多个仓库和成员。组织适用于团队协作，可设置不同的访问权限，并提供团队管理功能。\n主要功能：\n\n团队（Teams）：可以为不同的成员分配不同的权限。\n权限管理：可设置成员的访问级别，如管理员、写入权限或只读权限。\n企业级功能：支持审计日志、单点登录（SSO）等。",
    "crumbs": [
      "开发协作",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>与全世界开发者协作</span>"
    ]
  },
  {
    "objectID": "collaboration-with-global-developers.html#拉取请求pull-request-pr",
    "href": "collaboration-with-global-developers.html#拉取请求pull-request-pr",
    "title": "32  与全世界开发者协作",
    "section": "32.3 拉取请求（Pull Request, PR）",
    "text": "32.3 拉取请求（Pull Request, PR）\nPull Request（简称 PR）是 GitHub 代码协作的核心功能。开发者可以提交代码变更请求，等待团队成员审核并合并。\n主要流程：\n\n在分支中进行代码修改。\n提交 Pull Request，描述变更内容。\n其他开发者进行代码审核（Review）。\n审核通过后，合并到主分支。\n\nPR 还支持自动检查，如代码风格检查、测试运行等。",
    "crumbs": [
      "开发协作",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>与全世界开发者协作</span>"
    ]
  },
  {
    "objectID": "collaboration-with-global-developers.html#问题issue",
    "href": "collaboration-with-global-developers.html#问题issue",
    "title": "32  与全世界开发者协作",
    "section": "32.4 问题（Issue）",
    "text": "32.4 问题（Issue）\nIssue 用于跟踪任务、Bug 以及改进建议。它可以与 PR 关联，帮助开发者高效管理项目。\n主要功能：\n\n标签（Labels）：分类 Issue（如 bug、enhancement）。\n指派（Assignees）：分配 Issue 负责人。\n里程碑（Milestones）：跟踪项目进度。\n讨论（Discussion）：团队可以在 Issue 中讨论问题。\n\n\n\n\n\n\n\nNote\n\n\n\n如何在 GitHub Issue 上提问\n在 GitHub Issue 上提问时，确保问题清晰、可复现，并提供足够的信息，以便维护者或社区成员快速理解和解决。以下是一个高质量 Issue 的关键要素：\n1. 写一个清晰的标题\n标题应该简洁明了，概括问题的核心。例如：\n\n✅ “安装后运行时报错：ModuleNotFoundError”\n❌ “求助！！！出错了”\n\n2. 说明问题\n在 Issue 描述中，回答以下问题：\n\n发生了什么？（简要描述问题）\n你期望的行为是什么？\n问题如何复现？（提供最小可复现步骤）\n是否有错误日志？（截图或代码块格式化错误信息）\n\n示例：\n## 问题描述\n\n在运行 `my_script.py` 时，程序报错 `ModuleNotFoundError: No module named 'requests'`。\n\n## 复现步骤\n\n1. 安装项目 `git clone https://github.com/example/repo.git`\n2. 运行 `python my_script.py`\n3. 看到如下错误：\n   ```\n   Traceback (most recent call last):\n     File \"my_script.py\", line 1, in &lt;module&gt;\n       import requests\n   ModuleNotFoundError: No module named 'requests'\n   ```\n3. 提供环境信息\n不同环境可能导致不同的 Bug，所以提供你的系统和依赖信息：\n- 操作系统: macOS 14.2 / Ubuntu 22.04 / Windows 11\n- Python 版本: 3.9.6\n- 依赖项: `pip freeze | grep requests` 结果为空\n- 相关软件或工具版本：Git 2.34.1，Node.js 18.16.0\n4. 附加上下文\n\n你是否尝试过解决？哪些方法不起作用？\n你是否查阅了 README、Wiki、FAQ 或已有 Issue？\n是否有截图、GIF 或代码片段可以辅助理解？\n\n5. 避免无效 Issue\n\n避免模糊的描述：“软件坏了” → 具体描述问题。\n避免催促：“快点帮忙！” → 保持礼貌，维护者通常是志愿者。\n避免重复 Issue：先搜索是否已有类似问题。\n\n一个格式良好的 Issue，不仅能让维护者快速响应，还能帮助其他开发者遇到相同问题时更快找到解决方案。",
    "crumbs": [
      "开发协作",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>与全世界开发者协作</span>"
    ]
  },
  {
    "objectID": "collaboration-with-global-developers.html#代码审查review",
    "href": "collaboration-with-global-developers.html#代码审查review",
    "title": "32  与全世界开发者协作",
    "section": "32.5 代码审查（Review）",
    "text": "32.5 代码审查（Review）\nGitHub 提供代码审查功能，开发者可以对 PR 进行评论和建议，提高代码质量。\n主要功能：\n\n批准（Approve）：表示代码可以合并。\n请求更改（Request Changes）：开发者需要修改代码。\n评论（Comment）：对特定代码行提出建议。",
    "crumbs": [
      "开发协作",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>与全世界开发者协作</span>"
    ]
  },
  {
    "objectID": "collaboration-with-global-developers.html#github-actions",
    "href": "collaboration-with-global-developers.html#github-actions",
    "title": "32  与全世界开发者协作",
    "section": "32.6 GitHub Actions",
    "text": "32.6 GitHub Actions\nGitHub Actions 是 GitHub 提供的 CI/CD（持续集成/持续部署）工具。开发者可以通过 YAML 文件定义自动化任务，如测试、构建、部署等。\n主要功能：\n\n工作流（Workflow）：定义自动化流程。\n作业（Job）：工作流中的任务。\n步骤（Step）：具体执行的操作，如运行脚本。\n触发器（Trigger）：可以在 Push、PR、Issue 变更时触发工作流。\n\n示例 YAML 配置：\nname: CI\non: [push, pull_request]\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Run tests\n        run: make test\n\n\n\n\n\n\nNote\n\n\n\nCI（Continuous Integration，持续集成）是一种软件开发实践，开发者会频繁地将代码变更集成到主代码库中，并通过自动化构建和测试来验证代码的正确性。CI 的主要目标是尽早发现问题，提高代码质量，减少集成冲突。\n在 GitHub 中，CI 主要通过 GitHub Actions 或 第三方 CI 工具（如 Jenkins、Travis CI、CircleCI） 来实现，通常包括以下步骤：\n\n代码提交（Commit & Push）：开发者提交代码到远程仓库。\n触发 CI 流程：CI 工具检测到代码变更后，自动执行预定义的任务，如编译、测试等。\n运行自动化测试：执行单元测试、集成测试，确保代码正确。\n构建和部署（可选）：如果测试通过，可以自动部署应用到测试环境或生产环境。\n\n你在 GitHub 相关章节中提到的 GitHub Actions 就是 GitHub 提供的 CI/CD 解决方案，可以用 YAML 文件定义自动化流程。",
    "crumbs": [
      "开发协作",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>与全世界开发者协作</span>"
    ]
  },
  {
    "objectID": "collaboration-with-global-developers.html#github-pages",
    "href": "collaboration-with-global-developers.html#github-pages",
    "title": "32  与全世界开发者协作",
    "section": "32.7 GitHub Pages",
    "text": "32.7 GitHub Pages\nGitHub Pages 是 GitHub 提供的静态网站托管服务，适用于个人博客、软件文档等。\n主要功能：\n\n自动部署：支持从 main 或 gh-pages 分支自动构建网站。\nJekyll 支持：可使用 Jekyll 生成静态网站。\n自定义域名：支持绑定自定义域名。\n\n启用 GitHub Pages 的方法：\n\n在仓库的 Settings &gt; Pages 选项中配置发布源。\n选择 main 分支或 docs 目录作为站点源。\n访问 https://username.github.io/repository 预览。\n\n\n\n\n\n\n\nNote\n\n\n\n要将 GitHub Pages (gh-pages 分支) 发布到 Netlify，可以按照以下步骤进行：\n\n登录 Netlify：访问 Netlify 并使用 GitHub 账号登录。\n创建新站点：在 Netlify 仪表盘点击 “Add new site” → “Import from GitHub”。\n选择仓库：授权 Netlify 访问 GitHub，然后选择你的项目仓库。\n配置构建：\n\nBranch：选择 gh-pages 分支（如果是其他分支，手动修改）。\nBuild command：通常 gh-pages 分支已包含静态文件，留空即可。\nPublish directory：填写 . 或 _site（如果使用 Jekyll/Hugo）。\n\n部署：点击 “Deploy site”，Netlify 会自动拉取 gh-pages 分支内容并发布。",
    "crumbs": [
      "开发协作",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>与全世界开发者协作</span>"
    ]
  },
  {
    "objectID": "collaboration-with-global-developers.html#结论",
    "href": "collaboration-with-global-developers.html#结论",
    "title": "32  与全世界开发者协作",
    "section": "32.8 结论",
    "text": "32.8 结论\nGitHub 提供了丰富的功能，涵盖代码托管、协作、自动化和网站托管等方面。掌握这些功能可以帮助开发者更高效地管理项目，提高团队协作效率。",
    "crumbs": [
      "开发协作",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>与全世界开发者协作</span>"
    ]
  },
  {
    "objectID": "r-package-development.html",
    "href": "r-package-development.html",
    "title": "33  开发 R 包",
    "section": "",
    "text": "33.1 开发者档案\n开发 R 包的第一步，不妨建立一个开发者档案。最佳的地方便是 GitHub 了。GitHub 是世界上最大的代码托管平台，也是开发者们展示自己才华的舞台。\n我的 GitHub 主页是：https://github.com/gaospecial。从这里可以看到我的所有项目，包括 R 包。GitHub 会记录你所有的活动，包括提交代码、创建 issue、PR 等。并根据这些记录为你的水平打分。主页上出现一个“A”，说明你已经是一个合格的开发者了（Figure 33.1）。\nknitr::include_graphics(\"https://vnote-1251564393.cos.ap-chengdu.myqcloud.com/20231020095321.png\")\n\n\n\n\n\n\n\nFigure 33.1\n在这个项目中，我会以自己开发的 ggVennDiagram 包(Gao and Dusa 2024)为例，介绍开发一个 R 包的始末。",
    "crumbs": [
      "开发协作",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>开发 R 包</span>"
    ]
  },
  {
    "objectID": "r-package-development.html#需求的产生",
    "href": "r-package-development.html#需求的产生",
    "title": "33  开发 R 包",
    "section": "33.2 需求的产生",
    "text": "33.2 需求的产生\n正如这个包的名字一样，它最初的需求是画一个韦恩图。并基于图形语法在韦恩图的不同区域填充上不同的颜色，以表示每个子集中对象的多少。\n我们知道，韦恩图是用来表示集合之间关系的图。它最初是由数学家约翰·韦恩（John Venn）在 1881 年发明的。作为一种非常基础的图表，它在生物信息学、统计学、计算机科学等领域有着广泛的应用。然而，在 ggVennDiagram 包之前，对于绘制韦恩图，现有 R 包要么功能有限，要么使用不便。\n\n33.2.1 现有的工具\n当时可用的绘制韦恩图的工具有下列这些：\n\nVennDiagram\ncolorfulVennPlot\nvenn\nnVennR\neulerr\nvenneuler\ngplots\nggVennDiagram\nggvenn\n\n在开发之前，需要进行系统的调研。当时，我还专门写了一个文档比较这些包的优缺点（https://venn.bio-spring.top/）。\n\n# 安装这些包\ncran_packages &lt;- c(\"VennDiagram\",\"gplots\",\"venn\",\n                    \"eulerr\",\"venneuler\")\nxfun::pkg_load2(cran_packages)\n\n# GitHub\nif (!requireNamespace(\"ggvenn\")){\n  remotes::install_github(\"yanlinlin82/ggvenn\")\n}\n\n假设我们有一个基因列表，现在分别使用上面这些软件包绘制韦恩图。\n\ngenes &lt;- paste0(\"gene\",1:1000)\nset.seed(20210302)\ngene_list &lt;- list(A = sample(genes,100),\n                  B = sample(genes,200),\n                  C = sample(genes,300),\n                  D = sample(genes,200))\nlibrary(dplyr)\n\n\n33.2.1.1 VennDiagram\nVennDiagram (Chen 2022) 是当时最流行的韦恩图绘制工具。\n\nlibrary(VennDiagram)\nVennDiagram &lt;- venn.diagram(x = gene_list, filename = NULL)\ncowplot::plot_grid(VennDiagram)\n\n\n\n\n\n\n\n\n它提供了基本的函数来绘制欧拉图。\n\nvenn.plot &lt;- draw.triple.venn(\n    area1 = 65,\n    area2 = 75,\n    area3 = 85,\n    n12 = 35,\n    n23 = 15,\n    n13 = 25,\n    n123 = 5,\n    category = c(\"First\", \"Second\", \"Third\"),\n    fill = c(\"blue\", \"red\", \"green\"),\n    lty = \"blank\",\n    cex = 2,\n    cat.cex = 2,\n    cat.col = c(\"blue\", \"red\", \"green\"),\n    euler.d = TRUE\n    )\n\n\n\n\n\n\n\ncowplot::plot_grid(venn.plot)\n\n\n\n\n\n\n\n\n它还提供了一些辅助函数来获取韦恩图的分割。\n\nget.venn.partitions(gene_list) %&gt;% dplyr::as_tibble()\n\n# A tibble: 15 × 7\n   A     B     C     D     ..set..     ..values..   ..count..\n   &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;chr&gt;       &lt;named list&gt;     &lt;int&gt;\n 1 TRUE  TRUE  TRUE  TRUE  A∩B∩C∩D     &lt;chr [1]&gt;            1\n 2 FALSE TRUE  TRUE  TRUE  (B∩C∩D)∖(A) &lt;chr [7]&gt;            7\n 3 TRUE  FALSE TRUE  TRUE  (A∩C∩D)∖(B) &lt;chr [2]&gt;            2\n 4 FALSE FALSE TRUE  TRUE  (C∩D)∖(A∪B) &lt;chr [41]&gt;          41\n 5 TRUE  TRUE  FALSE TRUE  (A∩B∩D)∖(C) &lt;chr [5]&gt;            5\n 6 FALSE TRUE  FALSE TRUE  (B∩D)∖(A∪C) &lt;chr [25]&gt;          25\n 7 TRUE  FALSE FALSE TRUE  (A∩D)∖(B∪C) &lt;chr [12]&gt;          12\n 8 FALSE FALSE FALSE TRUE  (D)∖(A∪B∪C) &lt;chr [107]&gt;        107\n 9 TRUE  TRUE  TRUE  FALSE (A∩B∩C)∖(D) &lt;chr [5]&gt;            5\n10 FALSE TRUE  TRUE  FALSE (B∩C)∖(A∪D) &lt;chr [48]&gt;          48\n11 TRUE  FALSE TRUE  FALSE (A∩C)∖(B∪D) &lt;chr [22]&gt;          22\n12 FALSE FALSE TRUE  FALSE (C)∖(A∪B∪D) &lt;chr [174]&gt;        174\n13 TRUE  TRUE  FALSE FALSE (A∩B)∖(C∪D) &lt;chr [10]&gt;          10\n14 FALSE TRUE  FALSE FALSE (B)∖(A∪C∪D) &lt;chr [99]&gt;          99\n15 TRUE  FALSE FALSE FALSE (A)∖(B∪C∪D) &lt;chr [43]&gt;          43\n\n\n\n\n33.2.1.2 colorfulVennPlot\ncolorfulVennPlot 这个包可以指定每个区域的填充颜色，但第一个必需的参数是一个长度为 15 的数字向量，其中特定名称按适当顺序排列，这使得设置和使用非常复杂。\n此外，填充颜色需要一个一个地指定，这也很复杂。\n\nlibrary(colorfulVennPlot)\nColors &lt;- c('red', 'yellow', 'green', 'pink', 'darkgreen','blue','lightblue','tan', \n  'yellowgreen','orange','purple','white','grey','plum','brown')\nregions &lt;- seq(15)\nnames(regions) &lt;- c('1000', '0100', '1100', '0010', '1010', '0110', '1110', '0001', \n  '1001', '0101', '1101', '0011', '1011', '0111', '1111')\nplotVenn4d(regions, Colors=Colors)\n\n\n\n\n\n\n\nTip\n\n\n\ncolorfulVennPlot 这个包现在已经无法安装和使用了。其绘图效果参见 https://venn.bio-spring.top/。\n\n\n\n\n33.2.1.3 venn\nvenn 这个包(Dusa 2024)使用起来非常简单。\n\nlibrary(venn)\nvenn(gene_list)\n\n\n\n\n\n\n\n\nvenn 也支持不同的形状。\n\nvenn(x = gene_list, ellipse = TRUE)\n\n\n\n\n\n\n\n\nvenn 支持 ggplot，但依赖于 ggpolypath，这是一个不流行的包。\n不过，venn 有自己的特别之处，即它可以绘制多达 7 个集合的韦恩图。\n\nvenn::venn(5)\n\n\n\n\n\n\n\n\n\nvenn::venn(6)\n\n\n\n\n\n\n\n\n\nvenn::venn(7)\n\n\n\n\n\n\n\n\n除此之外，venn 接受多种格式的输入。这在基于逻辑关系绘制韦恩图时非常有用。\n\nintersections &lt;- c('~A~B~C~D', '~A~B~CD', '~A~BC~D', '~A~BCD', '~AB~C~D', '~AB~CD', '~ABC~D', '~ABCD', 'A~B~C~D', 'A~B~CD', 'A~BC~D', 'A~BCD', 'AB~C~D', 'AB~CD', 'ABC~D', 'ABCD')\nvenn(intersections, zcol = colors()[sample(1:657, 16)])\n\n\n\n\n\n\n\nvenn(\"A*D, A*B*~C + B*C*~D\", zcol = c(\"blue\", \"red\"))\n\n\n\n\n\n\n\nvenn(\"AD, AB~C + BC~D\", zcol = c(\"blue\", \"red\"))\n\nvenn(\"1-----\")\n\n\n\n\n\n\n\nvenn(\"100 + 110 + 101 + 111\")\n\n\n\n\n\n\n\n\n\n\n33.2.1.4 eulerr\neulerr (Larsson 2024)生成面积比例的欧拉图，显示集合之间的关系（交集、并集和不相交）。\n不同于韦恩图，欧拉图只显示相关的集合关系。\n韦恩图是欧拉图的更严格的版本。一个韦恩图必须包含所有 \\(2^n\\) 逻辑上可能的区域重叠，表示其组成部分集合的所有组合。\n\nlibrary(eulerr)\nvenn_plot &lt;- venn(gene_list)\nplot(venn_plot)\n\n\n\n\n\n\n\n\n\neuler_plot &lt;- euler(gene_list)\nplot(euler_plot)\n\n\n\n\n\n\n\n\n在 2021 年 3 月 9 日，可以向 eulerr 提供以下输入：\n\n一个命名数字向量，其中集合组合作为不相交的集合组合或并集（取决于 euler() 中的参数类型），\n一个矩阵或数据框，其中列表示集合，行表示每个观察的集合关系，\n一个样本空间列表，或\n一个表。\n\n\n# 作为逻辑矩阵输入\nset.seed(1)\nmat &lt;- cbind(\n  A = sample(c(TRUE, TRUE, FALSE), 50, TRUE),\n  B = sample(c(TRUE, FALSE), 50, TRUE),\n  C = sample(c(TRUE, FALSE, FALSE, FALSE), 50, TRUE)\n)\n(fit2 &lt;- euler(mat))\n\n      original fitted residuals regionError\nA           13     13         0       0.008\nB            4      4         0       0.002\nC            0      0         0       0.000\nA&B         17     17         0       0.010\nA&C          5      5         0       0.003\nB&C          1      0         1       0.024\nA&B&C        2      2         0       0.001\n\ndiagError: 0.024 \nstress:    0.002 \n\n\n诊断函数很好。\n\nerror_plot(fit2)\n\n\n\n\n\n\n\n\n\n\n33.2.1.5 venneuler\nvenneuler (Wilkinson 2024) 依赖于 JAVA 运行环境，这使得它的安装对于非 JAVA 用户来说非常痛苦。\n除此之外，它只支持列表输入。\n\nlibrary(venneuler)\ncombinations &lt;- c(A=0.3, B=0.3, C=1.1, \"A&B\"=0.1, \"A&C\"=0.2, \"B&C\"=0.1 ,\"A&B&C\"=0.1,\"D\"=0.2,\"C&D\"=0.1)\nvd &lt;- venneuler(combinations)\nplot(vd)\n\n\n\n33.2.1.6 RVenn\nRVenn (Akyol 2019) 是一个处理多个集合的包，而基础 R 函数（intersect、union 和 setdiff）只支持两个集合。\n该函数 overlap、unite 和 discern 抽象了细节，因此只需构造宇宙并选择要操作的集合即可通过索引或集合名称进行操作。\nRVenn 提供了一个 ggvenn() 函数来绘制 2-3 维的韦恩图。在这种情况下，它的优势主要依赖于集合操作函数。\n\nlibrary(RVenn)\nRVenn::ggvenn(Venn(gene_list[1:3]))\n\n\n\n\n\n\n\n\n\n\n33.2.1.7 gplots\ngplots (Warnes et al. 2024) 提供了各种 R 编程工具来绘制数据。它支持多达五个集合的韦恩图。\n\nlibrary(gplots)\nvenn(gene_list)\n\n\n\n\n\n\n\n\n\n\n33.2.1.8 ggvenn\n在当时，还有一个正在开发中的 ggvenn (Yan 2025)包，它可以显示多边形区域中的元素，比较好的支持图形语法。\n\nlibrary(ggvenn)\nggvenn::ggvenn(gene_list)\n\n\n\n\n\n\n\n\nggvenn 可以显示多边形区域中的元素。\n\na &lt;- list(A = c(\"apple\", \"pear\", \"peach\"),\n          B = c(\"apple\", \"lemon\"))\nggvenn::ggvenn(a, show_elements = TRUE)\n\n\n\n\n\n\n\nggvenn::ggvenn(a, show_elements = TRUE, label_sep = \"\\n\")  # show elements in line\n\n\n\n\n\n\n\n\n最初我并没有想开发一个包，只是想画一个韦恩图。而当发现所有的包都不能满足我的需求时，我决定自己动手，丰衣足食。",
    "crumbs": [
      "开发协作",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>开发 R 包</span>"
    ]
  },
  {
    "objectID": "r-package-development.html#任务的分解",
    "href": "r-package-development.html#任务的分解",
    "title": "33  开发 R 包",
    "section": "33.3 任务的分解",
    "text": "33.3 任务的分解\n韦恩图是一种几何图形，它只有简单的几类元素组成，分别是集合、子集以及标注。在数据层面，每个集合都 包含多个元素，集合间的元素可以相同或者不同；在图形上，它们可以通过线条、区域、文本等表示。因此，要画出它，需要做好以下几件事（以 4 个集合为例）：\n\n生成 4 sets椭圆\n获取元素的子集\n获取图形的子集\n计算中心点坐标，这是为了将标签添加到图形的中心位置\n添加文字标注\n\n\n33.3.1 生成 4 sets椭圆\n椭圆的生成可以借助简单的几何函数。在 R 中，可以使用 plot 结合 cos 和 sin 函数绘制椭圆。例如：\n\n# 椭圆参数\na = 5  # 长轴\nb = 3  # 短轴\ntheta = seq(0, 2 * pi, length.out = 100)\n\n# 计算椭圆上的点\nx = a * cos(theta)\ny = b * sin(theta)\n\n# 绘制椭圆\nplot(x, y, type = \"l\", asp = 1, xlab = \"X\", ylab = \"Y\", main = \"椭圆\")\n\n\n\n\n\n\n\n\n如果需要旋转椭圆，可以添加旋转矩阵：\n\nangle = pi / 6  # 旋转角度\nx_rot = x * cos(angle) - y * sin(angle)\ny_rot = x * sin(angle) + y * cos(angle)\n\nplot(x_rot, y_rot, type = \"l\", asp = 1, xlab = \"X\", ylab = \"Y\", main = \"旋转后的椭圆\")\n\n\n\n\n\n\n\n\n这样就能生成并绘制不同方向的椭圆。为了让 4 个椭圆的位置和大小合适，我们以 VennDiagram 包中的 venn.diagram 函数生成的椭圆为参考，并使用 ggplot2 包中的 geom_polygon 函数绘制椭圆。\nVennDiagram 使用 ell2poly 函数来画 4 个椭圆，我把其中的参数拿出来，生成了椭圆的数据，并使用 ggplot 画了出来。\n\nlibrary(VennDiagram)\nlibrary(tidyverse)\nlibrary(cowplot)\nsets &lt;- 4\nellipse_4d_parameters &lt;- list(c(0.65, 0.47, 0.35, 0.20,  45),\n                              c(0.35, 0.47, 0.35, 0.20, 135),\n                              c(0.50, 0.57, 0.33, 0.15,  45),\n                              c(0.50, 0.57, 0.35, 0.15, 135))\nellipse_4d_coordinations &lt;- lapply(1:sets,function(i){\n  x &lt;- ellipse_4d_parameters[[i]]\n  do.call(\"ell2poly\",as.list(c(x,n.sides=3000))) %&gt;% \n    data.frame() |&gt; \n    mutate(x = round(x,4),\n           y = round(y,4))\n})\n\n使用ggplot2的geom_polyon()函数画图。\n\ndf &lt;- bind_rows(ellipse_4d_coordinations, .id = \"group\")\nggplot(df,aes(x,y)) + geom_polygon(aes(group=group)) + coord_equal()\n\n\n\n\n\n\n\n\n可以使用其它映射改变填充色，线条属性等。\n\nggplot(df,aes(x,y)) +\n  geom_polygon(aes(group=group),color=\"grey\",fill=rep(1:4,each=3001),alpha=1/4,size=2,lty=\"dashed\") + \n  # geom_text(aes(label=group),check_overlap = T) +  \n  coord_equal()\n\n\n\n\n\n\n\n\n那么，现在要解决的只剩下一个问题：确定每个 Venn 图区域的坐标，这样调用 ggplot2 的 geom_polygon() 函数画出图片就可以了。\n\n\n33.3.2 获取元素的子集\n在数据层面，每个集合都 包含多个元素，集合间的元素可以相同或者不同。R 中提供了 union()、intersect()、setdiff() 等函数来进行集合计算，借助这些函数可以获取元素的子集。\n\nA &lt;- c(\"a\", \"b\", \"c\")\nB &lt;- c(\"b\", \"c\", \"d\")\nunion(A,B)\n\n[1] \"a\" \"b\" \"c\" \"d\"\n\nintersect(A,B)\n\n[1] \"b\" \"c\"\n\nsetdiff(A,B)\n\n[1] \"a\"\n\n\n\n\n33.3.3 获取图形的子集\n在图形层面，R 中提供了 st_union()、st_intersect()、st_difference() 等函数来进行集合计算，借助这些函数可以获取图形的子集Figure 33.2。\n\n# 将 4 个椭圆转变为 sf 对象\nlibrary(sf)\npolygons = lapply(ellipse_4d_coordinations,function(x)st_polygon(list(as.matrix(x))))\n\n# 计算交集\npar(mfrow=c(2,2))\nA &lt;- st_difference(st_difference(st_difference(polygons[[1]],polygons[[2]]),polygons[[3]]),polygons[[4]])\nplot(A,main=\"A\")\n\nABCD &lt;- st_intersection(st_intersection(st_intersection(polygons[[1]],polygons[[2]]),polygons[[3]]),polygons[[4]])\nplot(ABCD,main=\"ABCD\")\n\nABC &lt;- st_difference(st_intersection(st_intersection(polygons[[1]],polygons[[2]]),polygons[[3]]),polygons[[4]])\nplot(ABC,main = \"ABC\")\n\nAB &lt;- st_difference(st_intersection(polygons[[1]],polygons[[2]]),st_union(polygons[[3]],polygons[[4]]))\nplot(AB, main = \"AB\")\n\n\n\n\n\n\n\nFigure 33.2: 4 个椭圆转变为 sf 对象，可以对图形进行集合操作。\n\n\n\n\n\n\n\n33.3.4 计算中心点坐标\n为了让文字标注到图形的中心位置，需要计算每个图形的中心点坐标。这个过程可以借助 sf 包中的 st_centroid() 函数来完成。\n\nlibrary(sf)\nlibrary(ggplot2)\n\nnc &lt;- st_read(system.file('shape/nc.shp', package = \"sf\"), quiet = TRUE)\n\n# using sf\nsf_cent &lt;- st_centroid(nc)\n\n# plot both together to confirm that they are equivalent\nggplot() + \n  geom_sf(data = nc, fill = 'white') +\n  geom_sf(data = sf_cent, color = 'red')\n\n\n\n\n\n\n\n\n\n\n33.3.5 添加文字\n首先，获取所有图形子集。\n\nlibrary(sf)\nA &lt;- st_difference(st_difference(st_difference(polygons[[1]],polygons[[2]]),polygons[[3]]),polygons[[4]])\nB &lt;- st_difference(st_difference(st_difference(polygons[[2]],polygons[[1]]),polygons[[3]]),polygons[[4]])\nC &lt;- st_difference(st_difference(st_difference(polygons[[3]],polygons[[1]]),polygons[[2]]),polygons[[4]])\nD &lt;- st_difference(st_difference(st_difference(polygons[[4]],polygons[[1]]),polygons[[3]]),polygons[[2]])\nAB &lt;- st_difference(st_intersection(polygons[[1]],polygons[[2]]),st_union(polygons[[3]],polygons[[4]]))\nAC &lt;- st_difference(st_intersection(polygons[[1]],polygons[[3]]),st_union(polygons[[2]],polygons[[4]]))\nAD &lt;- st_difference(st_intersection(polygons[[1]],polygons[[4]]),st_union(polygons[[3]],polygons[[2]]))\nBC &lt;- st_difference(st_intersection(polygons[[3]],polygons[[2]]),st_union(polygons[[1]],polygons[[4]]))\nBD &lt;- st_difference(st_intersection(polygons[[4]],polygons[[2]]),st_union(polygons[[3]],polygons[[1]]))\nCD &lt;- st_difference(st_intersection(polygons[[3]],polygons[[4]]),st_union(polygons[[1]],polygons[[2]]))\nABC &lt;- st_difference(st_intersection(st_intersection(polygons[[1]],polygons[[2]]),polygons[[3]]),polygons[[4]])\nABD &lt;- st_difference(st_intersection(st_intersection(polygons[[1]],polygons[[2]]),polygons[[4]]),polygons[[3]])\nACD &lt;- st_difference(st_intersection(st_intersection(polygons[[1]],polygons[[4]]),polygons[[3]]),polygons[[2]])\nBCD &lt;- st_difference(st_intersection(st_intersection(polygons[[4]],polygons[[2]]),polygons[[3]]),polygons[[1]])\nABCD &lt;- st_intersection(st_intersection(st_intersection(polygons[[1]],polygons[[2]]),polygons[[3]]),polygons[[4]])\n\n然后，将所有图形子集放到一个列表中。计算多边形的参数。\n\nggpolygons &lt;- list(A=A,B=B,C=C,D=D,AB=AB,AC=AC,AD=AD,BC=BC,BD=BD,CD=CD,ABC=ABC,ABD=ABD,ACD=ACD,BCD=BCD,ABCD=ABCD)\npolygon_names &lt;- names(ggpolygons)\nggpolygons_df &lt;- lapply(1:length(ggpolygons), function(i){\n  df &lt;- unlist(ggpolygons[[i]]) %&gt;% matrix(ncol = 2) %&gt;% data.frame()\n  colnames(df) &lt;- c(\"x\",\"y\")\n  df$group &lt;- polygon_names[[i]]\n  return(df)\n})\ndata_ploygons &lt;- do.call(rbind,ggpolygons_df)\n\n计算多边形中心点。\n\ncenter_df &lt;- lapply(ggpolygons, st_centroid) %&gt;% unlist %&gt;% matrix(byrow = T,ncol=2) %&gt;% data.frame()\ncenter_df$group &lt;- polygon_names\ncolnames(center_df) &lt;- c(\"x\",\"y\",\"group\")\ndata_centers &lt;- center_df\n\n在正确的位置添加文字，查看一下结果：\n\nggplot(data_ploygons,aes(x,y,fill=group)) +\n  geom_polygon(show.legend = F) +\n  geom_text(aes(label=group),data=data_centers) +\n  coord_equal() +\n  theme_void()",
    "crumbs": [
      "开发协作",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>开发 R 包</span>"
    ]
  },
  {
    "objectID": "r-package-development.html#功能的取舍",
    "href": "r-package-development.html#功能的取舍",
    "title": "33  开发 R 包",
    "section": "33.4 功能的取舍",
    "text": "33.4 功能的取舍\n开发一个画 Venn 图的包，它可以：\n\n[√] 接受传统上的输出，输出优于 VennDiagram 和 gplots 的 Venn 图;\n[√] 颜色/透明度/线型/粗细等各种自定义\n[×] 是否固定比例：参数 scaled = True。\n[√] 随意显示标签(label 和 text)\n[×] 参数：circle 还是 ellipse？（circle在4d venn中并不能显示所有组合）\n[√] 参数：color是不同色块还是按梯度填充？\n[×] 接受直接的 data.frame 或者 matrix 输出, 通过指定 Threshold 自行创建存在矩阵, 按列分组, 按行计数;\n[√] 也可以接受 list 输入；\n[×] 如果分组过多(&gt;5), 则使用 upsetR 绘制多种组合条件下的无限 Venn 图.\n[√] Venn 图内每个空间内再分别上色.",
    "crumbs": [
      "开发协作",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>开发 R 包</span>"
    ]
  },
  {
    "objectID": "r-package-development.html#算法的优化",
    "href": "r-package-development.html#算法的优化",
    "title": "33  开发 R 包",
    "section": "33.5 算法的优化",
    "text": "33.5 算法的优化\n\n33.5.1 计算多边形的参数\n\n33.5.1.1 原始版本\nlibrary(sf)\nA &lt;- st_difference(st_difference(st_difference(polygons[[1]],polygons[[2]]),polygons[[3]]),polygons[[4]])\nB &lt;- st_difference(st_difference(st_difference(polygons[[2]],polygons[[1]]),polygons[[3]]),polygons[[4]])\nC &lt;- st_difference(st_difference(st_difference(polygons[[3]],polygons[[1]]),polygons[[2]]),polygons[[4]])\nD &lt;- st_difference(st_difference(st_difference(polygons[[4]],polygons[[1]]),polygons[[3]]),polygons[[2]])\n# ...\n\n\n33.5.1.2 现代版本\nfunction(venn, slice = \"all\"){\n  overlap = overlap(venn, slice = slice)\n  if (slice[1] == \"all\" | identical(venn@sets[slice], venn@sets)){\n    discern = NULL\n    return(overlap)\n  } else {\n    discern = discern(venn, slice1 = slice)\n    return(sf::st_intersection(overlap, discern))\n  }",
    "crumbs": [
      "开发协作",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>开发 R 包</span>"
    ]
  },
  {
    "objectID": "r-package-development.html#问题的解决",
    "href": "r-package-development.html#问题的解决",
    "title": "33  开发 R 包",
    "section": "33.6 问题的解决",
    "text": "33.6 问题的解决\n\n准备示例数据\n\n\nlibrary(ggVennDiagram)\ngenes &lt;- paste(\"gene\",1:1000,sep=\"\")\nset.seed(20210419)\nx &lt;- list(A=sample(genes,300),\n          B=sample(genes,525),\n          C=sample(genes,440),\n          D=sample(genes,350))\n\n\n绘制韦恩图\n\n\nlibrary(ggplot2)\nggVennDiagram(x) + scale_fill_gradient(low=\"white\",high = \"red\")\n\n\n\n\n\n\n\n\n\n更多维的数据\n\n\nx &lt;- list(A=sample(genes,300),\n          B=sample(genes,525),\n          C=sample(genes,440),\n          D=sample(genes,350),\n          E=sample(genes,200),\n          F=sample(genes,150),\n          G=sample(genes,100))\n\n#  七维\np7 = ggVennDiagram(x, label = \"none\", edge_size = 2) + \n  scale_fill_distiller(palette = \"RdBu\") +\n  theme(legend.position = \"none\")\n\n#   六维\np6 = ggVennDiagram(x[1:6], label = \"none\", edge_size = 2) +\n  scale_fill_distiller(palette = \"RdBu\") +\n  theme(legend.position = \"none\")\n\n#   五维\np5 = ggVennDiagram(x[1:5], label = \"none\", edge_size = 2) +\n  scale_fill_distiller(palette = \"RdBu\") +\n  theme(legend.position = \"none\")\n\n#   四维\np4 = ggVennDiagram(x[1:4], label = \"none\", edge_size = 2) +\n  scale_fill_distiller(palette = \"RdBu\") +\n  theme(legend.position = \"none\")\n\n#   三维\np3 = ggVennDiagram(x[1:3], label = \"none\", edge_size = 2) +\n  scale_fill_distiller(palette = \"RdBu\") +\n  theme(legend.position = \"none\")\n\n#   二维\np2 = ggVennDiagram(x[1:2], label = \"none\", edge_size = 2) +\n  scale_fill_distiller(palette = \"RdBu\") +\n  theme(legend.position = \"none\")\n\ncowplot::plot_grid(p7, p6, p5, p4, p3, p2, ncol = 3, labels = \"AUTO\")",
    "crumbs": [
      "开发协作",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>开发 R 包</span>"
    ]
  },
  {
    "objectID": "r-package-development.html#r-包开发者工具",
    "href": "r-package-development.html#r-包开发者工具",
    "title": "33  开发 R 包",
    "section": "33.7 R 包开发者工具",
    "text": "33.7 R 包开发者工具\n为了简化 R 包的开发，社区开发了多个工具，包括：\n\ndevtools：开发 R 包的工具\nusethis：简化开发流程\nroxygen2：生成 R 包的文档\npkgdown：将 R 包的文档发布为一个独立网站\ntestthat：测试 R 包的功能\nrmarkdown：制作 R 包的帮助文档\n\n你不用记住这些工具，它们会在你开发 R 包的过程中自动出现。\n\n33.7.1 工作流程\n\n33.7.1.1 初始化项目仓库\n软件开发通常使用 Git 管理代码，使用 GitHub 托管代码。因此，在开始开发之前，需要初始化项目仓库。这个工作可以交给 usethis 包来完成。\nlibrary(devtools)\ncreate_package(\"mypackage\")\n此外，也可以通过 RStudio 的菜单来完成。方法是：\n\n点击 File -&gt; New Project -&gt; Package\n输入项目名称，选中 Git 选项，点击 Create Project\n\n\n\n33.7.1.2 添加代码和文档\n在 RStudio 中，可以很方便地添加代码和文档。在终端中输入 use_r() 函数，可以添加一个 R 脚本。该函数会自动在 R/ 目录下创建一个 myfunction.R 文件。并且，将该文件打开进入编辑状态。\nuse_r(\"myfunction\")\n在这个文件中，我们可以编写自己的函数。这里，我们编写一个简单的函数，计算两个数的和。\nmyfunction &lt;- function(a, b) {\n  return(a + b)\n}\n运行 load_all() 函数，可以加载软件包中包含的所有函数。\nload_all()\n这时，就可以调用 myfunction() 函数计算两个数的和。\nmyfunction(1, 2)\n虽然这个函数已经可以运行，但是它还缺乏必要的文档。所以，接下来给它添加文档。文档直接在源代码中以 roxygen2 的格式编写（通常是英文）。\n#' 计算两个数的和\n#'\n#' @param a 第一个数\n#' @param b 第二个数\n#' @return 两个数的和\n#' @export\nmyfunction &lt;- function(a, b) {\n  return(a + b)\n}\n这是，运行 document() 函数，可以生成文档。运行后，在 man/ 目录下，会生成一个 myfunction.Rd 文件。这个文件就是文档。因为文档中有 @export 标签，所以这个函数会被更新到 NAMESPACE 文件中，这样就可以被正常调用了。\ndocument()\n有了文档，就可以使用 ?myfunction 查看函数的使用方法。\n?myfunction\n\n\n33.7.1.3 测试和上传\n现在，就可以测试这个只有一个函数的软件包了。\ncheck()\ncheck() 函数会运行一系列的测试，包括：\n\n语法检查\n示例代码运行\n文档检查\n等等\n\n如果没有问题，说明这个软件包符合开发规范。\n\n\n33.7.1.4 修改 DESCRIPTION 文件\nDESCRIPTION 文件是软件包的配置文件，包括软件包的名称、版本、依赖等信息。这些信息需要根据实际情况进行修改。\nedit_file(\"DESCRIPTION\")\n下面是 DESCRIPTION 文件的示例，这里面的 Title/Version/Author/Maintainer/Description 等都是需要开发者根据实际情况进行修改完善的。\nPackage: ggVennDiagram\nType: Package\nTitle: A 'ggplot2' Implement of Venn Diagram\nVersion: 1.5.3\nAuthors@R: c(\n  person(\"Chun-Hui\",\"Gao\", email=\"gaospecial@gmail.com\", role=c(\"aut\",\"cre\"), comment=c(ORCID = \"0000-0002-1445-7939\")),\n  person(\"Guangchuang\", \"Yu\", email = \"guangchuangyu@gmail.com\", role = c(\"ctb\"), comment = c(ORCID = \"0000-0002-6485-8781\")),\n  person(\"Adrian\", \"Dusa\",  email = \"dusa.adrian@unibuc.ro\", role = c(\"aut\",\"cph\"), \n    comment = c(ORCID = \"0000-0002-3525-9253\", \n    note = \"Adrian Dusa is the author and copyright holder of venn, where ggVennDiagram imports the polygon coordinates enabling 5 - 7 sets Venn diagram.\")),\n  person(\"Turgut Yigit\", \"Akyol\", email = \"tyakyol@gmail.com\", role = c(\"ctb\"), comment=c(ORCID = \"0000-0003-0897-7716\"))\n  )\nMaintainer: Chun-Hui Gao &lt;gaospecial@gmail.com&gt;\nDescription: Easy-to-use functions to generate 2-7 sets Venn or upset plot in publication quality. \n  'ggVennDiagram' plot Venn or upset using well-defined geometry dataset and 'ggplot2'. The shapes of 2-4 sets \n  Venn use circles and ellipses, while the shapes of 4-7 sets Venn use irregular polygons (4 has both forms), which \n  are developed and imported from another package 'venn', authored by Adrian Dusa. We provided internal functions to \n  integrate shape data with user provided sets data, and calculated the geometry of every regions/intersections \n  of them, then separately plot Venn in four components, set edges/labels, and region edges/labels.\n  From version 1.0, it is possible to customize these components as you demand in ordinary 'ggplot2' grammar.\n  From version 1.4.4, it supports unlimited number of sets, as it can draw a plain upset plot automatically when\n  number of sets is more than 7.\nDepends: \n    R (&gt;= 4.1.0)\nImports: \n    ggplot2 (&gt;= 3.4.0),\n    dplyr,\n    methods,\n    tibble,\n    aplot,\n    venn (&gt;= 1.12),\n    yulab.utils,\n    forcats\nURL: https://github.com/gaospecial/ggVennDiagram, https://gaospecial.github.io/ggVennDiagram/\nLicense: GPL-3\nEncoding: UTF-8\nRoxygenNote: 7.2.3\nSuggests: \n    testthat (&gt;= 2.1.0),\n    knitr,\n    plotly,\n    RColorBrewer,\n    shiny,\n    rmarkdown,\n    tidyr\nVignetteBuilder: knitr\nLazyData: true\n\n\n33.7.1.5 写一个 README 文件\nREADME 文件是软件包的说明文件，包括软件包的描述及基本用法。这些信息需要根据实际情况进行修改。\nuse_readme_rmd()\n\n\n33.7.1.6 上传到 GitHub\n开发版本的软件包，通常会托管到 GitHub 上。使用 usethis 包可以很方便地上传到 GitHub。\nuse_github()\n\n\n33.7.1.7 分发到 CRAN\n使用 devtools 包可以很方便地将软件包分发到 CRAN。\ndevtools::submit_cran()\n\n\n\n33.7.2 维护\n完成软件包的开发和发布后，就要开始对软件包进行维护了。软件包的维护通常包括：\n\n修复 bug\n添加新功能\n更新文档\n等等\n\n维护是软件开发中非常重要的事情。GitHub 提供了多种方式来便捷软件的维护。\n\nGitHub Issues：使用 GitHub Issues 来管理 bug 和功能请求。\nPull request：使用 Pull request 来合并用户贡献的代码和文档。\nCRAN update：使用 devtools::submit_cran() 来提交新版本的软件包到 CRAN。",
    "crumbs": [
      "开发协作",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>开发 R 包</span>"
    ]
  },
  {
    "objectID": "r-package-development.html#也写一个-r-包",
    "href": "r-package-development.html#也写一个-r-包",
    "title": "33  开发 R 包",
    "section": "33.8 也写一个 R 包？",
    "text": "33.8 也写一个 R 包？\n\n既为自己：你的需求是什么？\n也为他人：领域的需求是什么？\n安装开发版 R 包的方式\n\ndevtools::install_github(\"gaospecial/ggVennDiagram\")\nremotes::install_github(\"gaospecial/ggVennDiagram\")\npak::pak(\"gaospecial/ggVennDiagram\")",
    "crumbs": [
      "开发协作",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>开发 R 包</span>"
    ]
  },
  {
    "objectID": "r-package-development.html#参考资料",
    "href": "r-package-development.html#参考资料",
    "title": "33  开发 R 包",
    "section": "33.9 参考资料",
    "text": "33.9 参考资料\n\n团队的第一个 R 包 https://github.com/gaospecial/rconf2020-my-org-first-pkg\nR 包开发指南 https://r-pkgs.org/\n\n\n\n\n\nAkyol, Turgut Yigit. 2019. RVenn: Set Operations for Many Sets. https://CRAN.R-project.org/package=RVenn.\n\n\nChen, Hanbo. 2022. VennDiagram: Generate High-Resolution Venn and Euler Plots. https://CRAN.R-project.org/package=VennDiagram.\n\n\nDusa, Adrian. 2024. Venn: Draw Venn Diagrams. https://github.com/dusadrian/venn.\n\n\nGao, Chun-Hui, and Adrian Dusa. 2024. ggVennDiagram: A Ggplot2 Implement of Venn Diagram. https://github.com/gaospecial/ggVennDiagram.\n\n\nLarsson, Johan. 2024. Eulerr: Area-Proportional Euler and Venn Diagrams with Ellipses. https://github.com/jolars/eulerr.\n\n\nWarnes, Gregory R., Ben Bolker, Lodewijk Bonebakker, Robert Gentleman, Wolfgang Huber, Andy Liaw, Thomas Lumley, et al. 2024. Gplots: Various r Programming Tools for Plotting Data. https://github.com/talgalili/gplots.\n\n\nWilkinson, Lee. 2024. Venneuler: Venn and Euler Diagrams. https://www.rforge.net/venneuler/.\n\n\nYan, Linlin. 2025. Ggvenn: Draw Venn Diagram by Ggplot2. https://yanlinlin82.github.io/ggvenn/.",
    "crumbs": [
      "开发协作",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>开发 R 包</span>"
    ]
  },
  {
    "objectID": "FAQ.html",
    "href": "FAQ.html",
    "title": "34  FAQ",
    "section": "",
    "text": "34.1 找不到包，找不到函数，找不到文件\n找不到包怎么样？\n在 R 语言中，如果使用 library(\"package\") 后提示找不到包，那么说明该包没有安装成功。请参照 Section 4.3.1 部分安装缺失的包。\n找不到函数怎么办？\n如下面的错误提示：Error in read_csv(\"file\") : could not find function \"read_csv\"，说明函数对应的包还没有载入。使用 library(\"readr\") 包载入即可使用。不同的函数来自于不同的包，如果不知道函数所在的包可以在 R 终端输入 ??read_csv 查找函数所属的包（可能会有多个包提供相同名字的函数）。\n找不到文件怎么办？\n找不到文件通常是路径写得不对。在访问文件时，可以使用相对路径或绝对路径两种方式：\n在 R 中：",
    "crumbs": [
      "问题解答",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>FAQ</span>"
    ]
  },
  {
    "objectID": "FAQ.html#找不到包找不到函数找不到文件",
    "href": "FAQ.html#找不到包找不到函数找不到文件",
    "title": "34  FAQ",
    "section": "",
    "text": "这是每个人都常常会掉进去的坑。\n\n\n\n\n\n\n\n\n相对路径是基于当前工作目录 (getwd()) 的路径。例如，如果你的文件 data.xlsx 在当前目录下，则可以用 \"data.xlsx\" 访问它；如果在 data 目录下，则使用 \"data/data.xlsx\"。相对路径更灵活，适用于在不同环境运行相同代码时无需修改路径。\n绝对路径是文件在系统中的完整路径，例如 \"/Users/username/project/data.xlsx\"（macOS/Linux）或 \"C:/Users/username/project/data.xlsx\"（Windows）。它不会受当前工作目录影响，但在不同设备或用户环境下必然需要调整。\n\n\n\nlist.files()、file.exists() 和 read_excel() 等函数支持相对路径和绝对路径。\n\n使用 normalizePath(\"path\") 可将相对路径转换为绝对路径，避免路径混乱。\n\nsetwd(\"path\") 可更改当前工作目录，但通常不推荐在脚本中使用，以免影响其他代码的执行环境。",
    "crumbs": [
      "问题解答",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>FAQ</span>"
    ]
  },
  {
    "objectID": "FAQ.html#批量线性回归总表",
    "href": "FAQ.html#批量线性回归总表",
    "title": "34  FAQ",
    "section": "34.2 批量线性回归总表",
    "text": "34.2 批量线性回归总表\n\n来自同学汪意茹的提问：有多个变量和自变量，想分别计算两两之间的线性回归模型，将参数以表格输出。\n\n可以使用 lm() 进行线性回归，并提取所需的统计量，最终整理成一个表格。下面是 R 代码：\n\n# 示例数据\nset.seed(123)\nn = 100\nY = as.data.frame(matrix(rnorm(n * 5), ncol = 5))\nX = as.data.frame(matrix(rnorm(n * 5), ncol = 5))\ncolnames(Y) = paste0(\"Y\", 1:5)\ncolnames(X) = paste0(\"X\", 1:5)\n\n# 计算线性回归结果\nresults = data.frame()\nfor (y in colnames(Y)) {\n  for (x in colnames(X)) {\n    model = lm(Y[[y]] ~ X[[x]])\n    summary_model = summary(model)\n    \n    # 提取统计量\n    slope = coef(model)[2]\n    intercept = coef(model)[1]\n    f_stat = summary_model$fstatistic[1]\n    df1 = summary_model$fstatistic[2]\n    df2 = summary_model$fstatistic[3]\n    p_value = pf(f_stat, df1, df2, lower.tail = FALSE)\n    r_squared = summary_model$r.squared\n    \n    # 存入结果\n    results = rbind(results, data.frame(\n      Dependent = y, Independent = x,\n      Slope = slope, Intercept = intercept,\n      F = f_stat, df1 = df1, df2 = df2,\n      p_value = p_value, R_squared = r_squared\n    ))\n  }\n}\n\n# 查看结果\nkableExtra::kable(results)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDependent\nIndependent\nSlope\nIntercept\nF\ndf1\ndf2\np_value\nR_squared\n\n\n\n\nX[[x]]\nY1\nX1\n-0.0549252\n0.0880826\n0.3136684\n1\n98\n0.5767158\n0.0031905\n\n\nX[[x]]1\nY1\nX2\n-0.0305091\n0.0858404\n0.1158822\n1\n98\n0.7342728\n0.0011811\n\n\nX[[x]]2\nY1\nX3\n0.1641000\n0.0730321\n3.3410885\n1\n98\n0.0706125\n0.0329687\n\n\nX[[x]]3\nY1\nX4\n-0.0196157\n0.0922417\n0.0500912\n1\n98\n0.8233706\n0.0005109\n\n\nX[[x]]4\nY1\nX5\n0.0098457\n0.0905949\n0.0118710\n1\n98\n0.9134615\n0.0001211\n\n\nX[[x]]5\nY2\nX1\n0.1179343\n-0.1025582\n1.3015941\n1\n98\n0.2567026\n0.0131075\n\n\nX[[x]]6\nY2\nX2\n0.0734598\n-0.0965540\n0.6016299\n1\n98\n0.4398246\n0.0061016\n\n\nX[[x]]7\nY2\nX3\n-0.0317125\n-0.1041893\n0.1076404\n1\n98\n0.7435466\n0.0010972\n\n\nX[[x]]8\nY2\nX4\n-0.0416729\n-0.1036466\n0.2017709\n1\n98\n0.6542870\n0.0020547\n\n\nX[[x]]9\nY2\nX5\n-0.0867070\n-0.1092109\n0.8272319\n1\n98\n0.3653057\n0.0083705\n\n\nX[[x]]10\nY3\nX1\n0.0184264\n0.1212445\n0.0325083\n1\n98\n0.8572890\n0.0003316\n\n\nX[[x]]11\nY3\nX2\n0.0080233\n0.1216658\n0.0073929\n1\n98\n0.9316561\n0.0000754\n\n\nX[[x]]12\nY3\nX3\n-0.1081812\n0.1319186\n1.3141021\n1\n98\n0.2544446\n0.0132318\n\n\nX[[x]]13\nY3\nX4\n-0.0481185\n0.1249685\n0.2790104\n1\n98\n0.5985447\n0.0028390\n\n\nX[[x]]14\nY3\nX5\n-0.0134448\n0.1202071\n0.0204444\n1\n98\n0.8865965\n0.0002086\n\n\nX[[x]]15\nY4\nX1\n-0.0994960\n-0.0404316\n0.7987189\n1\n98\n0.3736662\n0.0080843\n\n\nX[[x]]16\nY4\nX2\n-0.0644415\n-0.0458662\n0.4003737\n1\n98\n0.5283700\n0.0040688\n\n\nX[[x]]17\nY4\nX3\n0.1726759\n-0.0545047\n2.8425657\n1\n98\n0.0949789\n0.0281882\n\n\nX[[x]]18\nY4\nX4\n-0.1630233\n-0.0209656\n2.7450284\n1\n98\n0.1007557\n0.0272473\n\n\nX[[x]]19\nY4\nX5\n0.2496455\n-0.0314315\n6.2696003\n1\n98\n0.0139343\n0.0601287\n\n\nX[[x]]20\nY5\nX1\n0.2177588\n0.1150621\n4.3702769\n1\n98\n0.0391590\n0.0426909\n\n\nX[[x]]21\nY5\nX2\n-0.0062566\n0.1049147\n0.0041439\n1\n98\n0.9488044\n0.0000423\n\n\nX[[x]]22\nY5\nX3\n-0.1377758\n0.1204377\n1.9779037\n1\n98\n0.1627745\n0.0197834\n\n\nX[[x]]23\nY5\nX4\n-0.0372328\n0.1093355\n0.1537924\n1\n98\n0.6957888\n0.0015669\n\n\nX[[x]]24\nY5\nX5\n-0.0155561\n0.1055524\n0.0252305\n1\n98\n0.8741209\n0.0002574\n\n\n\n\n\n这个代码的核心逻辑：\n\n生成 5 个因变量和 5 个自变量的随机数据。\n使用 lm() 进行回归，并提取斜率、截距、F 统计量、自由度（df1, df2）、p 值和 R²。\n结果整理成 data.frame 并输出。\n\n你可以用自己的数据替换 Y 和 X，代码仍然适用。\n\n\n\n\n\n\nTip\n\n\n\n自由度是什么?\n即如何理解类似下面的结果。\nCall:\nlm(formula = y ~ x, data = data.frame(x = 1:10, y = 21:30))\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-4.751e-15 -2.337e-15 -5.166e-16  9.302e-16  9.668e-15 \n\nCoefficients:\n             Estimate Std. Error   t value Pr(&gt;|t|)    \n(Intercept) 2.000e+01  2.891e-15 6.919e+15   &lt;2e-16 ***\nx           1.000e+00  4.659e-16 2.147e+15   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.231e-15 on 8 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:      1 \nF-statistic: 4.608e+30 on 1 and 8 DF,  p-value: &lt; 2.2e-16\n自由度（Degrees of Freedom, DF）在回归分析中表示用于估计统计参数的独立数据点的数量。不同的自由度在回归分析中有不同的含义：\n\n回归自由度（df1）：回归模型的自由度，等于自变量的个数（不包括截距）。在简单线性回归中，只有一个自变量，所以 df1 = 1。\n残差自由度（df2）：误差项的自由度，计算公式是 n - k - 1，其中：\n\nn 是样本大小，\nk 是自变量的个数（不包括截距），\n1 是因为估计了截距。\n\n\n在你的回归结果中：\n\nF-statistic: 4.608e+30 on 1 and 8 DF，表示：\n\ndf1 = 1（因为只有一个自变量 x）。\ndf2 = 8（样本量 n = 10，减去 1 个自变量和 1 个截距，即 10 - 1 - 1 = 8）。\n\n\n总结：\n\n回归自由度（df1）= 1（自变量 x）。\n残差自由度（df2）= 8（数据点 10 个，去掉 1 个自变量和 1 个截距）。",
    "crumbs": [
      "问题解答",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>FAQ</span>"
    ]
  },
  {
    "objectID": "FAQ.html#多变量分布和相关性作图",
    "href": "FAQ.html#多变量分布和相关性作图",
    "title": "34  FAQ",
    "section": "34.3 多变量分布和相关性作图",
    "text": "34.3 多变量分布和相关性作图\n\n来自同学汪意茹的提问：如何绘制多变量之间的关系图？\n\n使用 GGally 包中的 ggpairs 函数可以绘制多变量之间的关系图。\n下面是一个可复现的示例：\n\nlibrary(GGally)\nlibrary(ggplot2)\n\n# 生成示例数据\nset.seed(123)\ndf = as.data.frame(matrix(rnorm(100 * 5), ncol = 5))\ncolnames(df) = paste0(\"Var\", 1:5)\n\n# 绘制变量间的关系\nggpairs(df, \n        lower = list(continuous = wrap(\"points\", alpha = 0.5)),  # 下三角：散点图\n        diag = list(continuous = wrap(\"densityDiag\")),          # 对角线：密度图\n        upper = list(continuous = wrap(\"cor\", size = 5)))       # 上三角：相关系数\n\n\n\n\n\n\n\n\n上面的代码中，首先生成了一个 100 行 5 列的数据框，然后使用 ggpairs 函数绘制了变量间的关系图。其中，\n\nlower 参数用于指定下三角的图形，这里使用了 points 函数绘制散点图，alpha 参数用于指定透明度；\ndiag 参数用于指定对角线的图形，这里使用了 densityDiag 函数绘制密度图；\nupper 参数用于指定上三角的图形，这里使用了 cor 函数绘制相关系数图，size 参数用于指定相关系数的字体大小。\n\nGGally 包还提供了其他的图形函数，例如 barDiag、boxplotDiag、dotplotDiag、histDiag、qqDiag、rcorr、rcorrDiag、rcorrPlot、rcorrPlotDiag、smoothDiag、textDiag 等，可以根据需要选择合适的图形函数。",
    "crumbs": [
      "问题解答",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>FAQ</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Akyol, Turgut Yigit. 2019. RVenn: Set Operations for Many Sets.\nhttps://CRAN.R-project.org/package=RVenn.\n\n\nChang, Chang-Yu, Djordje Bajić, Jean C. C. Vila, Sylvie Estrela, and\nAlvaro Sanchez. 2023. “Emergent Coexistence in Multispecies\nMicrobial Communities.” Science 381 (6655): 343–48. https://doi.org/10.1126/science.adg0727.\n\n\nChen, Hanbo. 2022. VennDiagram: Generate High-Resolution Venn and\nEuler Plots. https://CRAN.R-project.org/package=VennDiagram.\n\n\n“Csho33/Bacteria-ID: Source Code and Demos for \"Rapid\nIdentification of Pathogenic Bacteria Using Raman Spectroscopy and Deep\nLearning\".” n.d. https://github.com/csho33/bacteria-ID.\n\n\nDusa, Adrian. 2024. Venn: Draw Venn Diagrams. https://github.com/dusadrian/venn.\n\n\nGao, Chun-Hui, Hui Cao, Peng Cai, and Søren J. Sørensen. 2021.\n“The Initial Inoculation Ratio Regulates Bacterial Coculture\nInteractions and Metabolic Capacity.” ISME Journal 15\n(1): 29–40. https://doi.org/10.1038/s41396-020-00751-7.\n\n\nGao, Chun-Hui, Hui Cao, Feng Ju, Ke-Qing Xiao, Peng Cai, Yichao Wu, and\nQiaoyun Huang. 2021. “Emergent Transcriptional Adaption\nFacilitates Convergent Succession Within a Synthetic Community.”\nISME Communications 1 (1): 46. https://doi.org/10.1038/s43705-021-00049-5.\n\n\nGao, Chun-Hui, and Adrian Dusa. 2024. ggVennDiagram: A Ggplot2\nImplement of Venn Diagram. https://github.com/gaospecial/ggVennDiagram.\n\n\nHo, Chi-Sing, Neal Jean, Catherine A. Hogan, Lena Blackmon, Stefanie S.\nJeffrey, Mark Holodniy, Niaz Banaei, Amr A. E. Saleh, Stefano Ermon, and\nJennifer Dionne. 2019. “Rapid Identification of Pathogenic\nBacteria Using Raman Spectroscopy and Deep Learning.” Nature\nCommunications 10 (1): 4927. https://doi.org/10.1038/s41467-019-12898-9.\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nLarsson, Johan. 2024. Eulerr: Area-Proportional Euler and Venn\nDiagrams with Ellipses. https://github.com/jolars/eulerr.\n\n\nSerouart, Mario, Simon Madec, Etienne David, Kaaviya Velumani, Raul\nLopez Lozano, Marie Weiss, and Frédéric Baret. 2022. “SegVeg:\nSegmenting RGB Images into Green and Senescent Vegetation by Combining\nDeep and Shallow Methods.” Plant Phenomics (Washington,\nD.C.) 2022: 9803570. https://doi.org/10.34133/2022/9803570.\n\n\nWarnes, Gregory R., Ben Bolker, Lodewijk Bonebakker, Robert Gentleman,\nWolfgang Huber, Andy Liaw, Thomas Lumley, et al. 2024. Gplots:\nVarious r Programming Tools for Plotting Data. https://github.com/talgalili/gplots.\n\n\nWilkinson, Lee. 2024. Venneuler: Venn and Euler Diagrams. https://www.rforge.net/venneuler/.\n\n\nYan, Linlin. 2025. Ggvenn: Draw Venn Diagram by Ggplot2. https://yanlinlin82.github.io/ggvenn/.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "markdown.html",
    "href": "markdown.html",
    "title": "Appendix A — Markdown 速查表",
    "section": "",
    "text": "A.1 概述\nMarkdown 由 Daring Fireball 创建；原始指南在这里。然而，其语法在不同的解析器或编辑器之间可能有所不同。Typora 使用 GitHub 风格的 Markdown。",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Markdown 速查表</span>"
    ]
  },
  {
    "objectID": "markdown.html#块级元素",
    "href": "markdown.html#块级元素",
    "title": "Appendix A — Markdown 速查表",
    "section": "A.2 块级元素",
    "text": "A.2 块级元素\n\nA.2.1 段落和换行\n段落是由一行或多行连续的文本组成。在 markdown 源代码中，段落之间需要用两个或更多空行分隔。在 Typora 中，只需要一个空行（按一次Return键）就可以创建新段落。\n按 Shift + Return 可以创建单行换行。大多数其他 markdown 解析器会忽略单行换行，因此为了让其他 markdown 解析器识别你的换行，你可以在行尾留下两个空格，或插入 &lt;br/&gt;。\n\n\nA.2.2 标题\n标题使用 1-6 个井号（#）字符在行首，对应 1-6 级标题。例如：\n# 这是一级标题\n\n## 这是二级标题\n\n###### 这是六级标题\n在 Typora 中，输入 ‘#’ 后跟标题内容，按 Return 键即可创建标题。\n\n\nA.2.3 引用块\nMarkdown 使用类似电子邮件的 &gt; 字符进行块引用。显示如下：\n&gt; 这是包含两个段落的引用块。这是第一段。\n&gt;\n&gt; 这是第二段。Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus.\n\n\n\n&gt; 这是另一个只有一段的引用块。使用三个空行来分隔两个引用块。\n在 Typora 中，输入 ‘&gt;’ 后跟引用内容将生成引用块。Typora 会为你自动插入适当的 ‘&gt;’ 或换行。通过添加额外的 ‘&gt;’ 层级可以创建嵌套引用块（引用块中的引用块）。\n\n\nA.2.4 列表\n输入 * 列表项目 1 将创建无序列表 - * 符号可以替换为 + 或 -。\n输入 1. 列表项目 1 将创建有序列表 - 它们的 markdown 源代码如下：\n## 无序列表\n*   红色\n*   绿色\n*   蓝色\n\n## 有序列表\n1.  红色\n2.  绿色\n3.  蓝色\n\n\nA.2.5 任务列表\n任务列表是带有 [ ] 或 [x]（未完成或已完成）标记的列表项。例如：\n- [ ] 一个任务列表项\n- [ ] 需要列表语法\n- [ ] 支持普通**格式化**，@提及，#1234 引用\n- [ ] 未完成\n- [x] 已完成\n你可以通过点击项目前的复选框来改变完成/未完成状态。\n\n\nA.2.6 （围栏式）代码块\nTypora 仅支持 GitHub 风格 Markdown 中的围栏式代码块。不支持原始 markdown 中的代码块。\n使用围栏很简单：输入 ``` 并按 return。在 ``` 后添加可选的语言标识符，我们将通过语法高亮显示它：\n这里是一个例子：\n\n```js\nfunction test() {\n  console.log(\"注意这个函数前面的空行？\");\n}\n```\n\n语法高亮：\n```ruby\nrequire 'redcarpet'\nmarkdown = Redcarpet.new(\"Hello World!\")\nputs markdown.to_html\n```\n\n\nA.2.7 数学公式块\n你可以使用 MathJax 渲染 LaTeX 数学表达式。\n要添加数学表达式，输入 $$ 并按 ‘Return’ 键。这将触发一个接受 Tex/LaTex 源代码的输入字段。例如：\n\\[\n\\mathbf{V}_1 \\times \\mathbf{V}_2 =  \\begin{vmatrix}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n\\frac{\\partial X}{\\partial u} &  \\frac{\\partial Y}{\\partial u} & 0 \\\\\n\\frac{\\partial X}{\\partial v} &  \\frac{\\partial Y}{\\partial v} & 0 \\\\\n\\end{vmatrix}\n\\]\n在 markdown 源文件中，数学公式块是由一对 ‘$$’ 标记包围的 LaTeX 表达式：\n$$\n\\mathbf{V}_1 \\times \\mathbf{V}_2 =  \\begin{vmatrix}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n\\frac{\\partial X}{\\partial u} &  \\frac{\\partial Y}{\\partial u} & 0 \\\\\n\\frac{\\partial X}{\\partial v} &  \\frac{\\partial Y}{\\partial v} & 0 \\\\\n\\end{vmatrix}\n$$\n你可以在这里找到更多详情。\n\n\nA.2.8 表格\n输入 | 第一个表头  | 第二个表头 | 并按 return 键。这将创建一个两列的表格。\n创建表格后，将焦点放在表格上会打开表格工具栏，你可以调整大小、对齐或删除表格。你还可以使用上下文菜单复制和添加/删除单个列/行。\n表格的完整语法如下所示，但不需要详细了解完整语法，因为 Typora 会自动生成表格的 markdown 源代码：\n| 第一个表头  | 第二个表头 |\n| ------------- | ------------- |\n| 内容单元格  | 内容单元格  |\n| 内容单元格  | 内容单元格  |\n你还可以在表格中包含内联 Markdown，如链接、粗体、斜体或删除线。\n最后，通过在表头行中包含冒号（:），你可以定义该列中的文本为左对齐、右对齐或居中对齐：\n| 左对齐  | 居中对齐  | 右对齐 |\n| :------------ |:---------------:| -----:|\n| 第3列是      | 一些很长的文字 | $1600 |\n| 第2列是      | 居中的        |   $12 |\n| 斑马条纹     | 很整洁        |    $1 |\n最左边的冒号表示左对齐列；最右边的冒号表示右对齐列；两边都有冒号表示居中对齐列。\n\n\nA.2.9 脚注\n你可以像这样创建脚注[^脚注]。\n\n[^脚注]: 这里是脚注的*文本*内容。\n将产生：\n你可以像这样创建脚注1。\n将鼠标悬停在”脚注”上标上可以查看脚注内容。\n\n\nA.2.10 水平分割线\n在空行上输入 *** 或 --- 并按 return 将绘制一条水平线。\n\n\n\nA.2.11 YAML Front Matter\nTypora 现在支持 YAML Front Matter。在文章顶部输入 --- 然后按 Return 键来引入元数据块。或者，你可以从 Typora 的顶部菜单插入元数据块。\n\n\nA.2.12 目录 (TOC)\n输入 [toc] 并按 Return 键。这将创建一个”目录”部分。TOC 从文档中提取所有标题，其内容会随着你向文档添加内容而自动更新。",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Markdown 速查表</span>"
    ]
  },
  {
    "objectID": "markdown.html#行内元素",
    "href": "markdown.html#行内元素",
    "title": "Appendix A — Markdown 速查表",
    "section": "A.3 行内元素",
    "text": "A.3 行内元素\n行内元素在输入后会立即被解析和渲染。将光标移动到这些行内元素中间会将它们展开为 markdown 源代码。以下是每个行内元素的语法说明。\n\nA.3.1 链接\nMarkdown 支持两种链接样式：内联和引用。\n在这两种样式中，链接文本都用[方括号]括起来。\n要创建内联链接，在链接文本的右方括号后立即使用一组常规括号。在括号内，放入你想要链接指向的 URL，以及可选的用引号括起来的链接标题。例如：\n这是[一个示例](http://example.com/ \"标题\")内联链接。\n\n[这个链接](http://example.net/)没有标题属性。\n将产生：\n这是一个示例内联链接。(&lt;p&gt;这是 &lt;a href=\"http://example.com/\" title=\"标题\"&gt;)\n这个链接没有标题属性。(&lt;p&gt;&lt;a href=\"http://example.net/\"&gt;这个链接&lt;/a&gt; 没有)\n\nA.3.1.1 内部链接\n你可以将 href 设置为标题，这将创建一个书签，允许你点击后跳转到该部分。例如：\nCommand(在 Windows 上：Ctrl) + 点击这个链接将跳转到”块级元素”标题。要查看如何编写，请按住 ⌘ 键移动光标或点击该链接以展开为 markdown 源代码。\n\n\nA.3.1.2 引用链接\n引用式链接使用第二组方括号，在其中放置你选择的标签来标识该链接：\n这是[一个示例][id]引用式链接。\n\n然后，在文档的任何地方，你可以像这样定义你的链接标签：\n\n[id]: http://example.com/  \"可选标题\"\n在 Typora 中，它们将被渲染为：\n这是一个示例引用式链接。\n隐式链接名称快捷方式允许你省略链接的名称，这种情况下链接文本本身将用作名称。只需使用一组空的方括号 — 例如，要将单词”Google”链接到 google.com 网站，你只需写：\n[Google][]\n然后定义链接：\n\n[Google]: http://google.com/\n在 Typora 中，点击链接将展开它以进行编辑，command+点击将在网络浏览器中打开超链接。\n\n\n\nA.3.2 URL\nTypora 允许你插入 URL 作为链接，用 &lt;尖括号&gt; 括起来。\n&lt;i@typora.io&gt; 变成 i@typora.io。\nTypora 还会自动链接标准 URL。例如：www.google.com。\n\n\nA.3.3 图片\n图片的语法与链接类似，但需要在链接开始前添加一个 ! 字符。插入图片的语法如下：\n![替代文本](/path/to/img.jpg)\n\n![替代文本](/path/to/img.jpg \"可选标题\")\n你可以使用拖放来从图片文件或网络浏览器插入图片。通过点击图片，你可以修改 markdown 源代码。如果使用拖放添加的图片与当前编辑的文档在同一目录或子目录中，将使用相对路径。\n如果你使用 markdown 来构建网站，你可以在 YAML Front Matter 中使用 typora-root-url 属性为本地计算机上的图片预览指定 URL 前缀。例如，在 YAML Front Matter 中输入 typora-root-url:/User/Abner/Website/typora.io/，然后 ![alt](/blog/img/test.png) 将在 Typora 中被视为 ![alt](file:///User/Abner/Website/typora.io/blog/img/test.png)。\n你可以在这里找到更多详情。\n\n\nA.3.4 强调\nMarkdown 将星号（*）和下划线（_）视为强调的标记。用一个 * 或 _ 包围的文本将被包装在 HTML &lt;em&gt; 标签中。例如：\n*单个星号*\n\n_单个下划线_\n输出：\n单个星号\n单个下划线\nGFM 会忽略单词中的下划线，这在代码和名称中很常见，比如：\n\nwow_great_stuff\ndo_this_and_do_that_and_another_thing.\n\n要在原本会被用作强调分隔符的位置产生字面意义上的星号或下划线，你可以用反斜杠转义：\n\\*这段文字被字面意义上的星号包围\\*\nTypora 推荐使用 * 符号。\n\n\nA.3.5 加粗\n双重的 * 或 _ 会导致其包含的内容被包装在 HTML &lt;strong&gt; 标签中，例如：\n**双星号**\n\n__双下划线__\n输出：\n双星号\n双下划线\nTypora 推荐使用 ** 符号。\n\n\nA.3.6 代码\n要标示一个行内代码段，用反引号（`）包围它。与预格式化的代码块不同，代码段表示正常段落中的代码。例如：\n使用 `printf()` 函数。\n将产生：\n使用 printf() 函数。\n\n\nA.3.7 删除线\nGFM 添加了创建删除线文本的语法，这在标准 Markdown 中是缺失的。\n~~wrong text~~ 变成 wrong text\n\n\nA.3.8 下划线\n下划线通过原始 HTML 实现。\n&lt;u&gt;underline&lt;/u&gt; 变成 underline。\n\n\nA.3.9 表情符号 :smile:\n使用语法 :smile: 输入表情符号。\n用户可以通过按 ESC 键触发表情符号的自动完成建议，或在偏好设置面板中启用后自动触发它。此外，通过菜单栏中的 编辑 -&gt; 表情符号与符号（macOS）也支持直接输入 UTF-8 表情符号字符。\n\n\nA.3.10 行内数学公式\n要使用此功能，请先在 偏好设置 面板 -&gt; Markdown 标签中启用它。然后，使用 $ 包围 TeX 命令。例如：$\\lim_{x \\to \\infty} \\exp(-x) = 0$ 将被渲染为 LaTeX 命令。\n要触发行内数学公式的预览：输入 “$”，然后按 ESC 键，再输入 TeX 命令。\n你可以在这里找到更多详情。\n\n\nA.3.11 下标\n要使用此功能，请先在 偏好设置 面板 -&gt; Markdown 标签中启用它。然后，使用 ~ 包围下标内容。例如：H~2~O，X~long\\ text~。\n\n\nA.3.12 上标\n要使用此功能，请先在 偏好设置 面板 -&gt; Markdown 标签中启用它。然后，使用 ^ 包围上标内容。例如：X^2^。\n\n\nA.3.13 高亮\n要使用此功能，请先在 偏好设置 面板 -&gt; Markdown 标签中启用它。然后，使用 == 包围高亮内容。例如：==高亮==。",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Markdown 速查表</span>"
    ]
  },
  {
    "objectID": "markdown.html#html",
    "href": "markdown.html#html",
    "title": "Appendix A — Markdown 速查表",
    "section": "A.4 HTML",
    "text": "A.4 HTML\n你可以使用 HTML 来设置纯 Markdown 不支持的内容样式。例如，使用 &lt;span style=\"color:red\"&gt;这段文字是红色的&lt;/span&gt; 来添加红色文字。\n\nA.4.1 嵌入内容\n一些网站提供基于 iframe 的嵌入代码，你也可以将其粘贴到 Typora 中。例如：\n&lt;iframe height='265' scrolling='no' title='Fancy Animated SVG Menu' src='http://codepen.io/jeangontijo/embed/OxVywj/?height=265&theme-id=0&default-tab=css,result&embed-version=2' frameborder='no' allowtransparency='true' allowfullscreen='true' style='width: 100%;'&gt;&lt;/iframe&gt;\n\n\nA.4.2 视频\n你可以使用 HTML 的 &lt;video&gt; 标签来嵌入视频。例如：\n&lt;video src=\"xxx.mp4\" /&gt;\n\n\nA.4.3 其他 HTML 支持\n你可以在这里找到更多详情。",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Markdown 速查表</span>"
    ]
  },
  {
    "objectID": "markdown.html#footnotes",
    "href": "markdown.html#footnotes",
    "title": "Appendix A — Markdown 速查表",
    "section": "",
    "text": "这里是脚注的文本内容。↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Markdown 速查表</span>"
    ]
  },
  {
    "objectID": "write-math-equation.html",
    "href": "write-math-equation.html",
    "title": "Appendix B — 书写数学公式",
    "section": "",
    "text": "B.1 希腊字母",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>书写数学公式</span>"
    ]
  },
  {
    "objectID": "write-math-equation.html#希腊字母",
    "href": "write-math-equation.html#希腊字母",
    "title": "Appendix B — 书写数学公式",
    "section": "",
    "text": "序号\n标准符号\nLaTeX\n首字母大写\nLaTeX\n\n\n\n\n1\n\\(\\alpha\\)\n\\alpha\n\n\n\n\n2\n\\(\\beta\\)\n\\beta\n\n\n\n\n3\n\\(\\gamma\\)\n\\gamma\n\\(\\Gamma\\)\n\\Gamma\n\n\n4\n\\(\\delta\\)\n\\delta\n\\(\\Delta\\)\n\\Delta\n\n\n5\n\\(\\epsilon\\)\n\\epsilon\n\n\n\n\n6\n\\(\\zeta\\)\n\\zeta\n\n\n\n\n7\n\\(\\eta\\)\n\\eta\n\n\n\n\n8\n\\(\\theta\\)\n\\theta\n\\(\\Theta\\)\n\\Theta\n\n\n9\n\\(\\iota\\)\n\\iota\n\n\n\n\n10\n\\(\\kappa\\)\n\\kappa\n\n\n\n\n11\n\\(\\lambda\\)\n\\lambda\n\\(\\Lambda\\)\n\\Lambda\n\n\n12\n\\(\\mu\\)\n\\mu\n\n\n\n\n13\n\\(\\nu\\)\n\\nu\n\n\n\n\n14\n\\(\\xi\\)\n\\xi\n\\(\\Xi\\)\n\\Xi\n\n\n15\n\\(o\\)\no\n\\(O\\)\nO\n\n\n16\n\\(\\pi\\)\n\\pi\n\\(\\Pi\\)\n\\Pi\n\n\n17\n\\(\\rho\\)\n\\rho\n\n\n\n\n18\n\\(\\sigma\\)\n\\sigma\n\\(\\Sigma\\)\n\\Sigma\n\n\n19\n\\(\\tau\\)\n\\tau\n\n\n\n\n20\n\\(\\upsilon\\)\n\\upsilon\n\\(\\Upsilon\\)\n\\Upsilon\n\n\n21\n\\(\\phi\\)\n\\phi\n\\(\\Phi\\)\n\\Phi\n\n\n22\n\\(\\chi\\)\n\\chi\n\n\n\n\n23\n\\(\\psi\\)\n\\psi\n\\(\\Psi\\)\n\\Psi\n\n\n24\n\\(\\omega\\)\n\\omega\n\\(\\Omega\\)\n\\Omega\n\n\n25\n\\(\\digamma\\)\n\\digamma",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>书写数学公式</span>"
    ]
  },
  {
    "objectID": "write-math-equation.html#二元运算符",
    "href": "write-math-equation.html#二元运算符",
    "title": "Appendix B — 书写数学公式",
    "section": "B.2 二元运算符",
    "text": "B.2 二元运算符\n\n\n\n序号\n符号\nLaTeX\n\n\n\n\n1\n\\(+\\)\n+\n\n\n2\n\\(-\\)\n-\n\n\n3\n\\(\\times\\)\n\\times\n\n\n4\n\\(\\div\\)\n\\div\n\n\n5\n\\(\\pm\\)\n\\pm\n\n\n6\n\\(\\mp\\)\n\\mp\n\n\n7\n\\(\\triangleleft\\)\n\\triangleleft\n\n\n8\n\\(\\triangleright\\)\n\\triangleright\n\n\n9\n\\(\\cdot\\)\n\\cdot\n\n\n10\n\\(\\setminus\\)\n\\setminus\n\n\n11\n\\(\\star\\)\n\\star\n\n\n12\n\\(\\ast\\)\n\\ast\n\n\n13\n\\(\\cup\\)\n\\cup\n\n\n14\n\\(\\cap\\)\n\\cap\n\n\n15\n\\(\\sqcup\\)\n\\sqcup\n\n\n16\n\\(\\sqcap\\)\n\\sqcap\n\n\n17\n\\(\\vee\\)\n\\vee\n\n\n18\n\\(\\wedge\\)\n\\wedge\n\n\n19\n\\(\\circ\\)\n\\circ",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>书写数学公式</span>"
    ]
  },
  {
    "objectID": "write-math-equation.html#二元关系符",
    "href": "write-math-equation.html#二元关系符",
    "title": "Appendix B — 书写数学公式",
    "section": "B.3 二元关系符",
    "text": "B.3 二元关系符\n\n\n\n序号\n符号\nLaTeX\n\n\n\n\n1\n\\(=\\)\n=\n\n\n2\n\\(\\ne\\)\n\\ne\n\n\n3\n\\(\\neq\\)\n\\neq\n\n\n4\n\\(\\equiv\\)\n\\equiv\n\n\n5\n\\(\\not\\equiv\\)\n\\not\\equiv\n\n\n6\n\\(\\doteq\\)\n\\doteq\n\n\n7\n\\(\\doteqdot\\)\n\\doteqdot",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>书写数学公式</span>"
    ]
  },
  {
    "objectID": "write-math-equation.html#逻辑符号",
    "href": "write-math-equation.html#逻辑符号",
    "title": "Appendix B — 书写数学公式",
    "section": "B.4 逻辑符号",
    "text": "B.4 逻辑符号\n\n\n\n序号\n符号\nLaTeX\n\n\n\n\n1\n\\(\\forall\\)\n\\forall\n\n\n2\n\\(\\exists\\)\n\\exists\n\n\n3\n\\(\\nexists\\)\n\\nexists\n\n\n4\n\\(\\therefore\\)\n\\therefore\n\n\n5\n\\(\\because\\)\n\\because\n\n\n6\n\\(\\And\\)\n\\And\n\n\n7\n\\(\\lor\\)\n\\lor\n\n\n8\n\\(\\vee\\)\n\\vee",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>书写数学公式</span>"
    ]
  },
  {
    "objectID": "write-math-equation.html#集合符号",
    "href": "write-math-equation.html#集合符号",
    "title": "Appendix B — 书写数学公式",
    "section": "B.5 集合符号",
    "text": "B.5 集合符号\n\n\n\n序号\n符号\nLaTeX\n\n\n\n\n1\n\\(\\{ \\}\\)\n\\{ \\}\n\n\n2\n\\(\\emptyset\\)\n\\emptyset\n\n\n3\n\\(\\varnothing\\)\n\\varnothing\n\n\n4\n\\(\\in\\)\n\\in\n\n\n5\n\\(\\notin\\)\n\\notin\n\n\n6\n\\(\\ni\\)\n\\ni\n\n\n7\n\\(\\cap\\)\n\\cap\n\n\n8\n\\(\\Cap\\)\n\\Cap\n\n\n9\n\\(\\sqcap\\)\n\\sqcap\n\n\n10\n\\(\\bigcap\\)\n\\bigcap\n\n\n11\n\\(\\cup\\)\n\\cup\n\n\n12\n\\(\\Cup\\)\n\\Cup\n\n\n13\n\\(\\sqcup\\)\n\\sqcup\n\n\n14\n\\(\\bigcup\\)\n\\bigcup\n\n\n15\n\\(\\bigsqcup\\)\n\\bigsqcup",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>书写数学公式</span>"
    ]
  },
  {
    "objectID": "write-math-equation.html#特殊符号",
    "href": "write-math-equation.html#特殊符号",
    "title": "Appendix B — 书写数学公式",
    "section": "B.6 特殊符号",
    "text": "B.6 特殊符号\n\n\n\n序号\n符号\nLaTeX\n\n\n\n\n1\n\\(\\infty\\)\n\\infty",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>书写数学公式</span>"
    ]
  },
  {
    "objectID": "write-math-equation.html#分数",
    "href": "write-math-equation.html#分数",
    "title": "Appendix B — 书写数学公式",
    "section": "B.7 分数",
    "text": "B.7 分数\n\n\n\n类型\n符号\nLaTeX\n\n\n\n\n分数\n\\(\\frac{2}{4}x=0.5x\\)\n\\frac{2}{4}x=0.5x\n\n\n分数\n\\({2 \\over 4}x=0.5x\\)\n{2 \\over 4}x=0.5x",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>书写数学公式</span>"
    ]
  },
  {
    "objectID": "write-math-equation.html#数值函数",
    "href": "write-math-equation.html#数值函数",
    "title": "Appendix B — 书写数学公式",
    "section": "B.8 数值函数",
    "text": "B.8 数值函数\n\n\n\n\n\n\n\n符号\nLaTeX\n\n\n\n\n\\(\\exp_a b = a^b, \\exp b = e^b, 10^m\\)\n\\exp_a b = a^b, \\exp b = e^b, 10^m\n\n\n\\(\\ln c, \\lg d = \\log e, \\log_{10} f\\)\n\\ln c, \\lg d = \\log e, \\log_{10} f\n\n\n\\(\\sin a, \\cos b, \\tan c, \\cot d, \\sec e, \\csc f\\)\n\\sin a, \\cos b, \\tan c, \\cot d, \\sec e, \\csc f\n\n\n\\(\\min(x,y), \\max(x,y)\\)\n\\min(x,y), \\max(x,y)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>书写数学公式</span>"
    ]
  },
  {
    "objectID": "write-math-equation.html#根号",
    "href": "write-math-equation.html#根号",
    "title": "Appendix B — 书写数学公式",
    "section": "B.9 根号",
    "text": "B.9 根号\n\n\n\n符号\nLaTeX\n\n\n\n\n\\(\\surd\\)\n\\surd\n\n\n\\(\\sqrt{\\pi}\\)\n\\sqrt{\\pi}\n\n\n\\(\\sqrt[n]{\\pi}\\)\n\\sqrt[n]{\\pi}\n\n\n\\(\\sqrt[3]{\\frac{x^3+y^3}{2}}*\\)\n\\sqrt[3]{\\frac{x^3+y^3}{2}}*",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>书写数学公式</span>"
    ]
  },
  {
    "objectID": "write-math-equation.html#微分与导数",
    "href": "write-math-equation.html#微分与导数",
    "title": "Appendix B — 书写数学公式",
    "section": "B.10 微分与导数",
    "text": "B.10 微分与导数\n\n\n\n\n\n\n\n符号\nLaTeX\n\n\n\n\n\\(dt, \\mathrm{d}t, \\partial t, \\nabla\\psi\\)\ndt, \\mathrm{d}t, \\partial t, \\nabla\\psi\n\n\n\\(dy/dx, \\mathrm{d}y/\\mathrm{d}x\\)\ndy/dx, \\mathrm{d}y/\\mathrm{d}x\n\n\n\\(\\frac{dy}{dx}, \\frac{\\mathrm{d}y}{\\mathrm{d}x}\\)\n\\frac{dy}{dx}, \\frac{\\mathrm{d}y}{\\mathrm{d}x}\n\n\n\\(\\prime, \\backprime, f^\\prime, f', f''\\)\n\\prime, \\backprime, f^\\prime, f', f''",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>书写数学公式</span>"
    ]
  },
  {
    "objectID": "write-math-equation.html#极限",
    "href": "write-math-equation.html#极限",
    "title": "Appendix B — 书写数学公式",
    "section": "B.11 极限",
    "text": "B.11 极限\n\n\n\n符号\nLaTeX\n\n\n\n\n\\(\\lim_{n \\to \\infty}x_n\\)\n\\lim_{n \\to \\infty}x_n",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>书写数学公式</span>"
    ]
  },
  {
    "objectID": "write-math-equation.html#大型运算符",
    "href": "write-math-equation.html#大型运算符",
    "title": "Appendix B — 书写数学公式",
    "section": "B.12 大型运算符",
    "text": "B.12 大型运算符\n\n\n\n分类\n符号\nLaTeX\n\n\n\n\n求和\n\\(\\sum_{a}^{b}\\)\n\\sum_{a}^{b}\n\n\n连乘积\n\\(\\prod_{a}^{b}\\)\n\\prod_{a}^{b}\n\n\n余积\n\\(\\coprod_{a}^{b}\\)\n\\coprod_{a}^{b}\n\n\n并集\n\\(\\bigcup_{a}^{b}\\)\n\\bigcup_{a}^{b}\n\n\n交集\n\\(\\bigcap_{a}^{b}\\)\n\\bigcap_{a}^{b}\n\n\n析取\n\\(\\bigvee_{a}^{b}\\)\n\\bigvee_{a}^{b}\n\n\n合取\n\\(\\bigwedge_{a}^{b}\\)\n\\bigwedge_{a}^{b}",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>书写数学公式</span>"
    ]
  },
  {
    "objectID": "write-math-equation.html#上下标",
    "href": "write-math-equation.html#上下标",
    "title": "Appendix B — 书写数学公式",
    "section": "B.13 上下标",
    "text": "B.13 上下标\n\n\n\n\n\n\n\n\n类型\n符号\n代码\n\n\n\n\n上标\n\\(a^2\\) \\(a^{x+3}\\)\na^2 a^{x+3}\n\n\n下标\n\\(a_2\\)\na_2\n\n\n组合\n\\(10^{30} a^{2+2}\\) \\(a{i,j} b{f'}\\)\n10^{30} a^{2+2} a{i,j} b{f'}\n\n\n上下标混合\n\\(x_2^3\\) \\({x_2}^3\\)\nx_2^3 {x_2}^3\n\n\n上标的上标\n\\(10^{10^{8}}\\)\n10^{10^{8}}",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>书写数学公式</span>"
    ]
  },
  {
    "objectID": "write-math-equation.html#二项式系数",
    "href": "write-math-equation.html#二项式系数",
    "title": "Appendix B — 书写数学公式",
    "section": "B.14 二项式系数",
    "text": "B.14 二项式系数\n\n\n\n类型\n符号\nLaTeX\n\n\n\n\n二项式系数\n\\(\\binom{n}{k}\\)\n\\binom{n}{k}\n\n\n小型二项式系数\n\\(\\tbinom{n}{k}\\)\n\\tbinom{n}{k}\n\n\n大型二项式系数\n\\(\\dbinom{n}{k}\\)\n\\dbinom{n}{k}",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>书写数学公式</span>"
    ]
  },
  {
    "objectID": "write-math-equation.html#矩阵",
    "href": "write-math-equation.html#矩阵",
    "title": "Appendix B — 书写数学公式",
    "section": "B.15 矩阵",
    "text": "B.15 矩阵\n\\[\n\\begin{matrix}\nx & y \\\\\nz & v\n\\end{matrix}\n\\]\n\\begin{matrix}\nx & y \\\\\nz & v\n\\end{matrix}\n\\[\n\\begin{vmatrix}\nx & y \\\\\nz & v\n\\end{vmatrix}\n\\]\n\\begin{vmatrix}\nx & y \\\\\nz & v\n\\end{vmatrix}",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>书写数学公式</span>"
    ]
  },
  {
    "objectID": "write-math-equation.html#数组",
    "href": "write-math-equation.html#数组",
    "title": "Appendix B — 书写数学公式",
    "section": "B.16 数组",
    "text": "B.16 数组\n\\[\n\\begin{array}{ | c | c | c | }\na & b & S \\\\\n\\hline\n0 & 0 & 1 \\\\\n0 & 1 & 1 \\\\\n1 & 0 & 1 \\\\\n1 & 1 & 0\n\\end{array}\n\\]\n\\begin{array}{ | c | c | c | }\na & b & S \\\\\n\\hline\n0 & 0 & 1 \\\\\n0 & 1 & 1 \\\\\n1 & 0 & 1 \\\\\n1 & 1 & 0\n\\end{array}",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>书写数学公式</span>"
    ]
  },
  {
    "objectID": "write-math-equation.html#方程组",
    "href": "write-math-equation.html#方程组",
    "title": "Appendix B — 书写数学公式",
    "section": "B.17 方程组",
    "text": "B.17 方程组\n\\[\n\\begin{cases}\n3x + 5y + z \\\\\n7x - 2y + 4z \\\\\n-6x + 3y + 2z\n\\end{cases}\n\\]\n\\begin{cases}\n3x + 5y + z \\\\\n7x - 2y + 4z \\\\\n-6x + 3y + 2z\n\\end{cases}",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>书写数学公式</span>"
    ]
  },
  {
    "objectID": "write-math-equation.html#多行等式",
    "href": "write-math-equation.html#多行等式",
    "title": "Appendix B — 书写数学公式",
    "section": "B.18 多行等式",
    "text": "B.18 多行等式\nMathJax 3.0 取消了单行公式环境下 \\\\ 的强制换行功能，因此强制换行命令 \\\\ 仅能用于 eqnarray、align、array、matrix 等多行环境当中。\n\\[\n\\begin{array}{rl}\nf(x) & = (a+b)^2 \\\\\n& = a^2+2ab+b^2\n\\end{array}\n\\]\n\\begin{array}{rl}\nf(x) & = (a+b)^2 \\\\\n& = a^2+2ab+b^2\n\\end{array}",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>书写数学公式</span>"
    ]
  },
  {
    "objectID": "write-math-equation.html#对齐",
    "href": "write-math-equation.html#对齐",
    "title": "Appendix B — 书写数学公式",
    "section": "B.19 对齐",
    "text": "B.19 对齐\n使用对齐符号 & 和 array 可以将多个元素按照指定规则 lcl 对齐。\n\\[\n\\begin{array}{lcl}\nz & = & a \\\\\nf(x,y,z) & = & x + y + z\n\\end{array}\n\\]\n\\begin{array}{lcl}\nz & = & a \\\\\nf(x,y,z) & = & x + y + z\n\\end{array}\n改变对齐规则后：\n\\[\n\\begin{array}{rcl}\nz & = & a \\\\\nf(x,y,z) & = & x + y + z\n\\end{array}\n\\]\n\\begin{array}{rcl}\nz & = & a \\\\\nf(x,y,z) & = & x + y + z\n\\end{array}",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>书写数学公式</span>"
    ]
  },
  {
    "objectID": "write-math-equation.html#括号",
    "href": "write-math-equation.html#括号",
    "title": "Appendix B — 书写数学公式",
    "section": "B.20 括号",
    "text": "B.20 括号\n常用的 ()、[]、{} 括号符号可以在 LaTeX 环境当中直接进行使用，但是如果处于较大的符号当中，就应该配合 \\left 与 \\right 命令来使用：\n\n\n\n\n\n\n\n\n类型\n符号\nLaTeX\n\n\n\n\n圆括号、小括号\n\\(\\left ( \\frac{a}{b} \\right )\\)\n\\left ( \\frac{a}{b} \\right )\n\n\n圆括号、小括号\n\\(( \\frac{a}{b} )\\)\n( \\frac{a}{b} )",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>书写数学公式</span>"
    ]
  },
  {
    "objectID": "write-math-equation.html#空格",
    "href": "write-math-equation.html#空格",
    "title": "Appendix B — 书写数学公式",
    "section": "B.21 空格",
    "text": "B.21 空格\n\n\n\n序号\n符号\nLaTeX\n\n\n\n\n双空格\n\\(a \\qquad b\\)\na \\qquad b\n\n\n单空格\n\\(a \\quad b\\)\na \\quad b\n\n\n字符空格\n\\(a\\ b\\)\na\\ b\n\n\n文本模式中的字符空格\n\\(a \\text{ } b\\)\na \\text{ } b\n\n\n大空格\n\\(a\\;b\\)\na\\;b\n\n\n小空格\n\\(a\\,b\\)\na\\,b",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>书写数学公式</span>"
    ]
  },
  {
    "objectID": "quarto-cheatsheet.html",
    "href": "quarto-cheatsheet.html",
    "title": "Appendix C — Quarto 速查表",
    "section": "",
    "text": "C.1 使用 Bootstrap 样式类\n作为表格标题旁边的属性给出的 Bootstrap 表格类会插入 &lt;table&gt; 元素。允许使用的类是那些明确适用于整个表格的类，它们是：\"primary\", \"secondary\", \"success\", \"danger\", \"warning\", \"info\", \"light\", \"dark\", \"striped\", \"hover\", \"active\", \"bordered\", \"borderless\", \"sm\", \"responsive\", \"responsive-sm\", \"responsive-md\", \"responsive-lg\", \"responsive-xl\", \"responsive-xxl\"。\n例如，下面的 Markdown 表格将以行条纹呈现，悬停时还会高亮显示行：",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quarto 速查表</span>"
    ]
  },
  {
    "objectID": "quarto-cheatsheet.html#使用-bootstrap-样式类",
    "href": "quarto-cheatsheet.html#使用-bootstrap-样式类",
    "title": "Appendix C — Quarto 速查表",
    "section": "",
    "text": "| fruit  | price  |\n|--------|--------|\n| apple  | 2.05   |\n| pear   | 1.37   |\n| orange | 3.09   |\n\n: Fruit prices {.striped .hover}\n\nFruit prices\n\n\nfruit\nprice\n\n\n\n\napple\n2.05\n\n\npear\n1.37\n\n\norange\n3.09",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quarto 速查表</span>"
    ]
  },
  {
    "objectID": "quarto-cheatsheet.html#小贴士",
    "href": "quarto-cheatsheet.html#小贴士",
    "title": "Appendix C — Quarto 速查表",
    "section": "C.2 小贴士",
    "text": "C.2 小贴士\n:::{.callout-note}\nNote that there are five types of callouts, including: \n`note`, `tip`, `warning`, `caution`, and `important`.\n:::\n\n\n\n\n\n\nNote\n\n\n\nNote that there are five types of callouts, including: note, tip, warning, caution, and important.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quarto 速查表</span>"
    ]
  },
  {
    "objectID": "quarto-cheatsheet.html#sec-quarto-page-break",
    "href": "quarto-cheatsheet.html#sec-quarto-page-break",
    "title": "Appendix C — Quarto 速查表",
    "section": "C.3 分页符",
    "text": "C.3 分页符\npage 1\n\n{{&lt; pagebreak &gt;}}\n\npage 2",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quarto 速查表</span>"
    ]
  },
  {
    "objectID": "quarto-cheatsheet.html#视频",
    "href": "quarto-cheatsheet.html#视频",
    "title": "Appendix C — Quarto 速查表",
    "section": "C.4 视频",
    "text": "C.4 视频\n{{&lt; video local-video.mp4 &gt;}}\n\n{{&lt; video https://www.youtube.com/embed/wo9vZccmqwc &gt;}}\n\n{{&lt; video https://vimeo.com/548291297 &gt;}}\n\n{{&lt; video https://youtu.be/wo9vZccmqwc width=\"400\" height=\"300\" &gt;}}\n\n{{&lt; video https://www.youtube.com/embed/wo9vZccmqwc\n    title=\"What is the CERN?\"\n    start=\"116\"\n    aspect-ratio=\"21x9\" \n&gt;}}",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quarto 速查表</span>"
    ]
  },
  {
    "objectID": "quarto-cheatsheet.html#div",
    "href": "quarto-cheatsheet.html#div",
    "title": "Appendix C — Quarto 速查表",
    "section": "C.5 DIV",
    "text": "C.5 DIV\nDIV 插入使用 3 个以上连续的英文分号（:::），并在对应数量的连续分号处结束。通过改变分号的数量，可以实现 DIV 的嵌套。\n::: {.border}\nThis content can be styled with a border\n:::\n::::: {#special .sidebar}\n\n::: {.warning}\nHere is a warning.\n:::\n\nMore content.\n:::::\n\n\nHere is a warning.\n\nMore content.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quarto 速查表</span>"
    ]
  },
  {
    "objectID": "quarto-cheatsheet.html#流程图",
    "href": "quarto-cheatsheet.html#流程图",
    "title": "Appendix C — Quarto 速查表",
    "section": "C.6 流程图",
    "text": "C.6 流程图\n```{mermaid}\nflowchart LR\n  A[Hard edge] --&gt; B(Round edge)\n  B --&gt; C{Decision}\n  C --&gt; D[Result one]\n  C --&gt; E[Result two]\n```\n效果如下：\n\n\n\n\n\nflowchart LR\n  A[Hard edge] --&gt; B(Round edge)\n  B --&gt; C{Decision}\n  C --&gt; D[Result one]\n  C --&gt; E[Result two]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quarto 速查表</span>"
    ]
  },
  {
    "objectID": "quarto-cheatsheet.html#键盘快捷键",
    "href": "quarto-cheatsheet.html#键盘快捷键",
    "title": "Appendix C — Quarto 速查表",
    "section": "C.7 键盘快捷键",
    "text": "C.7 键盘快捷键\nTo print, press {{&lt; kbd Shift-Ctrl-P &gt;}}. \n\nTo open an existing new project, press {{&lt; kbd mac=Shift-Command-O win=Shift-Control-O linux=Shift-Ctrl-L &gt;}}.\n在运行上面代码时，会根据操作系统显示相应的快捷键。",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quarto 速查表</span>"
    ]
  },
  {
    "objectID": "quarto-cheatsheet.html#交叉引用",
    "href": "quarto-cheatsheet.html#交叉引用",
    "title": "Appendix C — Quarto 速查表",
    "section": "C.8 交叉引用",
    "text": "C.8 交叉引用\n交叉引用需要用到索引标签。\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n类型\n标签前缀\n引用方法\n\n\n\n\n表格\ntbl-\n@tbl-id\n\n\n图片\nfig-\n\n\n\n代码块\n#tbl-，fig-\n\n\n\n公式\n#eq-\n\n\n\n章节\n#sec-\n\n\n\n代码列表\n#lst-customers\n@lst-customers\n\n\n定理与证明\n#thm-，#lem-\n\n\n\n\n\n\n\nC.8.1 引用格式\n\n\n\n类型\n语法\n\n\n\n\n默认\n@fig-elephant\n\n\n首字母大写\n@Fig-elephant\n\n\n自定义前缀\nFig @fig-elephant\n\n\n仅序号\n-@fig-elephant\n\n\n\n标签可以在代码块属性中设置：\n```{python}\n#| label: tbl-planets\n#| tbl-cap: Astronomical object\n\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\ntable = [[\"Sun\",\"696,000\",1.989e30],\n         [\"Earth\",\"6,371\",5.972e24],\n         [\"Moon\",\"1,737\",7.34e22],\n         [\"Mars\",\"3,390\",6.39e23]]\nMarkdown(tabulate(\n  table, \n  headers=[\"Astronomical object\",\"R (km)\", \"mass (kg)\"]\n))\n```\n\n\nC.8.2 表格\n\n\n\n\n\n\nImportant\n\n\n\nIn order for a table to be cross-referenceable, its label must start with the tbl- prefix.\n\n\n对于 Markdown 表格来说，如此添加索引标签。\n| Col1 | Col2 | Col3 |\n|------|------|------|\n| A    | B    | C    |\n| E    | F    | G    |\n| A    | G    | G    |\n\n: My Caption {#tbl-letters}\n\nSee @tbl-letters.\n如果表格是由代码段生成的，则依据前缀规则指定代码段标签。\n```{r}\n#| label: tbl-tables\n#| tbl-cap: \"Tables\"\n#| tbl-subcap: true\n#| layout-ncol: 2\n\nlibrary(knitr)\nkable(head(cars))\nkable(head(pressure))\n```\n\n\nC.8.3 图片\n![Elephant](elephant.png){#fig-elephant}\n\nSee @fig-elephant for an illustration.\n如果图片是由代码段生成的，则依据前缀规则指定代码段标签。\n```{r}\n#| label: fig-plot\n#| fig-cap: \"Plot\"\n\nplot(cars)\n```\nFor example, see @fig-plot.\n\n\nC.8.4 公式\n$$\nE = m C^2\n$$ {#eq-mass-energy}\n\\[\nE = m C^2\n\\tag{C.1}\\]\n质能方程参见 Equation C.1 。",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quarto 速查表</span>"
    ]
  },
  {
    "objectID": "quarto-cheatsheet.html#代码",
    "href": "quarto-cheatsheet.html#代码",
    "title": "Appendix C — Quarto 速查表",
    "section": "C.9 代码",
    "text": "C.9 代码\n指定代码语言使用 ```&lt;lang&gt;，指定代码语言并运行使用 ```{lang}。",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quarto 速查表</span>"
    ]
  },
  {
    "objectID": "quarto-cheatsheet.html#标签页",
    "href": "quarto-cheatsheet.html#标签页",
    "title": "Appendix C — Quarto 速查表",
    "section": "C.10 标签页",
    "text": "C.10 标签页\n\nJupyterKnitr\n\n\n```{python}\n#| label: fig-plot\n#| fig-cap: \"Plot\"\n\nimport matplotlib.pyplot as plt\nplt.plot([1,23,2,4])\nplt.show()\n```\n\nFor example, see @fig-plot.\n\n\n```{r}\n#| label: fig-plot\n#| fig-cap: \"Plot\"\n\nplot(cars)\n```\n\nFor example, see @fig-plot.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quarto 速查表</span>"
    ]
  },
  {
    "objectID": "quarto-cheatsheet.html#特殊符号",
    "href": "quarto-cheatsheet.html#特殊符号",
    "title": "Appendix C — Quarto 速查表",
    "section": "C.11 特殊符号",
    "text": "C.11 特殊符号\n\n如果想要插入 ```，则需要连续用 4 个反引号，将 3 个反引号包括起来。\n如果想要插入 ```{r}，需要使用两个大括号 {r}。\n如果想要插入一段代码却不执行，还可以增加缩进。",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quarto 速查表</span>"
    ]
  },
  {
    "objectID": "ggplot2-cheatsheet.html",
    "href": "ggplot2-cheatsheet.html",
    "title": "Appendix D — ggplot2 速查表",
    "section": "",
    "text": "D.1 基础\nggplot2 基于图形语法理念，即所有图形都可以通过相同组件构建：数据集、坐标系和几何对象（代表数据点的视觉标记）。\nlibrary(ggplot2)\n要展示数值，需要将数据中的变量映射到几何对象的视觉属性（美学映射），如大小、颜色以及x和y位置。\n使用以下模板构建图形：\n数据、几何对象函数和美学映射是必需项。统计变换、位置调整以及坐标系、分面、比例尺和主题函数为可选项，会提供合理的默认值。\nggplot(data = mpg, aes(x = cty, y = hwy)) + \n  geom_point()\nlast_plot()\nggsave(\"plot.png\", width = 5, height = 5)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>ggplot2 速查表</span>"
    ]
  },
  {
    "objectID": "ggplot2-cheatsheet.html#基础",
    "href": "ggplot2-cheatsheet.html#基础",
    "title": "Appendix D — ggplot2 速查表",
    "section": "",
    "text": "ggplot(data = &lt;数据&gt;) +\n  &lt;几何对象函数&gt;(mapping = aes(&lt;映射关系&gt;),\n  stat = &lt;统计变换&gt;,\n  position = &lt;位置调整&gt;) +\n  &lt;坐标系函数&gt; +\n  &lt;分面函数&gt; +\n  &lt;比例尺函数&gt; +\n  &lt;主题函数&gt;\n\n\nggplot(data = mpg, aes(x = cty, y = hwy)): 初始化绘图，后续通过添加图层完成。每个图层添加一个几何对象函数。\n\n\n\nlast_plot(): 返回上一次绘制的图形。\n\n\n\nggsave(\"plot.png\", width = 5, height = 5): 将最后一次绘制的图形保存为5英寸x5英寸的”plot.png”文件。文件类型自动匹配扩展名。",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>ggplot2 速查表</span>"
    ]
  },
  {
    "objectID": "ggplot2-cheatsheet.html#美学映射",
    "href": "ggplot2-cheatsheet.html#美学映射",
    "title": "Appendix D — ggplot2 速查表",
    "section": "D.2 美学映射",
    "text": "D.2 美学映射\n常用美学属性值：\n\ncolor和fill: 字符串（\"red\", \"#RRGGBB\"）\nlinetype: 整数或字符串（0=\"blank\",1=\"solid\",2=\"dashed\",3=\"dotted\",4=\"dotdash\",5=\"longdash\",6=\"twodash\"）\nsize: 整数（点的大小和文本尺寸，单位为毫米）\nlinewidth: 整数（线的宽度，单位为毫米）\nshape: 整数/形状名称或单个字符（\"a\"）\n\n形状整数/名称对应关系：0=\"square open\",1=\"circle open\",2=\"triangle open\",3=\"plus\",4=\"cross\",5=\"diamond open\",6=\"triangle down open\",7=\"square cross\",8=\"asterisk\",9=\"diamond plus\",10=\"circle plus\",11=\"star\",12=\"square plus\",13=\"circle cross\",14=\"square triangle\",15=\"square\",16=\"circle\",17=\"triangle\",18=\"diamond\",19=\"circle small\",20=\"bullet\",21=\"circle filled\",22=\"square filled\",23=\"diamond filled\",24=\"triangle filled\",25=\"triangle down filled\"",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>ggplot2 速查表</span>"
    ]
  },
  {
    "objectID": "ggplot2-cheatsheet.html#几何对象",
    "href": "ggplot2-cheatsheet.html#几何对象",
    "title": "Appendix D — ggplot2 速查表",
    "section": "D.3 几何对象",
    "text": "D.3 几何对象\n使用几何对象函数表示数据点，通过几何对象的美学属性映射变量。每个函数返回一个图层。\n\nD.3.1 图形基元\n\na &lt;- ggplot(economics, aes(date, unemploy))\nb &lt;- ggplot(seals, aes(x = long, y = lat))\n\n\na + geom_blank() 和 a + expand_limits(): 确保所有图形的范围包含所有值。\n\n\na + geom_blank()\n\n\n\n\n\n\n\n\n\nb + geom_curve(aes(yend = lat + 1, xend = long + 1), curvature = 1): 绘制从(x, y)到(xend, yend)的曲线。aes()参数：x, xend, y, yend, alpha, angle, color, curvature, linetype, size\n\n\nb + geom_curve(aes(yend = lat + 1, xend = long + 1), curvature = 1)\n\n\n\n\n\n\n\n\n\na + geom_path(lineend = \"butt\", linejoin = \"round\", linemitre = 1): 按数据出现顺序连接观测点。aes()参数：x, y, alpha, color, group, linetype, size\n\n\na + geom_path(lineend = \"butt\", linejoin = \"round\", linemitre = 1)\n\n\n\n\n\n\n\n\n\na + geom_polygon(aes(alpha = 50)): 将点连接成多边形。aes()参数：x, y, alpha, color, fill, group, subgroup, linetype, size\n\n\na + geom_polygon(aes(alpha = 50))\n\n\n\n\n\n\n\n\n\nb + geom_rect(aes(xmin = long, ymin = lat, xmax = long + 1, ymax = lat + 1)): 通过四个角点绘制矩形。aes()参数：xmax, xmin, ymax, ymin, alpha, color, fill, linetype, size\n\n\nb + geom_rect(aes(xmin = long, ymin = lat, xmax = long + 1, ymax = lat + 1))\n\n\n\n\n\n\n\n\n\na + geom_ribbon(aes(ymin = unemploy - 900, ymax = unemploy + 900)): 为每个x绘制从ymin到ymax的区间。aes()参数：x, ymax, ymin, alpha, color, fill, group, linetype, size\n\n\na + geom_ribbon(aes(ymin = unemploy - 900, ymax = unemploy + 900))\n\n\n\n\n\n\n\n\n\nD.3.1.1 线段\n常用美学属性：x, y, alpha, color, linetype, size, linewidth\n\nb + geom_abline(aes(intercept = 0, slope = 1)): 绘制指定斜率和截距的参考线\n\n\nb + geom_abline(aes(intercept = 0, slope = 1))\n\n\n\n\n\n\n\n\n\nb + geom_hline(aes(yintercept = lat)): 绘制水平参考线\n\n\nb + geom_hline(aes(yintercept = lat))\n\n\n\n\n\n\n\n\n\nb + geom_vline(aes(xintercept = long)): 绘制垂直参考线\n\n\nb + geom_vline(aes(xintercept = long))\n\n\n\n\n\n\n\n\n\nb + geom_segment(aes(yend = lat + 1, xend = long + 1)): 绘制两点间直线\n\n\nb + geom_segment(aes(yend = lat + 1, xend = long + 1))\n\n\n\n\n\n\n\n\n\nb + geom_spoke(aes(angle = 1:1155, radius = 1)): 使用极坐标绘制线段\n\n\nb + geom_spoke(aes(angle = 1:1155, radius = 1))\n\n\n\n\n\n\n\n\n\n\n\nD.3.2 单变量-连续型\n\nc &lt;- ggplot(mpg, aes(hwy))\nc2 &lt;- ggplot(mpg)\n\n\nc + geom_area(stat = \"bin\"): 面积图\n\n\nc + geom_area(stat = \"bin\")\n\n\n\n\n\n\n\n\n\nc + geom_density(kernel = \"gaussian\"): 核密度估计曲线\n\n\nc + geom_density(kernel = \"gaussian\")\n\n\n\n\n\n\n\n\n\nc + geom_dotplot(): 点图\n\n\nc + geom_dotplot()\n\n\n\n\n\n\n\n\n\nc + geom_freqpoly(): 频率多边形\n\n\nc + geom_freqpoly()\n\n\n\n\n\n\n\n\n\nc + geom_histogram(binwidth = 5): 直方图\n\n\nc + geom_histogram(binwidth = 5)\n\n\n\n\n\n\n\n\n\nc2 + geom_qq(aes(sample = hwy)): Q-Q图\n\n\nc2 + geom_qq(aes(sample = hwy))\n\n\n\n\n\n\n\n\n\n\nD.3.3 单变量-离散型\n\nd &lt;- ggplot(mpg, aes(fl))\n\n\nd + geom_bar(): 条形图\n\n\nd + geom_bar()\n\n\n\n\n\n\n\n\n\n\nD.3.4 双变量-连续型\n\ne &lt;- ggplot(mpg, aes(cty, hwy))\n\n\ne + geom_label(aes(label = cty), nudge_x = 1, nudge_y = 1): 带背景的文字标注\n\n\ne + geom_label(aes(label = cty), nudge_x = 1, nudge_y = 1)\n\n\n\n\n\n\n\n\n\ne + geom_point(): 散点图\n\n\ne + geom_point()\n\n\n\n\n\n\n\n\n\ne + geom_quantile(): 分位数回归线\n\n\ne + geom_quantile()\n\n\n\n\n\n\n\n\n\ne + geom_rug(sides = \"bl\"): 地毯图\n\n\ne + geom_rug(sides = \"bl\")\n\n\n\n\n\n\n\n\n\ne + geom_smooth(method = lm): 平滑条件均值\n\n\ne + geom_smooth(method = lm)\n\n\n\n\n\n\n\n\n\ne + geom_text(aes(label = cty), nudge_x = 1, nudge_y = 1): 文字标注\n\n\ne + geom_text(aes(label = cty), nudge_x = 1, nudge_y = 1)\n\n\n\n\n\n\n\n\n\n\nD.3.5 双变量-离散型与连续型\n\nf &lt;- ggplot(mpg, aes(class, hwy))\n\n\nf + geom_col(): 柱状图\n\n\nf + geom_col()\n\n\n\n\n\n\n\n\n\nf + geom_boxplot(): 箱线图\n\n\nf + geom_boxplot()\n\n\n\n\n\n\n\n\n\nf + geom_dotplot(binaxis =\"y\", stackdir = \"center\"): 点图\n\n\nf + geom_dotplot(binaxis =\"y\", stackdir = \"center\")\n\n\n\n\n\n\n\n\n\nf + geom_violin(scale = \"area\"): 小提琴图\n\n\nf + geom_violin(scale = \"area\")\n\n\n\n\n\n\n\n\n\n\nD.3.6 双变量-离散型\n\ng &lt;- ggplot(diamonds, aes(cut, color))\n\n\ng + geom_count(): 计数点图\n\n\ng + geom_count()\n\n\n\n\n\n\n\n\n\ne + geom_jitter(height = 2, width = 2): 抖动点图\n\n\ne + geom_jitter(height = 2, width = 2)\n\n\n\n\n\n\n\n\n\n\nD.3.7 双变量-连续型双变量分布\n\nh &lt;- ggplot(diamonds, aes(carat, price))\n\n\nh + geom_bin2d(binwidth = c(0.25, 500)): 二维分箱热图\n\n\nh + geom_bin2d(binwidth = c(0.25, 500))\n\n\n\n\n\n\n\n\n\nh + geom_density_2d(): 二维核密度等高线\n\n\nh + geom_density_2d()\n\n\n\n\n\n\n\n\n\nh + geom_hex(): 六边形分箱热图\n\n\nh + geom_hex()\n\n\n\n\n\n\n\n\n\n\nD.3.8 双变量-连续型函数\n\ni &lt;- ggplot(economics, aes(date, unemploy))\n\n\ni + geom_area(): 面积图\n\n\ni + geom_area()\n\n\n\n\n\n\n\n\n\ni + geom_line(): 折线图\n\n\ni + geom_line()\n\n\n\n\n\n\n\n\n\ni + geom_step(direction = \"hv\"): 阶梯图\n\n\ni + geom_step(direction = \"hv\")\n\n\n\n\n\n\n\n\n\n\nD.3.9 双变量-误差可视化\n\ndf &lt;- data.frame(grp = c(\"A\", \"B\"), fit = 4:5, se = 1:2)\nj &lt;- ggplot(df, aes(grp, fit, ymin = fit - se, ymax = fit + se))\n\n\nj + geom_crossbar(fatten = 2): 交叉条\n\n\nj + geom_crossbar(fatten = 2)\n\n\n\n\n\n\n\n\n\nj + geom_errorbar(): 误差条\n\n\nj + geom_errorbar()\n\n\n\n\n\n\n\n\n\nj + geom_linerange(): 线条范围\n\n\nj + geom_linerange()\n\n\n\n\n\n\n\n\n\nj + geom_pointrange(): 点范围\n\n\nj + geom_pointrange()\n\n\n\n\n\n\n\n\n\n\nD.3.10 双变量-地图\n根据数据中的简单要素绘制几何对象：\n\nnc &lt;- sf::st_read(system.file(\"shape/nc.shp\", package = \"sf\"))\n\nReading layer `nc' from data source \n  `/Users/gaoch/Library/Caches/org.R-project.R/R/renv/cache/v5/macos/R-4.4/aarch64-apple-darwin20/sf/1.0-19/fe02eec2f6b3ba0e24afe83d5ccfb528/sf/shape/nc.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n\nggplot(nc) +\n  geom_sf(aes(fill = AREA))\n\n\n\n\n\n\n\n\n\n\nD.3.11 三变量\n\nseals$z &lt;- with(seals, sqrt(delta_long^2 + delta_lat^2))\nl &lt;- ggplot(seals, aes(long, lat))\n\n\nl + geom_contour(aes(z = z)): 等高线图\n\n\nl + geom_contour(aes(z = z))\n\n\n\n\n\n\n\n\n\nl + geom_contour_filled(aes(fill = z)): 填充等高线图\n\n\nl + geom_contour_filled(aes(z = z))\n\n\n\n\n\n\n\n\n\nl + geom_raster(aes(fill = z), hjust = 0.5, vjust = 0.5, interpolate = FALSE): 栅格图\n\n\nl + geom_raster(aes(fill = z), hjust = 0.5, vjust = 0.5, interpolate = FALSE)\n\n\n\n\n\n\n\n\n\nl + geom_tile(aes(fill = z)): 瓦片图\n\n\nl + geom_tile(aes(fill = z))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>ggplot2 速查表</span>"
    ]
  },
  {
    "objectID": "ggplot2-cheatsheet.html#统计变换",
    "href": "ggplot2-cheatsheet.html#统计变换",
    "title": "Appendix D — ggplot2 速查表",
    "section": "D.4 统计变换",
    "text": "D.4 统计变换\n通过统计变换构建图层的另一种方式。统计变换会生成新的绘图变量（如计数、比例）。使用after_stat(变量名)语法将统计变量映射到美学属性。\n\ni + stat_density_2d(aes(fill = after_stat(level)), geom = \"polygon\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>ggplot2 速查表</span>"
    ]
  },
  {
    "objectID": "ggplot2-cheatsheet.html#比例尺",
    "href": "ggplot2-cheatsheet.html#比例尺",
    "title": "Appendix D — ggplot2 速查表",
    "section": "D.5 比例尺",
    "text": "D.5 比例尺\n使用scales包覆盖默认设置。比例尺将数据值映射到美学属性的视觉值。\n\nn &lt;- d + geom_bar(aes(fill = fl))\nn + scale_fill_manual(\n  values = c(\"skyblue\", \"royalblue\", \"blue\", \"navy\"),\n  limits = c(\"d\", \"e\", \"p\", \"r\"),\n  breaks =c(\"d\", \"e\", \"p\", \"r\"),\n  name = \"fuel\", \n  labels = c(\"D\", \"E\", \"P\", \"R\")\n)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>ggplot2 速查表</span>"
    ]
  },
  {
    "objectID": "ggplot2-cheatsheet.html#坐标系",
    "href": "ggplot2-cheatsheet.html#坐标系",
    "title": "Appendix D — ggplot2 速查表",
    "section": "D.6 坐标系",
    "text": "D.6 坐标系\n\nu &lt;- d + geom_bar()\n\n\nu + coord_cartesian(xlim = c(0, 5)): 笛卡尔坐标系\n\n\nu + coord_cartesian(xlim = c(0, 5))\n\n\n\n\n\n\n\n\n\nu + coord_fixed(ratio = 1/2): 固定比例坐标系\n\n\nu + coord_fixed(ratio = 1/2)\n\n\n\n\n\n\n\n\n\nu + coord_polar(theta = \"x\", direction = 1): 极坐标系\n\n\nu + coord_polar(theta = \"x\", direction = 1)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>ggplot2 速查表</span>"
    ]
  },
  {
    "objectID": "ggplot2-cheatsheet.html#位置调整",
    "href": "ggplot2-cheatsheet.html#位置调整",
    "title": "Appendix D — ggplot2 速查表",
    "section": "D.7 位置调整",
    "text": "D.7 位置调整\n位置调整决定如何排列重叠的几何对象。\n\ns &lt;- ggplot(mpg, aes(fl, fill = drv))\n\n\ns + geom_bar(position = \"dodge\"): 并列排列\n\n\ns + geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\ns + geom_bar(position = \"fill\"): 堆叠标准化\n\n\ns + geom_bar(position = \"fill\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>ggplot2 速查表</span>"
    ]
  },
  {
    "objectID": "ggplot2-cheatsheet.html#主题",
    "href": "ggplot2-cheatsheet.html#主题",
    "title": "Appendix D — ggplot2 速查表",
    "section": "D.8 主题",
    "text": "D.8 主题\n\nu + theme_bw(): 白底网格主题\n\n\nu + theme_bw()\n\n\n\n\n\n\n\n\n\nu + theme_classic(): 经典无网格主题\n\n\nu + theme_classic()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>ggplot2 速查表</span>"
    ]
  },
  {
    "objectID": "ggplot2-cheatsheet.html#分面",
    "href": "ggplot2-cheatsheet.html#分面",
    "title": "Appendix D — ggplot2 速查表",
    "section": "D.9 分面",
    "text": "D.9 分面\n分面根据离散变量值将图形划分为子图。\n\nt &lt;- ggplot(mpg, aes(cty, hwy)) + geom_point()\n\n\nt + facet_grid(. ~ fl): 列分面\n\n\nt + facet_grid(. ~ fl)\n\n\n\n\n\n\n\n\n\nt + facet_wrap(~ fl): 环绕分面\n\n\nt + facet_wrap(~ fl)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>ggplot2 速查表</span>"
    ]
  },
  {
    "objectID": "ggplot2-cheatsheet.html#标签与图例",
    "href": "ggplot2-cheatsheet.html#标签与图例",
    "title": "Appendix D — ggplot2 速查表",
    "section": "D.10 标签与图例",
    "text": "D.10 标签与图例\n使用labs()添加图形标签：\n\nt + labs(x = \"城市油耗\", y = \"高速油耗\", \n         title = \"油耗关系图\", \n         subtitle = \"城市与高速公路油耗对比\",\n         caption = \"数据来源: mpg数据集\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>ggplot2 速查表</span>"
    ]
  },
  {
    "objectID": "ggplot2-cheatsheet.html#缩放",
    "href": "ggplot2-cheatsheet.html#缩放",
    "title": "Appendix D — ggplot2 速查表",
    "section": "D.11 缩放",
    "text": "D.11 缩放\n\nt + coord_cartesian(xlim = c(0, 100), ylim = c(10,20)): 无损缩放\n\n\nt + coord_cartesian(xlim = c(0, 30), ylim = c(10,40))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>ggplot2 速查表</span>"
    ]
  },
  {
    "objectID": "dplyr-cheatsheet.html",
    "href": "dplyr-cheatsheet.html",
    "title": "Appendix E — dplyr 速查表",
    "section": "",
    "text": "E.1 汇总观测值\n对列应用汇总函数生成汇总统计表。汇总函数接收向量输入并返回单个值输出（参见汇总函数章节）。",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>dplyr 速查表</span>"
    ]
  },
  {
    "objectID": "dplyr-cheatsheet.html#汇总观测值",
    "href": "dplyr-cheatsheet.html#汇总观测值",
    "title": "Appendix E — dplyr 速查表",
    "section": "",
    "text": "summarize(.data, ...)：创建汇总统计表\n::: {.cell}\nmtcars |&gt; summarize(avg = mean(mpg))\n::: {.cell-output .cell-output-stdout}\n# A tibble: 1 × 1\n    avg\n  &lt;dbl&gt;\n1  20.1\n::: :::\ncount(.data, ..., wt = NULL, sort = FLASE, name = NULL)：按 ... 变量分组的行数统计。类似函数还有 tally(), add_count(), add_tally()。\n::: {.cell}\nmtcars |&gt; count(cyl)\n::: {.cell-output .cell-output-stdout}\n# A tibble: 3 × 2\n    cyl     n\n  &lt;dbl&gt; &lt;int&gt;\n1     4    11\n2     6     7\n3     8    14\n::: :::",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>dplyr 速查表</span>"
    ]
  },
  {
    "objectID": "dplyr-cheatsheet.html#分组观测",
    "href": "dplyr-cheatsheet.html#分组观测",
    "title": "Appendix E — dplyr 速查表",
    "section": "E.2 分组观测",
    "text": "E.2 分组观测\n\ngroup_by(.data, ..., .add = FALSE, .drop = TRUE) 创建分组副本，dplyr 函数会对每个分组单独操作后合并结果\n::: {.cell}\nmtcars |&gt;\n  group_by(cyl) |&gt;\n  summarize(avg = mean(mpg))\n::: {.cell-output .cell-output-stdout}\n# A tibble: 3 × 2\n    cyl   avg\n  &lt;dbl&gt; &lt;dbl&gt;\n1     4  26.7\n2     6  19.7\n3     8  15.1\n::: :::\nrowwise(.data, ...) 将数据转换为逐行分组模式，便于按行运算（适合处理列表类列数据），详见 tidyr 速查表中的列表列工作流\n::: {.cell}\nstarwars |&gt;\n  rowwise() |&gt;\n  mutate(film_count = length(films))\n::: {.cell-output .cell-output-stdout}\n# A tibble: 87 × 15\n# Rowwise: \n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Luke Sk…    172    77 blond      fair       blue            19   male  mascu…\n 2 C-3PO       167    75 &lt;NA&gt;       gold       yellow         112   none  mascu…\n 3 R2-D2        96    32 &lt;NA&gt;       white, bl… red             33   none  mascu…\n 4 Darth V…    202   136 none       white      yellow          41.9 male  mascu…\n 5 Leia Or…    150    49 brown      light      brown           19   fema… femin…\n 6 Owen La…    178   120 brown, gr… light      blue            52   male  mascu…\n 7 Beru Wh…    165    75 brown      light      blue            47   fema… femin…\n 8 R5-D4        97    32 &lt;NA&gt;       white, red red             NA   none  mascu…\n 9 Biggs D…    183    84 black      light      brown           24   male  mascu…\n10 Obi-Wan…    182    77 auburn, w… fair       blue-gray       57   male  mascu…\n# ℹ 77 more rows\n# ℹ 6 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;, film_count &lt;int&gt;\n::: :::\nungroup(x, ...) 返回非分组数据副本",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>dplyr 速查表</span>"
    ]
  },
  {
    "objectID": "dplyr-cheatsheet.html#操作观测",
    "href": "dplyr-cheatsheet.html#操作观测",
    "title": "Appendix E — dplyr 速查表",
    "section": "E.3 操作观测",
    "text": "E.3 操作观测\n\nE.3.1 提取观测\n行操作函数返回满足条件的行子集表\n\nfilter(.data, ..., .preserve = FALSE) 筛选符合逻辑条件的行\n::: {.cell}\nmtcars |&gt; filter(mpg &gt; 20)\n::: {.cell-output .cell-output-stdout}\n# A tibble: 14 × 11\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  21       6 160     110  3.9   2.62  16.5     0     1     4     4\n 2  21       6 160     110  3.9   2.88  17.0     0     1     4     4\n 3  22.8     4 108      93  3.85  2.32  18.6     1     1     4     1\n 4  21.4     6 258     110  3.08  3.22  19.4     1     0     3     1\n 5  24.4     4 147.     62  3.69  3.19  20       1     0     4     2\n 6  22.8     4 141.     95  3.92  3.15  22.9     1     0     4     2\n 7  32.4     4  78.7    66  4.08  2.2   19.5     1     1     4     1\n 8  30.4     4  75.7    52  4.93  1.62  18.5     1     1     4     2\n 9  33.9     4  71.1    65  4.22  1.84  19.9     1     1     4     1\n10  21.5     4 120.     97  3.7   2.46  20.0     1     0     3     1\n11  27.3     4  79      66  4.08  1.94  18.9     1     1     4     1\n12  26       4 120.     91  4.43  2.14  16.7     0     1     5     2\n13  30.4     4  95.1   113  3.77  1.51  16.9     1     1     5     2\n14  21.4     4 121     109  4.11  2.78  18.6     1     1     4     2\n::: :::\ndistinct(.data, ..., .keep_all = FALSE) 删除重复行\n::: {.cell}\nmtcars |&gt; distinct(gear)\n::: {.cell-output .cell-output-stdout}\n# A tibble: 3 × 1\n   gear\n  &lt;dbl&gt;\n1     4\n2     3\n3     5\n::: :::\nslice(.data, ...,, .preserve = FALSE) 按位置选择行\n::: {.cell}\nmtcars |&gt; slice(10:15)\n::: {.cell-output .cell-output-stdout}\n# A tibble: 6 × 11\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n2  17.8     6  168.   123  3.92  3.44  18.9     1     0     4     4\n3  16.4     8  276.   180  3.07  4.07  17.4     0     0     3     3\n4  17.3     8  276.   180  3.07  3.73  17.6     0     0     3     3\n5  15.2     8  276.   180  3.07  3.78  18       0     0     3     3\n6  10.4     8  472    205  2.93  5.25  18.0     0     0     3     4\n::: :::\nslice_sample(.data, ..., n, prop, weight_by = NULL, replace = FALSE) 随机抽样行，支持数量抽样（n）和比例抽样（prop）\n::: {.cell}\nmtcars |&gt; slice_sample(n = 5, replace = TRUE)\n::: {.cell-output .cell-output-stdout}\n# A tibble: 5 × 11\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  26       4  120.    91  4.43  2.14  16.7     0     1     5     2\n2  19.2     8  400    175  3.08  3.84  17.0     0     0     3     2\n3  10.4     8  472    205  2.93  5.25  18.0     0     0     3     4\n4  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n5  17.8     6  168.   123  3.92  3.44  18.9     1     0     4     4\n::: :::\nslice_min(.data, order_by, ..., n, prop, with_ties = TRUE) 与 slice_max() 选择极值行\n::: {.cell}\nmtcars |&gt; slice_min(mpg, prop = 0.25)\n::: {.cell-output .cell-output-stdout}\n# A tibble: 8 × 11\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  10.4     8  472    205  2.93  5.25  18.0     0     0     3     4\n2  10.4     8  460    215  3     5.42  17.8     0     0     3     4\n3  13.3     8  350    245  3.73  3.84  15.4     0     0     3     4\n4  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n5  14.7     8  440    230  3.23  5.34  17.4     0     0     3     4\n6  15       8  301    335  3.54  3.57  14.6     0     1     5     8\n7  15.2     8  276.   180  3.07  3.78  18       0     0     3     3\n8  15.2     8  304    150  3.15  3.44  17.3     0     0     3     2\n::: :::\nslice_head(.data, ..., n, prop) 与 slice_tail() 选择首尾行\n::: {.cell}\nmtcars |&gt; slice_head(n = 5)\n::: {.cell-output .cell-output-stdout}\n# A tibble: 5 × 11\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  21       6   160   110  3.9   2.62  16.5     0     1     4     4\n2  21       6   160   110  3.9   2.88  17.0     0     1     4     4\n3  22.8     4   108    93  3.85  2.32  18.6     1     1     4     1\n4  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1\n5  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2\n::: :::\n\n\nE.3.1.1 适用于 filter() 的逻辑运算符\n\n==\n&lt;\n&lt;=\nis.na()\n%in%\n|\nxor()\n!=\n&gt;\n&gt;=\n!is.na()\n!\n&\n其他运算符参见 ?base::Logic 和 ?Comparison\n\n\n\n\nE.3.2 排序观测\n\narrange(.data, ..., .by_group = FALSE) 按列值升序排列，配合 desc() 实现降序\n::: {.cell}\nmtcars |&gt; arrange(mpg)\n::: {.cell-output .cell-output-stdout}\n# A tibble: 32 × 11\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  10.4     8  472    205  2.93  5.25  18.0     0     0     3     4\n 2  10.4     8  460    215  3     5.42  17.8     0     0     3     4\n 3  13.3     8  350    245  3.73  3.84  15.4     0     0     3     4\n 4  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 5  14.7     8  440    230  3.23  5.34  17.4     0     0     3     4\n 6  15       8  301    335  3.54  3.57  14.6     0     1     5     8\n 7  15.2     8  276.   180  3.07  3.78  18       0     0     3     3\n 8  15.2     8  304    150  3.15  3.44  17.3     0     0     3     2\n 9  15.5     8  318    150  2.76  3.52  16.9     0     0     3     2\n10  15.8     8  351    264  4.22  3.17  14.5     0     1     5     4\n# ℹ 22 more rows\n:::\nmtcars |&gt; arrange(desc(mpg))\n::: {.cell-output .cell-output-stdout}\n# A tibble: 32 × 11\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  33.9     4  71.1    65  4.22  1.84  19.9     1     1     4     1\n 2  32.4     4  78.7    66  4.08  2.2   19.5     1     1     4     1\n 3  30.4     4  75.7    52  4.93  1.62  18.5     1     1     4     2\n 4  30.4     4  95.1   113  3.77  1.51  16.9     1     1     5     2\n 5  27.3     4  79      66  4.08  1.94  18.9     1     1     4     1\n 6  26       4 120.     91  4.43  2.14  16.7     0     1     5     2\n 7  24.4     4 147.     62  3.69  3.19  20       1     0     4     2\n 8  22.8     4 108      93  3.85  2.32  18.6     1     1     4     1\n 9  22.8     4 141.     95  3.92  3.15  22.9     1     0     4     2\n10  21.5     4 120.     97  3.7   2.46  20.0     1     0     3     1\n# ℹ 22 more rows\n::: :::\n\n\n\nE.3.3 添加观测\n\nadd_row(.data, ..., .before = NULL, .after = NULL) 添加一行或多行数据\n::: {.cell}\ncars |&gt; add_row(speed = 1, dist = 1)\n::: {.cell-output .cell-output-stdout}\n   speed dist\n1      4    2\n2      4   10\n3      7    4\n4      7   22\n5      8   16\n6      9   10\n7     10   18\n8     10   26\n9     10   34\n10    11   17\n11    11   28\n12    12   14\n13    12   20\n14    12   24\n15    12   28\n16    13   26\n17    13   34\n18    13   34\n19    13   46\n20    14   26\n21    14   36\n22    14   60\n23    14   80\n24    15   20\n25    15   26\n26    15   54\n27    16   32\n28    16   40\n29    17   32\n30    17   40\n31    17   50\n32    18   42\n33    18   56\n34    18   76\n35    18   84\n36    19   36\n37    19   46\n38    19   68\n39    20   32\n40    20   48\n41    20   52\n42    20   56\n43    20   64\n44    22   66\n45    23   54\n46    24   70\n47    24   92\n48    24   93\n49    24  120\n50    25   85\n51     1    1\n::: :::",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>dplyr 速查表</span>"
    ]
  },
  {
    "objectID": "dplyr-cheatsheet.html#操作变量",
    "href": "dplyr-cheatsheet.html#操作变量",
    "title": "Appendix E — dplyr 速查表",
    "section": "E.4 操作变量",
    "text": "E.4 操作变量\n\nE.4.1 提取变量\n列操作函数返回列的子集（向量或表格）\n\npull(.data, var = -1, name = NULL, ...) 按名称或位置提取列向量\n::: {.cell}\nmtcars |&gt; pull(wt)\n::: {.cell-output .cell-output-stdout}\n [1] 2.620 2.875 2.320 3.215 3.440 3.460 3.570 3.190 3.150 3.440 3.440 4.070\n[13] 3.730 3.780 5.250 5.424 5.345 2.200 1.615 1.835 2.465 3.520 3.435 3.840\n[25] 3.845 1.935 2.140 1.513 3.170 2.770 3.570 2.780\n::: :::\nselect(.data, ...) 提取列生成新表\n::: {.cell}\nmtcars |&gt; select(mpg, wt)\n::: {.cell-output .cell-output-stdout}\n# A tibble: 32 × 2\n     mpg    wt\n   &lt;dbl&gt; &lt;dbl&gt;\n 1  21    2.62\n 2  21    2.88\n 3  22.8  2.32\n 4  21.4  3.22\n 5  18.7  3.44\n 6  18.1  3.46\n 7  14.3  3.57\n 8  24.4  3.19\n 9  22.8  3.15\n10  19.2  3.44\n# ℹ 22 more rows\n::: :::\nrelocate(.data, ..., .before = NULL, .after = NULL) 调整列位置\n::: {.cell}\nmtcars |&gt; relocate(mpg, cyl, after = last_col())\n::: {.cell-output .cell-output-stdout}\n# A tibble: 32 × 11\n     mpg   cyl after  disp    hp  drat    wt  qsec    vs    am  gear\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  21       6     4  160    110  3.9   2.62  16.5     0     1     4\n 2  21       6     4  160    110  3.9   2.88  17.0     0     1     4\n 3  22.8     4     1  108     93  3.85  2.32  18.6     1     1     4\n 4  21.4     6     1  258    110  3.08  3.22  19.4     1     0     3\n 5  18.7     8     2  360    175  3.15  3.44  17.0     0     0     3\n 6  18.1     6     1  225    105  2.76  3.46  20.2     1     0     3\n 7  14.3     8     4  360    245  3.21  3.57  15.8     0     0     3\n 8  24.4     4     2  147.    62  3.69  3.19  20       1     0     4\n 9  22.8     4     2  141.    95  3.92  3.15  22.9     1     0     4\n10  19.2     6     4  168.   123  3.92  3.44  18.3     1     0     4\n# ℹ 22 more rows\n::: :::\n\n\nE.4.1.1 select() 和 across() 辅助函数\n\nmtcars |&gt; select(mpg:cyl)\n\n# A tibble: 32 × 2\n     mpg   cyl\n   &lt;dbl&gt; &lt;dbl&gt;\n 1  21       6\n 2  21       6\n 3  22.8     4\n 4  21.4     6\n 5  18.7     8\n 6  18.1     6\n 7  14.3     8\n 8  24.4     4\n 9  22.8     4\n10  19.2     6\n# ℹ 22 more rows\n\n\n\ncontains(match)\nnum_range(prefix, range)\n:，例如 mpg:cyl\nends_with(match)\nall_of(x) 或 any_of(x, ..., vars)\n!，例如 !gear\nstarts_with(match)\nmatches(match)\neverything()\n\n\n\n\nE.4.2 批量操作多列\n\ndf &lt;- tibble(x_1 = c(1, 2), x_2 = c(3, 4), y = c(4, 5))\n\n\nacross(.cols, .fun, ..., .name = NULL) 对多列进行统一汇总或转换操作\n::: {.cell}\ndf |&gt; summarize(across(everything(), mean))\n::: {.cell-output .cell-output-stdout}\n# A tibble: 1 × 3\n    x_1   x_2     y\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   1.5   3.5   4.5\n::: :::\nc_across(.cols) 在行式数据中跨列运算\n::: {.cell}\ndf |&gt; \n  rowwise() |&gt;\n  mutate(x_total = sum(c_across(1:2)))\n::: {.cell-output .cell-output-stdout}\n# A tibble: 2 × 4\n# Rowwise: \n    x_1   x_2     y x_total\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1     1     3     4       4\n2     2     4     5       6\n::: :::\n\n\n\nE.4.3 创建新变量\n对列应用向量化函数（接收向量输入并返回等长向量，参见向量化函数章节）生成新列\n\nmutate(.data, ..., .keep = \"all\", .before = NULL, .after = NULL) 创建新列，类似函数 add_column()\n::: {.cell}\nmtcars |&gt; mutate(gpm = 1 / mpg)\n::: {.cell-output .cell-output-stdout}\n# A tibble: 32 × 12\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb    gpm\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1  21       6  160    110  3.9   2.62  16.5     0     1     4     4 0.0476\n 2  21       6  160    110  3.9   2.88  17.0     0     1     4     4 0.0476\n 3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1 0.0439\n 4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1 0.0467\n 5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2 0.0535\n 6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1 0.0552\n 7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4 0.0699\n 8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2 0.0410\n 9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2 0.0439\n10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4 0.0521\n# ℹ 22 more rows\n:::\nmtcars |&gt; mutate(mtcars, gpm = 1 / mpg, .keep = \"none\")\n::: {.cell-output .cell-output-stdout}\n# A tibble: 32 × 12\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb    gpm\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1  21       6  160    110  3.9   2.62  16.5     0     1     4     4 0.0476\n 2  21       6  160    110  3.9   2.88  17.0     0     1     4     4 0.0476\n 3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1 0.0439\n 4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1 0.0467\n 5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2 0.0535\n 6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1 0.0552\n 7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4 0.0699\n 8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2 0.0410\n 9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2 0.0439\n10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4 0.0521\n# ℹ 22 more rows\n::: :::\nrename(.data, ...) 重命名列，rename_with() 可通过函数批量重命名\n::: {.cell}\nmtcars |&gt; rename(miles_per_gallon = mpg)\n::: {.cell-output .cell-output-stdout}\n# A tibble: 32 × 11\n   miles_per_gallon   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n              &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1             21       6  160    110  3.9   2.62  16.5     0     1     4     4\n 2             21       6  160    110  3.9   2.88  17.0     0     1     4     4\n 3             22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n 4             21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n 5             18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 6             18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n 7             14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 8             24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n 9             22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n10             19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n# ℹ 22 more rows\n::: :::",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>dplyr 速查表</span>"
    ]
  },
  {
    "objectID": "dplyr-cheatsheet.html#向量化函数",
    "href": "dplyr-cheatsheet.html#向量化函数",
    "title": "Appendix E — dplyr 速查表",
    "section": "E.5 向量化函数",
    "text": "E.5 向量化函数\n\nE.5.1 适用于 mutate()\n\n\nE.5.2 位移操作\n\ndplyr::lag() 向前位移\ndplyr::lead() 向后位移\n\n\n\nE.5.3 累积计算\n\ndplyr::cumall() 累积逻辑与\ndply::cumany() 累积逻辑或\ncummax() 累积最大值\ndplyr::cummean() 累积均值\ncummin() 累积最小值\ncumprod() 累积乘积\ncumsum() 累积求和\n\n\n\nE.5.4 排序计算\n\ndplyr::cume_dist() 累积分布比例\ndplyr::dense_rank() 紧密排名（无间隔）\ndplyr::min_rank() 最小排名\ndplyr::ntile() 分位数分组\ndplyr::percent_rank() 标准化排名\ndplyr::row_number() 行编号（首位优先）\n\n\n\nE.5.5 数学运算\n\n+, -, /, ^, %/%, %% 算术运算符\nlog(), log2(), log10() 对数函数\n&lt;, &lt;=, &gt;, &gt;=, !=, == 比较运算符\ndplyr::between() 区间判断\ndplyr::near() 近似相等判断\n\n\n\nE.5.6 扩展功能\n\ndplyr::case_when() 多条件判断\n::: {.cell}\nstarwars |&gt;\n  mutate(type = case_when(\n    height &gt; 200 | mass &gt; 200 ~ \"large\",\n    species == \"Droid\" ~ \"robot\",\n    TRUE ~ \"other\"\n  ))\n::: {.cell-output .cell-output-stdout}\n# A tibble: 87 × 15\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Luke Sk…    172    77 blond      fair       blue            19   male  mascu…\n 2 C-3PO       167    75 &lt;NA&gt;       gold       yellow         112   none  mascu…\n 3 R2-D2        96    32 &lt;NA&gt;       white, bl… red             33   none  mascu…\n 4 Darth V…    202   136 none       white      yellow          41.9 male  mascu…\n 5 Leia Or…    150    49 brown      light      brown           19   fema… femin…\n 6 Owen La…    178   120 brown, gr… light      blue            52   male  mascu…\n 7 Beru Wh…    165    75 brown      light      blue            47   fema… femin…\n 8 R5-D4        97    32 &lt;NA&gt;       white, red red             NA   none  mascu…\n 9 Biggs D…    183    84 black      light      brown           24   male  mascu…\n10 Obi-Wan…    182    77 auburn, w… fair       blue-gray       57   male  mascu…\n# ℹ 77 more rows\n# ℹ 6 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;, type &lt;chr&gt;\n::: :::\ndplyr::coalesce() 首有效值选取\ndplyr::if_else() 元素级条件判断\ndplyr::na_if() 特定值替换为 NA\npmax() 元素级最大值\npmin() 元素级最小值",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>dplyr 速查表</span>"
    ]
  },
  {
    "objectID": "dplyr-cheatsheet.html#汇总函数",
    "href": "dplyr-cheatsheet.html#汇总函数",
    "title": "Appendix E — dplyr 速查表",
    "section": "E.6 汇总函数",
    "text": "E.6 汇总函数\n\nE.6.1 适用于 summarize()\n\n\nE.6.2 计数类\n\ndplyr::n() 计数\ndplyr::n_distinct() 唯一值计数\nsum(!is.na()) 非空值计数\n\n\n\nE.6.3 位置度量\n\nmean() 均值\nmedian() 中位数\n\n\n\nE.6.4 逻辑运算\n\nmean() 真值比例\nsum() 真值数量\n\n\n\nE.6.5 顺序提取\n\ndplyr::first() 首元素\ndplyr::last() 末元素\ndplyr::nth() 指定位置元素\n\n\n\nE.6.6 极值提取\n\nquantile() 分位数\nmin() 最小值\nmax() 最大值\n\n\n\nE.6.7 离散度量\n\nIQR() 四分位距\nmad() 绝对中位差\nsd() 标准差\nvar() 方差",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>dplyr 速查表</span>"
    ]
  },
  {
    "objectID": "dplyr-cheatsheet.html#行名处理",
    "href": "dplyr-cheatsheet.html#行名处理",
    "title": "Appendix E — dplyr 速查表",
    "section": "E.7 行名处理",
    "text": "E.7 行名处理\n整洁数据不使用行名（存储于列外部的变量）。需要行名操作时：\n\ntibble::rownames_to_column() 行名转列\n::: {.cell}\na &lt;- rownames_to_column(mtcars, var = \"C\")\n:::\ntibble::columns_to_rownames() 列转行名\n::: {.cell}\ncolumn_to_rownames(a, var = \"C\")\n::: {.cell-output .cell-output-stdout}\n    mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n1  21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\n2  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\n3  22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\n4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\n5  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\n6  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\n7  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\n8  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n9  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\n10 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\n11 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\n12 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\n13 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\n14 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\n15 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\n16 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n17 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\n18 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n19 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n20 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n21 21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\n22 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\n23 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\n24 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\n25 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\n26 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n27 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n28 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n29 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\n30 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n31 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n32 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n::: :::\n配套函数 tibble::has_rownames() 和 tibble::remove_rownames()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>dplyr 速查表</span>"
    ]
  },
  {
    "objectID": "dplyr-cheatsheet.html#表格合并",
    "href": "dplyr-cheatsheet.html#表格合并",
    "title": "Appendix E — dplyr 速查表",
    "section": "E.8 表格合并",
    "text": "E.8 表格合并\n\nx &lt;- tribble(\n   ~A,  ~B, ~C,\n  \"a\", \"t\",  1,\n  \"b\", \"u\",  2,\n  \"c\", \"v\",  3\n)\n\ny &lt;- tribble(\n   ~A,  ~B, ~D,\n  \"a\", \"t\",  3,\n  \"b\", \"u\",  2,\n  \"d\", \"w\",  1\n)\n\n\nE.8.1 横向合并\n\nbind_cols(..., .name_repair) 横向拼接表格（不自动匹配列名）\n\n\n\nE.8.2 纵向叠加\n\nbind_rows(..., .id = NULL) 纵向合并，.id 参数可添加来源标识列\n\n\n\nE.8.3 关系连接\n扩展连接通过匹配键合并表格列：\n\nleft_join(x, y) 左连接\nright_join(x, y) 右连接\ninner_join(x, y) 内连接\nfull_join(x, y) 全连接\n\n过滤连接按另一个表筛选当前表：\n\nsemi_join(x, y) 交集筛选\nanti_join(x, y) 差集筛选\n\n嵌套连接：\n\nnest_join(x, y) 内连接结果嵌套存储\n\n\n\nE.8.4 连接键设置\n\n多列匹配：by = join_by(col1, col2)\n::: {.cell}\nleft_join(x, y, by = join_by(A))\n::: {.cell-output .cell-output-stdout}\n# A tibble: 3 × 5\n  A     B.x       C B.y       D\n  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1 a     t         1 t         3\n2 b     u         2 u         2\n3 c     v         3 &lt;NA&gt;     NA\n:::\nleft_join(x, y, by = join_by(A, B))\n::: {.cell-output .cell-output-stdout}\n# A tibble: 3 × 4\n  A     B         C     D\n  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 a     t         1     3\n2 b     u         2     2\n3 c     v         3    NA\n::: :::\n跨名匹配：by = join_by(col1 == col2)\n::: {.cell}\nleft_join(x, y, by = join_by(C == D))\n::: {.cell-output .cell-output-stdout}\n# A tibble: 3 × 5\n  A.x   B.x       C A.y   B.y  \n  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1 a     t         1 d     w    \n2 b     u         2 b     u    \n3 c     v         3 a     t    \n::: :::\n同名后缀参数：suffix\n::: {.cell}\nleft_join(x, y, by = join_by(C == D), suffix = c(\"1\", \"2\"))\n::: {.cell-output .cell-output-stdout}\n# A tibble: 3 × 5\n  A1    B1        C A2    B2   \n  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1 a     t         1 d     w    \n2 b     u         2 b     u    \n3 c     v         3 a     t    \n::: :::\n\n\n\nE.8.5 集合运算\n\nintersect(x, y) 交集\nsetdiff(x, y) 差集\nunion(x, y) 并集（去重）\nsetequal() 集合相等判断\n\n\n# 设置随机种子\nset.seed(123)\n\n# 生成两个随机字母向量\nx = sample(letters, 5)\ny = sample(letters, 8)\nprint(x)\n\n[1] \"o\" \"s\" \"n\" \"c\" \"j\"\n\nprint(y)\n\n[1] \"r\" \"v\" \"k\" \"e\" \"t\" \"n\" \"w\" \"s\"\n\n# 交集  \nintersect(x, y)\n\n[1] \"s\" \"n\"\n\n# 差集\nsetdiff(x, y)\n\n[1] \"o\" \"c\" \"j\"\n\n# 并集\nunion(x, y)\n\n [1] \"o\" \"s\" \"n\" \"c\" \"j\" \"r\" \"v\" \"k\" \"e\" \"t\" \"w\"\n\n# 集合相等\nsetequal(x, y)\n\n[1] FALSE",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>dplyr 速查表</span>"
    ]
  }
]